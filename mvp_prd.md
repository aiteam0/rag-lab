# MVP RAG System - Product Requirements Document (PRD)

## ğŸ“‹ Executive Summary

**í”„ë¡œì íŠ¸ëª…**: Multimodal RAG MVP System  
**ë²„ì „**: 1.0.0  
**ì‘ì„±ì¼**: 2025-01-10 

### ëª©ì 
ë³µì¡í•œ ê¸°ì¡´ ì‹œìŠ¤í…œì„ ê°„ì†Œí™”í•˜ì—¬ í•µì‹¬ ê¸°ëŠ¥ë§Œ í¬í•¨í•œ MVP(Minimum Viable Product) RAG ì‹œìŠ¤í…œ êµ¬ì¶•. LangGraphì˜ Plan-Execute-Observe íŒ¨í„´ê³¼ DDU ì¹´í…Œê³ ë¦¬ ê¸°ë°˜ ë™ì  í•„í„°ë§ì„ í™œìš©í•œ ì¼ë°˜í™”ëœ ë¬¸ì„œ ì²˜ë¦¬ RAG ì—ì´ì „íŠ¸ ê°œë°œ.

### í•µì‹¬ íŠ¹ì§•
- âœ… **ê°„ì†Œí™”ëœ DDU ìŠ¤í‚¤ë§ˆ**: 5ê°œ í•„ë“œë§Œ ì‚¬ìš©í•˜ëŠ” ë‹¨ìˆœí™”ëœ ê²€ìƒ‰ í•„í„°
- âœ… **ì´ì¤‘ ì–¸ì–´ ì§€ì›**: í•œêµ­ì–´/ì˜ì–´ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ (RRF ë³‘í•©)
- âœ… **LangGraph ì›Œí¬í”Œë¡œìš°**: P-E-O íŒ¨í„´ ê¸°ë°˜ ìˆœì°¨ì  ì„œë¸ŒíƒœìŠ¤í¬ ì‹¤í–‰
- âœ… **CRAG ê²€ì¦ ë¡œì§**: í™˜ê° ì²´í¬ ë° ë‹µë³€ í’ˆì§ˆ í‰ê°€
- âœ… **ë‹¨ì¼ LLM í”„ë¡œë°”ì´ë”**: OpenAIë§Œ ì‚¬ìš© (.env ê¸°ë°˜ ì„¤ì •)

---

## ğŸ—ï¸ ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜

### ì „ì²´ êµ¬ì¡°ë„

```mermaid
graph TB
    subgraph "Ingest Phase"
        A[Pickle File] --> B[DDU Loader]
        B --> C[Dual Embeddings]
        C --> D[PostgreSQL + pgvector]
    end
    
    subgraph "Retrieval Phase"
        E[Query] --> F[Hybrid Search]
        F --> G[Semantic Search]
        F --> H[Keyword Search]
        G --> I[RRF Merger]
        H --> I
    end
    
    subgraph "Workflow Phase"
        J[User Query] --> K[Planning Agent]
        K --> L[Subtask Executor]
        L --> M[Retrieval Node]
        M --> N[Synthesis Node]
        N --> O[Hallucination Check]
        O --> P[Answer Grader]
        P --> Q[Final Answer]
    end
```

### ë””ë ‰í† ë¦¬ êµ¬ì¡°

```
project/
â”œâ”€â”€ .env                           # í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
â”œâ”€â”€ requirements.txt               # Python íŒ¨í‚¤ì§€ ì˜ì¡´ì„±
â”œâ”€â”€ README.md                      # MVP ì‚¬ìš© ê°€ì´ë“œ
â”œâ”€â”€ pyproject.toml                 # UV íŒ¨í‚¤ì§€ ê´€ë¦¬
â”‚
â”œâ”€â”€ ingest/                        # Phase 1: ë°ì´í„° ì¸ì œìŠ¤íŠ¸
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ database.py               # DB ì—°ê²° ë° í…Œì´ë¸” ìƒì„±
â”‚   â”œâ”€â”€ models.py                 # ê°„ì†Œí™”ëœ DDU ëª¨ë¸
â”‚   â”œâ”€â”€ embeddings.py             # ì´ì¤‘ ì–¸ì–´ ì„ë² ë”© ì²˜ë¦¬
â”‚   â”œâ”€â”€ loader.py                 # Pickle íŒŒì¼ ë¡œë”
â”‚   â””â”€â”€ vector_store.py           # ë²¡í„° ì €ì¥ì†Œ ê´€ë¦¬
â”‚
â”œâ”€â”€ retrieval/                     # ê²€ìƒ‰ ëª¨ë“ˆ
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ search_filter.py          # MVP SearchFilter (5ê°œ í•„ë“œ)
â”‚   â”œâ”€â”€ hybrid_search.py          # RRF ê¸°ë°˜ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰
â”‚   â””â”€â”€ keyword_search.py         # í•œêµ­ì–´/ì˜ì–´ í‚¤ì›Œë“œ ê²€ìƒ‰
â”‚
â”œâ”€â”€ workflow/                      # Phase 2: LangGraph ì›Œí¬í”Œë¡œìš°
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ state.py                  # ì›Œí¬í”Œë¡œìš° ìƒíƒœ ì •ì˜
â”‚   â”œâ”€â”€ nodes/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ planning_agent.py    # ê³„íš ìˆ˜ë¦½ ë…¸ë“œ
â”‚   â”‚   â”œâ”€â”€ subtask_executor.py  # ì„œë¸ŒíƒœìŠ¤í¬ ì‹¤í–‰ ë…¸ë“œ
â”‚   â”‚   â”œâ”€â”€ retrieval.py         # ê²€ìƒ‰ ë…¸ë“œ
â”‚   â”‚   â”œâ”€â”€ synthesis.py         # ë‹µë³€ ìƒì„± ë…¸ë“œ
â”‚   â”‚   â”œâ”€â”€ hallucination.py     # í™˜ê° ì²´í¬ (CRAG)
â”‚   â”‚   â””â”€â”€ answer_grader.py     # ë‹µë³€ í‰ê°€ (CRAG)
â”‚   â”œâ”€â”€ tools/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ tavily_search.py     # Tavily ì›¹ ê²€ìƒ‰ ë„êµ¬
â”‚   â””â”€â”€ graph.py                  # ë©”ì¸ ì›Œí¬í”Œë¡œìš° ê·¸ë˜í”„
â”‚
â”œâ”€â”€ scripts/                       # ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸
â”‚   â”œâ”€â”€ 1_setup_database.py      # DB ì´ˆê¸°í™”
â”‚   â”œâ”€â”€ 2_ingest_documents.py    # ë¬¸ì„œ ì¸ì œìŠ¤íŠ¸
â”‚   â”œâ”€â”€ 3_test_workflow.py       # ì›Œí¬í”Œë¡œìš° í…ŒìŠ¤íŠ¸
â”‚   â”œâ”€â”€ 4_test_streaming.py      # ìŠ¤íŠ¸ë¦¬ë° í…ŒìŠ¤íŠ¸
â”‚   â””â”€â”€ test_nodes/               # ë…¸ë“œë³„ ë‹¨ìœ„ í…ŒìŠ¤íŠ¸
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ test_planning_agent.py     # Planning ë…¸ë“œ í…ŒìŠ¤íŠ¸
â”‚       â”œâ”€â”€ test_subtask_executor.py   # Subtask ì‹¤í–‰ í…ŒìŠ¤íŠ¸
â”‚       â”œâ”€â”€ test_retrieval.py          # ê²€ìƒ‰ ë…¸ë“œ í…ŒìŠ¤íŠ¸
â”‚       â”œâ”€â”€ test_dual_search.py        # ì´ì¤‘ ê²€ìƒ‰ ì „ëµ í…ŒìŠ¤íŠ¸
â”‚       â”œâ”€â”€ test_synthesis.py          # í•©ì„± ë…¸ë“œ í…ŒìŠ¤íŠ¸
â”‚       â”œâ”€â”€ test_hallucination.py      # í™˜ê° ì²´í¬ í…ŒìŠ¤íŠ¸
â”‚       â”œâ”€â”€ test_answer_grader.py      # ë‹µë³€ í‰ê°€ í…ŒìŠ¤íŠ¸
â”‚       â”œâ”€â”€ test_metadata_helper.py    # ë©”íƒ€ë°ì´í„° í—¬í¼ í…ŒìŠ¤íŠ¸
â”‚       â””â”€â”€ run_all_tests.py          # ì „ì²´ ë…¸ë“œ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
â”‚
â”œâ”€â”€ tutorial/                    # Beginnerë¥¼ ìœ„í•œ ìƒì„¸í•œ íŠœí† ë¦¬ì–¼ ìŠ¤í¬ë¦½íŠ¸(Python Interactive Mode, # %%)
â”‚   â”œâ”€â”€ 1_setup_database.py      # DB ì´ˆê¸°í™”
â”‚   â”œâ”€â”€ 2_ingest_documents.py    # ë¬¸ì„œ ì¸ì œìŠ¤íŠ¸
â”‚   â”œâ”€â”€ 3_test_workflow.py       # ì›Œí¬í”Œë¡œìš° í…ŒìŠ¤íŠ¸
â”‚   â”œâ”€â”€ 4_test_streaming.py      # ìŠ¤íŠ¸ë¦¬ë° í…ŒìŠ¤íŠ¸
â”‚   â””â”€â”€ test_nodes/               # ë…¸ë“œë³„ ë‹¨ìœ„ í…ŒìŠ¤íŠ¸
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ test_planning_agent.py     # Planning ë…¸ë“œ í…ŒìŠ¤íŠ¸
â”‚       â”œâ”€â”€ test_subtask_executor.py   # Subtask ì‹¤í–‰ í…ŒìŠ¤íŠ¸
â”‚       â”œâ”€â”€ test_retrieval.py          # ê²€ìƒ‰ ë…¸ë“œ í…ŒìŠ¤íŠ¸
â”‚       â”œâ”€â”€ test_dual_search.py        # ì´ì¤‘ ê²€ìƒ‰ ì „ëµ í…ŒìŠ¤íŠ¸
â”‚       â”œâ”€â”€ test_synthesis.py          # í•©ì„± ë…¸ë“œ í…ŒìŠ¤íŠ¸
â”‚       â”œâ”€â”€ test_hallucination.py      # í™˜ê° ì²´í¬ í…ŒìŠ¤íŠ¸
â”‚       â”œâ”€â”€ test_answer_grader.py      # ë‹µë³€ í‰ê°€ í…ŒìŠ¤íŠ¸
â”‚       â”œâ”€â”€ test_metadata_helper.py    # ë©”íƒ€ë°ì´í„° í—¬í¼ í…ŒìŠ¤íŠ¸
â”‚       â””â”€â”€ run_all_tests.py          # ì „ì²´ ë…¸ë“œ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
```

---

## ğŸ“Š Phase 1: Ingest ì‹œìŠ¤í…œ

### 1.1 ë°ì´í„°ë² ì´ìŠ¤ ìŠ¤í‚¤ë§ˆ

```sql
-- MVP ì „ìš© í…Œì´ë¸”
CREATE TABLE mvp_ddu_documents (
    -- ê¸°ë³¸ ì‹ë³„ì
    id SERIAL PRIMARY KEY,
    
    -- í•µì‹¬ ë©”íƒ€ë°ì´í„° (ê²€ìƒ‰ í•„í„°ìš©)
    source TEXT NOT NULL,              -- ì†ŒìŠ¤ íŒŒì¼ëª…
    page INTEGER,                      -- í˜ì´ì§€ ë²ˆí˜¸
    category TEXT NOT NULL,            -- DDU ì¹´í…Œê³ ë¦¬ (14ì¢…)
    
    -- ì½˜í…ì¸  í•„ë“œ
    page_content TEXT,                 -- ì›ë³¸ ì½˜í…ì¸ 
    translation_text TEXT,             -- ì˜ì–´ ë²ˆì—­
    contextualize_text TEXT,           -- í•œêµ­ì–´ ì»¨í…ìŠ¤íŠ¸
    caption TEXT,                      -- ìº¡ì…˜ (í‘œ/ê·¸ë¦¼)
    
    -- êµ¬ì¡°í™” ë°ì´í„°
    entity JSONB,                      -- ì—”í‹°í‹° ì •ë³´ (image, tableìš©)
    image_path TEXT,                   -- ì´ë¯¸ì§€ ê²½ë¡œ
    
    -- ì¶”ê°€ ì •ë³´
    human_feedback TEXT DEFAULT '',    -- íœ´ë¨¼ í”¼ë“œë°±
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    
    -- ë²¡í„° ì„ë² ë”© (OpenAI text-embedding-3-small)
    embedding_korean vector(1536),     -- í•œêµ­ì–´ ì„ë² ë”©
    embedding_english vector(1536),    -- ì˜ì–´ ì„ë² ë”©
    
    -- ì „ë¬¸ ê²€ìƒ‰ ì¸ë±ìŠ¤
    search_vector_korean tsvector,     -- í•œêµ­ì–´ ì „ë¬¸ ê²€ìƒ‰
    search_vector_english tsvector     -- ì˜ì–´ ì „ë¬¸ ê²€ìƒ‰
);

-- ë²¡í„° ê²€ìƒ‰ ì¸ë±ìŠ¤ (IVFFlat)
CREATE INDEX idx_korean_embedding ON mvp_ddu_documents 
    USING ivfflat (embedding_korean vector_cosine_ops)
    WITH (lists = 100);

CREATE INDEX idx_english_embedding ON mvp_ddu_documents 
    USING ivfflat (embedding_english vector_cosine_ops)
    WITH (lists = 100);

-- ì „ë¬¸ ê²€ìƒ‰ ì¸ë±ìŠ¤ (GIN)
CREATE INDEX idx_korean_fts ON mvp_ddu_documents 
    USING gin(search_vector_korean);

CREATE INDEX idx_english_fts ON mvp_ddu_documents 
    USING gin(search_vector_english);

-- ë©”íƒ€ë°ì´í„° ì¸ë±ìŠ¤
CREATE INDEX idx_source ON mvp_ddu_documents(source);
CREATE INDEX idx_category ON mvp_ddu_documents(category);
CREATE INDEX idx_page ON mvp_ddu_documents(page);
```

### 1.2 ê°„ì†Œí™”ëœ SearchFilter

```python
# retrieval/search_filter.py
from typing import Optional, List, Dict
from pydantic import BaseModel, Field

class MVPSearchFilter(BaseModel):
    """MVPìš© ê°„ì†Œí™”ëœ ê²€ìƒ‰ í•„í„° (5ê°œ í•„ë“œë§Œ ì‚¬ìš©)"""
    
    # í•µì‹¬ í•„í„° í•„ë“œ
    categories: Optional[List[str]] = Field(
        None, 
        description="DDU ì¹´í…Œê³ ë¦¬ í•„í„° (ì˜ˆ: ['paragraph', 'table', 'figure'])"
    )
    pages: Optional[List[int]] = Field(
        None,
        description="í˜ì´ì§€ ë²ˆí˜¸ í•„í„° (ì˜ˆ: [1, 2, 3])"
    )
    sources: Optional[List[str]] = Field(
        None,
        description="ì†ŒìŠ¤ íŒŒì¼ í•„í„° (ì˜ˆ: ['manual.pdf'])"
    )
    caption: Optional[str] = Field(
        None,
        description="ìº¡ì…˜ í…ìŠ¤íŠ¸ ê²€ìƒ‰ (LIKE ê²€ìƒ‰)"
    )
    entity: Optional[Dict] = Field(
        None,
        description="ì—”í‹°í‹° JSONB í•„í„° - type, title, keywords ë“± ê²€ìƒ‰"
    )
    
    def to_sql_where(self) -> tuple[str, dict]:
        """SQL WHERE ì ˆê³¼ íŒŒë¼ë¯¸í„° ìƒì„±"""
        conditions = []
        params = {}
        
        if self.categories:
            conditions.append("category = ANY(%(categories)s)")
            params['categories'] = self.categories
            
        if self.pages:
            conditions.append("page = ANY(%(pages)s)")
            params['pages'] = self.pages
            
        if self.sources:
            conditions.append("source = ANY(%(sources)s)")
            params['sources'] = self.sources
            
        if self.caption:
            conditions.append("caption ILIKE %(caption)s")
            params['caption'] = f"%{self.caption}%"
            
        if self.entity:
            # JSONB í•„ë“œ ê²€ìƒ‰ - ë‹¤ì–‘í•œ ë°©ì‹ ì§€ì›
            entity_conditions = []
            
            # type í•„ë“œ ê²€ìƒ‰
            if 'type' in self.entity:
                entity_conditions.append("entity->>'type' = %(entity_type)s")
                params['entity_type'] = self.entity['type']
            
            # title í•„ë“œ ê²€ìƒ‰
            if 'title' in self.entity:
                entity_conditions.append("entity->>'title' ILIKE %(entity_title)s")
                params['entity_title'] = f"%{self.entity['title']}%"
            
            # keywords ë°°ì—´ ê²€ìƒ‰
            if 'keywords' in self.entity:
                entity_conditions.append("entity->'keywords' ?| %(entity_keywords)s")
                params['entity_keywords'] = self.entity['keywords']
            
            # ëª¨ë“  entity ê´€ë ¨ ì¡°ê±´ì„ ANDë¡œ ê²°í•©
            if entity_conditions:
                conditions.append(f"({' AND '.join(entity_conditions)})")
            
        where_clause = " AND ".join(conditions) if conditions else "1=1"
        return where_clause, params
```

### 1.3 ì´ì¤‘ ì–¸ì–´ ì„ë² ë”© ì²˜ë¦¬

```python
# ingest/embeddings.py
from langchain_openai import OpenAIEmbeddings
from typing import List, Tuple
import os

class DualLanguageEmbeddings:
    """í•œêµ­ì–´/ì˜ì–´ ì´ì¤‘ ì„ë² ë”© ì²˜ë¦¬"""
    
    def __init__(self):
        self.embeddings = OpenAIEmbeddings(
            model=os.getenv("OPENAI_EMBEDDING_MODEL", "text-embedding-3-small"),
            openai_api_key=os.getenv("OPENAI_API_KEY")
        )
    
    async def embed_document(self, doc: dict) -> Tuple[List[float], List[float]]:
        """ë¬¸ì„œì— ëŒ€í•œ í•œêµ­ì–´/ì˜ì–´ ì„ë² ë”© ìƒì„±"""
        
        # í•œêµ­ì–´ í…ìŠ¤íŠ¸ ì¡°í•©
        korean_text = self._combine_korean_text(doc)
        
        # ì˜ì–´ í…ìŠ¤íŠ¸
        english_text = doc.get('translation_text', '')
        
        # ì„ë² ë”© ìƒì„±
        if korean_text and english_text:
            embeddings = await self.embeddings.aembed_documents([korean_text, english_text])
            return embeddings[0], embeddings[1]
        elif korean_text:
            embedding = await self.embeddings.aembed_query(korean_text)
            return embedding, None
        elif english_text:
            embedding = await self.embeddings.aembed_query(english_text)
            return None, embedding
        else:
            return None, None
    
    def _combine_korean_text(self, doc: dict) -> str:
        """í•œêµ­ì–´ í…ìŠ¤íŠ¸ ì¡°í•© (contextualize_text + page_content)"""
        texts = []
        
        if doc.get('contextualize_text'):
            texts.append(doc['contextualize_text'])
        
        if doc.get('page_content'):
            texts.append(doc['page_content'])
            
        return " ".join(texts)
```

### 1.4 í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ êµ¬í˜„

```python
# retrieval/hybrid_search.py
import asyncio
from typing import List, Dict, Any
from kiwipiepy import Kiwi
import asyncpg
import numpy as np

class HybridSearch:
    """RRF ê¸°ë°˜ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ (ì‹œë§¨í‹± + í‚¤ì›Œë“œ)"""
    
    def __init__(self, connection_pool: asyncpg.Pool):
        self.pool = connection_pool
        self.kiwi = Kiwi()
        self.k = 60  # RRF íŒŒë¼ë¯¸í„°
    
    async def search(
        self, 
        query: str, 
        filter: MVPSearchFilter,
        language: str = 'korean',
        top_k: int = 10
    ) -> List[Dict[str, Any]]:
        """í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì‹¤í–‰"""
        
        # ë³‘ë ¬ ê²€ìƒ‰ ì‹¤í–‰
        semantic_task = self._semantic_search(query, filter, language, top_k * 2)
        keyword_task = self._keyword_search(query, filter, language, top_k * 2)
        
        semantic_results, keyword_results = await asyncio.gather(
            semantic_task, keyword_task
        )
        
        # RRF ë³‘í•©
        merged_results = self._rrf_merge(semantic_results, keyword_results, top_k)
        
        return merged_results
    
    async def _semantic_search(
        self, 
        query: str, 
        filter: MVPSearchFilter,
        language: str,
        limit: int
    ) -> List[Dict[str, Any]]:
        """ë²¡í„° ìœ ì‚¬ë„ ê²€ìƒ‰"""
        
        # ì¿¼ë¦¬ ì„ë² ë”© ìƒì„±
        embeddings = DualLanguageEmbeddings()
        if language == 'korean':
            query_embedding = await embeddings.embeddings.aembed_query(query)
            embedding_column = 'embedding_korean'
        else:
            query_embedding = await embeddings.embeddings.aembed_query(query)
            embedding_column = 'embedding_english'
        
        # SQL ì¿¼ë¦¬ êµ¬ì„±
        where_clause, params = filter.to_sql_where()
        
        sql = f"""
        SELECT 
            id, source, page, category, page_content,
            translation_text, contextualize_text, caption, entity,
            1 - ({embedding_column} <=> $1::vector) as similarity
        FROM mvp_ddu_documents
        WHERE {where_clause}
            AND {embedding_column} IS NOT NULL
        ORDER BY {embedding_column} <=> $1::vector
        LIMIT $2
        """
        
        async with self.pool.acquire() as conn:
            results = await conn.fetch(
                sql, 
                query_embedding, 
                limit,
                **params
            )
            
        return [dict(r) for r in results]
    
    async def _keyword_search(
        self, 
        query: str, 
        filter: MVPSearchFilter,
        language: str,
        limit: int
    ) -> List[Dict[str, Any]]:
        """í‚¤ì›Œë“œ ì „ë¬¸ ê²€ìƒ‰"""
        
        where_clause, params = filter.to_sql_where()
        
        if language == 'korean':
            # Kiwi í† í¬ë‚˜ì´ì €ë¡œ í‚¤ì›Œë“œ ì¶”ì¶œ
            tokens = self._extract_korean_keywords(query)
            search_query = ' & '.join(tokens)
            search_column = 'search_vector_korean'
        else:
            # PostgreSQL to_tsquery ì‚¬ìš©
            search_query = ' & '.join(query.split())
            search_column = 'search_vector_english'
        
        sql = f"""
        SELECT 
            id, source, page, category, page_content,
            translation_text, contextualize_text, caption, entity,
            ts_rank({search_column}, to_tsquery('simple', $1)) as rank
        FROM mvp_ddu_documents
        WHERE {where_clause}
            AND {search_column} @@ to_tsquery('simple', $1)
        ORDER BY rank DESC
        LIMIT $2
        """
        
        async with self.pool.acquire() as conn:
            results = await conn.fetch(
                sql,
                search_query,
                limit,
                **params
            )
            
        return [dict(r) for r in results]
    
    def _extract_korean_keywords(self, text: str) -> List[str]:
        """Kiwië¥¼ ì‚¬ìš©í•œ í•œêµ­ì–´ í‚¤ì›Œë“œ ì¶”ì¶œ"""
        result = self.kiwi.tokenize(text)
        
        # ëª…ì‚¬, ë™ì‚¬, í˜•ìš©ì‚¬ë§Œ ì¶”ì¶œ
        keywords = []
        for token in result[0][0]:
            if token.tag.startswith(('NN', 'VV', 'VA')):
                keywords.append(token.form)
                
        return keywords
    
    def _rrf_merge(
        self,
        semantic_results: List[Dict],
        keyword_results: List[Dict],
        top_k: int
    ) -> List[Dict[str, Any]]:
        """Reciprocal Rank Fusion ë³‘í•©"""
        
        scores = {}
        
        # ì‹œë§¨í‹± ê²€ìƒ‰ ì ìˆ˜ ê³„ì‚°
        for rank, doc in enumerate(semantic_results, 1):
            doc_id = doc['id']
            scores[doc_id] = scores.get(doc_id, 0) + 1 / (self.k + rank)
            
        # í‚¤ì›Œë“œ ê²€ìƒ‰ ì ìˆ˜ ê³„ì‚°
        for rank, doc in enumerate(keyword_results, 1):
            doc_id = doc['id']
            scores[doc_id] = scores.get(doc_id, 0) + 1 / (self.k + rank)
        
        # ì ìˆ˜ë³„ ì •ë ¬
        sorted_ids = sorted(scores.keys(), key=lambda x: scores[x], reverse=True)[:top_k]
        
        # ë¬¸ì„œ ì •ë³´ ì¡°í•©
        doc_map = {}
        for doc in semantic_results + keyword_results:
            doc_map[doc['id']] = doc
            
        return [doc_map[doc_id] for doc_id in sorted_ids]
```

---

## ğŸ”„ Phase 2: LangGraph ì›Œí¬í”Œë¡œìš°

### 2.1 ì›Œí¬í”Œë¡œìš° ìƒíƒœ ì •ì˜

```python
# workflow/state.py
from typing import TypedDict, List, Dict, Optional, Annotated
from operator import add
from langchain_core.documents import Document

class MVPWorkflowState(TypedDict):
    """MVP ì›Œí¬í”Œë¡œìš° ìƒíƒœ"""
    
    # ì…ë ¥
    query: str                                    # ì‚¬ìš©ì ì¿¼ë¦¬
    
    # Plan-Execute-Observe
    subtasks: List[Dict]                         # ë¶„í•´ëœ ì„œë¸ŒíƒœìŠ¤í¬ ëª©ë¡
    current_subtask_idx: int                     # í˜„ì¬ ì‹¤í–‰ ì¤‘ì¸ ì„œë¸ŒíƒœìŠ¤í¬ ì¸ë±ìŠ¤
    subtask_results: Annotated[List[Dict], add]  # ì„œë¸ŒíƒœìŠ¤í¬ ì‹¤í–‰ ê²°ê³¼
    
    # Multi-Query (ì„œë¸ŒíƒœìŠ¤í¬ ë ˆë²¨)
    query_variations: List[str]                  # ì¿¼ë¦¬ ë³€í˜• ëª©ë¡
    
    # ê²€ìƒ‰ ê²°ê³¼
    documents: Annotated[List[Document], add]    # ê²€ìƒ‰ëœ ë¬¸ì„œë“¤
    
    # í’ˆì§ˆ ì²´í¬
    hallucination_check: Optional[Dict]          # í™˜ê° ì²´í¬ ê²°ê³¼
    answer_grade: Optional[Dict]                 # ë‹µë³€ í’ˆì§ˆ í‰ê°€
    
    # ìµœì¢… ê²°ê³¼
    final_answer: Optional[str]                  # ìµœì¢… ë‹µë³€
    
    # ì œì–´ í”Œë˜ê·¸
    iteration_count: int                         # ë°˜ë³µ íšŸìˆ˜
    should_use_web: bool                        # ì›¹ ê²€ìƒ‰ ì‚¬ìš© ì—¬ë¶€
    confidence_score: float                      # ë‹µë³€ ì‹ ë¢°ë„
    
    # ë©”íƒ€ë°ì´í„°
    error: Optional[str]                        # ì—ëŸ¬ ë©”ì‹œì§€
    metadata: Dict                              # ì¶”ê°€ ë©”íƒ€ë°ì´í„°
```

### 2.2 í•µì‹¬ ë…¸ë“œ êµ¬í˜„

#### 2.2.1 Planning Agent Node

```python
# workflow/nodes/planning_agent.py
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from pydantic import BaseModel, Field
from typing import List, Dict
import os

class Subtask(BaseModel):
    """ì„œë¸ŒíƒœìŠ¤í¬ ìŠ¤í‚¤ë§ˆ"""
    id: str = Field(description="ì„œë¸ŒíƒœìŠ¤í¬ ID")
    query: str = Field(description="ì„œë¸ŒíƒœìŠ¤í¬ ì¿¼ë¦¬")
    priority: int = Field(description="ìš°ì„ ìˆœìœ„ (1-5)")
    dependencies: List[str] = Field(default_factory=list, description="ì˜ì¡´ì„±")

class ExecutionPlan(BaseModel):
    """ì‹¤í–‰ ê³„íš ìŠ¤í‚¤ë§ˆ"""
    subtasks: List[Subtask] = Field(description="ì„œë¸ŒíƒœìŠ¤í¬ ëª©ë¡")
    strategy: str = Field(description="ì‹¤í–‰ ì „ëµ")

class PlanningAgentNode:
    """ì¿¼ë¦¬ë¥¼ ì„œë¸ŒíƒœìŠ¤í¬ë¡œ ë¶„í•´í•˜ëŠ” ê³„íš ë…¸ë“œ"""
    
    def __init__(self):
        self.llm = ChatOpenAI(
            model=os.getenv("OPENAI_MODEL", "gpt-4o-mini"),
            temperature=0,
            openai_api_key=os.getenv("OPENAI_API_KEY")
        )
        
        self.planning_prompt = ChatPromptTemplate.from_messages([
            ("system", """You are a query planning expert. Break down complex queries into subtasks.
            
Rules:
1. Create 3-5 subtasks maximum
2. Each subtask should be specific and answerable
3. Order by logical dependencies
4. Keep subtasks focused and atomic"""),
            ("human", "Query: {query}\n\nCreate an execution plan with subtasks.")
        ])
    
    async def __call__(self, state: MVPWorkflowState) -> Dict:
        """ë…¸ë“œ ì‹¤í–‰"""
        query = state["query"]
        
        # LLMìœ¼ë¡œ ì¿¼ë¦¬ ë¶„ì„ ë° ì„œë¸ŒíƒœìŠ¤í¬ ìƒì„±
        structured_llm = self.llm.with_structured_output(ExecutionPlan)
        
        plan = await structured_llm.ainvoke(
            self.planning_prompt.format_messages(query=query)
        )
        
        # ì„œë¸ŒíƒœìŠ¤í¬ë¥¼ ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ ë³€í™˜
        subtasks = [
            {
                "id": task.id,
                "query": task.query,
                "priority": task.priority,
                "dependencies": task.dependencies,
                "status": "pending"
            }
            for task in plan.subtasks
        ]
        
        return {
            "subtasks": subtasks,
            "current_subtask_idx": 0,
            "metadata": {"strategy": plan.strategy}
        }
```

#### 2.2.2 Subtask Executor Node

```python
# workflow/nodes/subtask_executor.py
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from pydantic import BaseModel, Field
from typing import List, Dict, Optional, Any
import os
import asyncpg

class SimplifiedMetadataHelper:
    """ê°„ì†Œí™”ëœ DB ë©”íƒ€ë°ì´í„° í—¬í¼"""
    
    def __init__(self, db_connection_string: str):
        self.db_connection = db_connection_string
    
    async def get_basic_metadata(self) -> Dict[str, Any]:
        """ê¸°ë³¸ ë©”íƒ€ë°ì´í„°ë§Œ ì¡°íšŒ"""
        conn = await asyncpg.connect(self.db_connection)
        
        try:
            # 1. ì†ŒìŠ¤ ëª©ë¡
            sources = await conn.fetch(
                "SELECT DISTINCT source FROM mvp_ddu_documents LIMIT 10"
            )
            
            # 2. í˜ì´ì§€ ë²”ìœ„
            page_range = await conn.fetchrow(
                "SELECT MIN(page) as min_page, MAX(page) as max_page FROM mvp_ddu_documents"
            )
            
            # 3. ì¹´í…Œê³ ë¦¬ ëª©ë¡
            categories = await conn.fetch(
                "SELECT DISTINCT category FROM mvp_ddu_documents"
            )
            
            # 4. Entity íƒ€ì…
            entity_types = await conn.fetch(
                "SELECT DISTINCT entity->>'type' as type FROM mvp_ddu_documents WHERE entity IS NOT NULL"
            )
            
            return {
                "sources": [r['source'] for r in sources],
                "page_min": page_range['min_page'] or 1,
                "page_max": page_range['max_page'] or 100,
                "categories": [r['category'] for r in categories],
                "entity_types": [r['type'] for r in entity_types if r['type']]
            }
            
        finally:
            await conn.close()

class QueryVariations(BaseModel):
    """ì¿¼ë¦¬ ë³€í˜• ìŠ¤í‚¤ë§ˆ"""
    variations: List[str] = Field(description="ì¿¼ë¦¬ ë³€í˜• ëª©ë¡ (3ê°œ)")

class QueryExtraction(BaseModel):
    """ì¿¼ë¦¬ì—ì„œ ì¶”ì¶œí•œ í•„í„°ë§ ì •ë³´"""
    page_numbers: List[int] = Field(default_factory=list, description="í˜ì´ì§€ ë²ˆí˜¸ (ì˜ˆ: 3, 5-7)")
    categories_mentioned: List[str] = Field(default_factory=list, description="ì–¸ê¸‰ëœ DDU ì¹´í…Œê³ ë¦¬")
    entity_type: Optional[str] = Field(None, description="image/table ì–¸ê¸‰ ì—¬ë¶€")
    keywords: List[str] = Field(default_factory=list, description="í•µì‹¬ í‚¤ì›Œë“œ")
    specific_requirements: str = Field(default="", description="íŠ¹ì • ìš”êµ¬ì‚¬í•­")

class DDUFilter(BaseModel):
    """DDU í•„í„° ìƒì„± ìŠ¤í‚¤ë§ˆ"""
    categories: List[str] = Field(default_factory=list)
    pages: List[int] = Field(default_factory=list)
    sources: List[str] = Field(default_factory=list)
    caption: str = Field(default="")
    entity: Dict = Field(default_factory=dict, description="Entity filter with type, title, keywords")

class SubtaskExecutorNode:
    """ì„œë¸ŒíƒœìŠ¤í¬ ì‹¤í–‰ ë…¸ë“œ (ë©”íƒ€ë°ì´í„° í™œìš© ë° Multi-Query í¬í•¨)"""
    
    def __init__(self, db_connection_string: str = None):
        self.llm = ChatOpenAI(
            model=os.getenv("OPENAI_MODEL", "gpt-4o-mini"),
            temperature=0.3,
            openai_api_key=os.getenv("OPENAI_API_KEY")
        )
        
        # DB ë©”íƒ€ë°ì´í„° í—¬í¼ (ì˜µì…”ë„)
        self.metadata_helper = None
        if db_connection_string:
            self.metadata_helper = SimplifiedMetadataHelper(db_connection_string)
        
        self.variation_prompt = ChatPromptTemplate.from_messages([
            ("system", """Generate 3 query variations for better search coverage.
Make variations that capture different aspects and phrasings."""),
            ("human", "Original query: {query}\n\nGenerate variations:")
        ])
        
        # Queryì—ì„œ ì •ë³´ ì¶”ì¶œ í”„ë¡¬í”„íŠ¸
        self.extraction_prompt = ChatPromptTemplate.from_messages([
            ("system", """Extract filtering information from the user query.
            
Look for:
- Page numbers: 3í˜ì´ì§€, page 5, 10-15í˜ì´ì§€, ì„¸ ë²ˆì§¸ í˜ì´ì§€
- Document types: í‘œ(table), ê·¸ë¦¼(figure), ì°¨íŠ¸(chart), ê·¸ë˜í”„, ì´ë¯¸ì§€
- Specific topics or keywords: ê¸°ìˆ  ìš©ì–´, ì œí’ˆëª…, ê¸°ëŠ¥ëª…
- Structural elements: ì œëª©, ëª©ë¡, ìˆ˜ì‹, ìº¡ì…˜, ê°ì£¼

Examples:
"3í˜ì´ì§€ì˜ ì•ˆì „ ê¸°ëŠ¥ í‘œ" â†’ pages: [3], entity_type: "table", keywords: ["ì•ˆì „", "ê¸°ëŠ¥"]
"ì—”ì§„ ì„±ëŠ¥ ì°¨íŠ¸" â†’ entity_type: "chart", keywords: ["ì—”ì§„", "ì„±ëŠ¥"]
"10-15í˜ì´ì§€ ì‚¬ì´ì˜ ê·¸ë¦¼ë“¤" â†’ pages: [10,11,12,13,14,15], entity_type: "image"
"""),
            ("human", "Query: {query}\n\nExtract filter information:")
        ])
        
        # ê°œì„ ëœ í•„í„° ìƒì„± í”„ë¡¬í”„íŠ¸
        self.filter_prompt = ChatPromptTemplate.from_messages([
            ("system", """Generate DDU filter based on the query and extracted information.

DDU Categories Available:
- heading1, heading2, heading3: ì œëª© ë ˆë²¨ (ì£¼ì œëª©, ë¶€ì œëª©, ì†Œì œëª©)
- paragraph: ì¼ë°˜ í…ìŠ¤íŠ¸ ë‹¨ë½
- list: ë¦¬ìŠ¤íŠ¸ í•­ëª© (ë¶ˆë¦¿, ë²ˆí˜¸ ëª©ë¡)
- table: í…Œì´ë¸” (í‘œ)
- figure: ê·¸ë¦¼/ì´ë¯¸ì§€
- chart: ì°¨íŠ¸/ê·¸ë˜í”„
- equation: ìˆ˜ì‹
- caption: ìº¡ì…˜ (ê·¸ë¦¼/í‘œ ì„¤ëª…)
- footnote: ê°ì£¼
- header, footer: í˜ì´ì§€ í—¤ë”/í‘¸í„°
- reference: ì°¸ì¡°ë¬¸í—Œ

Entity Structure (for image/table categories):
{{
    "type": "imgage" | "table",  // ì—”í‹°í‹° íƒ€ì…
    "title": "ì œëª© í…ìŠ¤íŠ¸",                   // ì—”í‹°í‹° ì œëª©
    "details": "ìƒì„¸ ì„¤ëª…",                    // ìƒì„¸ ë‚´ìš©
    "keywords": ["í‚¤ì›Œë“œ1", "í‚¤ì›Œë“œ2"],        // ê´€ë ¨ í‚¤ì›Œë“œë“¤
    "hypothetical_questions": ["ì˜ˆìƒ ì§ˆë¬¸ë“¤"],  // ë‹µë³€ ê°€ëŠ¥í•œ ì§ˆë¬¸ë“¤
    "raw_output": "ì›ë³¸ LLM ì¶œë ¥"             // LLM ì›ë³¸ ì¶œë ¥
}}

Filter Creation Guidelines:
1. Extract specific information from query (page numbers, categories, keywords)
2. Map Korean terms to DDU categories:
   - í‘œ â†’ table
   - ê·¸ë¦¼/ì´ë¯¸ì§€ â†’ figure
   - ì°¨íŠ¸/ê·¸ë˜í”„ â†’ chart
   - ì œëª© â†’ heading1/2/3
   - ëª©ë¡ â†’ list
3. For entity searches:
   - Use "type" for image/table filtering
   - Use "keywords" for topic matching
   - Use "title" for specific entity names
4. Be specific - only add filters that are clearly needed
5. Page numbers should be exact integers extracted from query

Examples:
Query: "5í˜ì´ì§€ì˜ ì•ˆì „ ê¸°ëŠ¥ í‘œë¥¼ ë³´ì—¬ì¤˜"
Filter: {{"pages": [5], "categories": ["table"], "entity": {{"keywords": ["ì•ˆì „", "ê¸°ëŠ¥"]}}}}

Query: "ì—”ì§„ ì„±ëŠ¥ì— ëŒ€í•œ ì°¨íŠ¸ê°€ ìˆë‚˜ìš”?"
Filter: {{"categories": ["chart", "figure"], "entity": {{"keywords": ["ì—”ì§„", "ì„±ëŠ¥"]}}}}

Query: "10-15í˜ì´ì§€ ì‚¬ì´ì˜ ëª¨ë“  ê·¸ë¦¼"
Filter: {{"pages": [10,11,12,13,14,15], "categories": ["figure"]}}
"""),
            ("human", """Query: {query}
Extracted Information:
- Pages: {pages}
- Entity Type: {entity_type}
- Keywords: {keywords}
- Categories Mentioned: {categories}

Database Metadata:
{metadata}

Generate appropriate DDU filter:""")
        ])
    
    async def __call__(self, state: MVPWorkflowState) -> Dict:
        """ë…¸ë“œ ì‹¤í–‰"""
        subtasks = state["subtasks"]
        current_idx = state["current_subtask_idx"]
        
        # í˜„ì¬ ì„œë¸ŒíƒœìŠ¤í¬ ê°€ì ¸ì˜¤ê¸°
        if current_idx >= len(subtasks):
            return {"current_subtask_idx": current_idx}
            
        current_subtask = subtasks[current_idx]
        
        # 1. ì¿¼ë¦¬ ë³€í˜• ìƒì„± (Multi-Query)
        variation_llm = self.llm.with_structured_output(QueryVariations)
        variations = await variation_llm.ainvoke(
            self.variation_prompt.format_messages(query=current_subtask["query"])
        )
        
        # 2. ì¿¼ë¦¬ì—ì„œ ì •ë³´ ì¶”ì¶œ
        extraction_llm = self.llm.with_structured_output(QueryExtraction)
        extracted_info = await extraction_llm.ainvoke(
            self.extraction_prompt.format_messages(query=current_subtask["query"])
        )
        
        # 3. DB ë©”íƒ€ë°ì´í„° ê°€ì ¸ì˜¤ê¸° (ìˆìœ¼ë©´)
        metadata_str = "No metadata available"
        if self.metadata_helper:
            metadata = await self.metadata_helper.get_basic_metadata()
            metadata_str = f"""Available sources: {metadata['sources'][:3]}...
Page range: {metadata['page_min']}-{metadata['page_max']}
Categories: {metadata['categories']}
Entity types: {metadata['entity_types']}"""
        
        # 4. ì¶”ì¶œëœ ì •ë³´ì™€ ë©”íƒ€ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ DDU í•„í„° ìƒì„±
        filter_llm = self.llm.with_structured_output(DDUFilter)
        ddu_filter = await filter_llm.ainvoke(
            self.filter_prompt.format_messages(
                query=current_subtask["query"],
                pages=extracted_info.page_numbers,
                entity_type=extracted_info.entity_type,
                keywords=extracted_info.keywords,
                categories=extracted_info.categories_mentioned,
                metadata=metadata_str
            )
        )
        
        # 5. ì„œë¸ŒíƒœìŠ¤í¬ ì‹¤í–‰ ê²°ê³¼ ì €ì¥
        result = {
            "subtask_id": current_subtask["id"],
            "original_query": current_subtask["query"],
            "variations": [current_subtask["query"]] + variations.variations,
            "extracted_info": extracted_info.model_dump(),
            "filter": ddu_filter.model_dump(),
            "status": "executing"
        }
        
        return {
            "query_variations": result["variations"],
            "subtask_results": [result],
            "metadata": {
                "current_filter": ddu_filter.model_dump(),
                "extracted_info": extracted_info.model_dump()
            }
        }
```

#### 2.2.3 Retrieval Node

```python
# workflow/nodes/retrieval.py
import asyncpg
import os
from typing import Dict, List
from langchain_core.documents import Document
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from pydantic import BaseModel, Field
from ...retrieval.hybrid_search import HybridSearch
from ...retrieval.search_filter import MVPSearchFilter

class LanguageDetection(BaseModel):
    """ì–¸ì–´ ê°ì§€ ê²°ê³¼"""
    language: str = Field(description="Detected language: 'korean' or 'english'")
    confidence: float = Field(description="Detection confidence (0.0-1.0)")

class DualSearchStrategy:
    """Entity í•„í„°ì™€ ì¼ë°˜ í•„í„°ë¥¼ ë¶„ë¦¬í•˜ì—¬ ê²€ìƒ‰í•˜ëŠ” ì „ëµ"""
    
    def __init__(self, search_engine):
        self.search_engine = search_engine
    
    async def search_with_dual_strategy(
        self, 
        query: str,
        filter_dict: Dict,
        language: str = 'korean',
        top_k: int = 10
    ) -> List[Document]:
        """ì´ì¤‘ ê²€ìƒ‰ ì „ëµ ì‹¤í–‰
        
        1. Entity í•„í„°ë¥¼ ë¶„ë¦¬
        2. ì¼ë°˜ í•„í„°ë¡œ ëª¨ë“  ì¹´í…Œê³ ë¦¬ ê²€ìƒ‰
        3. Entity í•„í„°ê°€ ìˆìœ¼ë©´ image/tableë§Œ ì¶”ê°€ ê²€ìƒ‰
        4. ê²°ê³¼ ë³‘í•© (ì¤‘ë³µ ì œê±°)
        """
        
        # Entity í•„í„° ë¶„ë¦¬
        entity_filter = filter_dict.pop("entity", None) if filter_dict else None
        
        all_documents = []
        seen_ids = set()
        
        # 1. ì¼ë°˜ í•„í„°ë¡œ ê²€ìƒ‰ (Entity ì—†ì´)
        general_filter = MVPSearchFilter(
            categories=filter_dict.get("categories") if filter_dict else None,
            pages=filter_dict.get("pages") if filter_dict else None,
            sources=filter_dict.get("sources") if filter_dict else None,
            caption=filter_dict.get("caption") if filter_dict else None,
            entity=None  # Entity í•„í„° ì œê±°
        )
        
        general_results = await self.search_engine.search(
            query=query,
            filter=general_filter,
            language=language,
            top_k=top_k
        )
        
        for doc in general_results:
            doc_id = doc["id"]
            if doc_id not in seen_ids:
                all_documents.append(doc)
                seen_ids.add(doc_id)
        
        # 2. Entity í•„í„°ê°€ ìˆìœ¼ë©´ ì¶”ê°€ ê²€ìƒ‰
        if entity_filter:
            entity_search_filter = MVPSearchFilter(
                categories=["image", "table"],  # Entity ì¹´í…Œê³ ë¦¬ë§Œ
                pages=filter_dict.get("pages") if filter_dict else None,
                sources=filter_dict.get("sources") if filter_dict else None,
                entity=entity_filter
            )
            
            entity_results = await self.search_engine.search(
                query=query,
                filter=entity_search_filter,
                language=language,
                top_k=5  # Entity ê²€ìƒ‰ì€ 5ê°œ
            )
            
            for doc in entity_results:
                doc_id = doc["id"]
                if doc_id not in seen_ids:
                    doc["search_type"] = "entity"  # ê²€ìƒ‰ íƒ€ì… í‘œì‹œ
                    all_documents.append(doc)
                    seen_ids.add(doc_id)
        
        return all_documents

class RetrievalNode:
    """í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ìˆ˜í–‰ ë…¸ë“œ (Dual Search Strategy í¬í•¨)"""
    
    def __init__(self, connection_string: str):
        self.connection_string = connection_string
        self.search_engine = None
        self.dual_search = None
        self.llm = ChatOpenAI(
            model=os.getenv("OPENAI_MODEL", "gpt-4o-mini"),
            temperature=0,
            openai_api_key=os.getenv("OPENAI_API_KEY")
        )
        
        self.language_prompt = ChatPromptTemplate.from_messages([
            ("system", """Detect the primary language of the query.
Return 'korean' for Korean text and 'english' for English text.
For mixed text, identify the dominant language."""),
            ("human", "Query: {query}")
        ])
    
    async def _ensure_connection(self):
        """DB ì—°ê²° í™•ì¸"""
        if not self.search_engine:
            pool = await asyncpg.create_pool(self.connection_string)
            self.search_engine = HybridSearch(pool)
            self.dual_search = DualSearchStrategy(self.search_engine)
    
    async def _detect_language(self, query: str) -> str:
        """LLM ê¸°ë°˜ ì–¸ì–´ ê°ì§€"""
        try:
            structured_llm = self.llm.with_structured_output(LanguageDetection)
            result = await structured_llm.ainvoke(
                self.language_prompt.format_messages(query=query)
            )
            return result.language
        except:
            # ì˜¤ë¥˜ ì‹œ ê¸°ë³¸ê°’
            return "korean"
    
    async def __call__(self, state: MVPWorkflowState) -> Dict:
        """ë…¸ë“œ ì‹¤í–‰"""
        await self._ensure_connection()
        
        query_variations = state.get("query_variations", [])
        metadata = state.get("metadata", {})
        filter_dict = metadata.get("current_filter", {})
        
        all_documents = []
        
        # ê° ì¿¼ë¦¬ ë³€í˜•ì— ëŒ€í•´ ê²€ìƒ‰ ìˆ˜í–‰
        for query in query_variations:
            # LLM ê¸°ë°˜ ì–¸ì–´ ê°ì§€
            language = await self._detect_language(query)
            
            # Dual Search Strategy ì‚¬ìš©
            results = await self.dual_search.search_with_dual_strategy(
                query=query,
                filter_dict=filter_dict.copy(),  # ì›ë³¸ ë³´ì¡´ì„ ìœ„í•´ ë³µì‚¬
                language=language,
                top_k=5  # ê° ë³€í˜•ë‹¹ 5ê°œ
            )
            
            # Document ê°ì²´ë¡œ ë³€í™˜
            for doc in results:
                all_documents.append(
                    Document(
                        page_content=doc.get("page_content", ""),
                        metadata={
                            "id": doc["id"],
                            "source": doc["source"],
                            "page": doc["page"],
                            "category": doc["category"],
                            "caption": doc.get("caption"),
                            "entity": doc.get("entity"),
                            "query": query
                        }
                    )
                )
        
        return {"documents": all_documents}
```

#### 2.2.4 Synthesis Node


```python
# workflow/nodes/synthesis.py
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from typing import Dict
import os

class SynthesisNode:
    """ê²€ìƒ‰ ê²°ê³¼ ê¸°ë°˜ ë‹µë³€ ìƒì„± ë…¸ë“œ"""
    
    def __init__(self):
        self.llm = ChatOpenAI(
            model=os.getenv("OPENAI_MODEL", "gpt-4o-mini"),
            temperature=0,
            openai_api_key=os.getenv("OPENAI_API_KEY")
        )
        
        self.synthesis_prompt = ChatPromptTemplate.from_messages([
            ("system", """You are an expert at synthesizing information from documents.
            
Create a comprehensive answer based on the provided documents.
Be specific and cite sources when possible.
If information is insufficient, acknowledge it."""),
            ("human", """Query: {query}

Documents:
{documents}

Synthesize a complete answer:""")
        ])
    
    async def __call__(self, state: MVPWorkflowState) -> Dict:
        """ë…¸ë“œ ì‹¤í–‰"""
        query = state["query"]
        documents = state.get("documents", [])
        
        # ë¬¸ì„œ ì»¨í…ìŠ¤íŠ¸ ì¤€ë¹„
        doc_context = self._prepare_documents(documents)
        
        if not doc_context:
            return {
                "final_answer": "ì£„ì†¡í•©ë‹ˆë‹¤. ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                "confidence_score": 0.0
            }
        
        # ë‹µë³€ ìƒì„±
        response = await self.llm.ainvoke(
            self.synthesis_prompt.format_messages(
                query=query,
                documents=doc_context
            )
        )
        
        # LLM ê¸°ë°˜ ì‹ ë¢°ë„ ê³„ì‚°
        confidence = await self._calculate_confidence(documents, response.content, query)
        
        return {
            "final_answer": response.content,
            "confidence_score": confidence
        }
    
    def _prepare_documents(self, documents: list) -> str:
        """ë¬¸ì„œë¥¼ ì»¨í…ìŠ¤íŠ¸ ë¬¸ìì—´ë¡œ ë³€í™˜ (ì „ì²´ ë‚´ìš© ì‚¬ìš©)"""
        if not documents:
            return ""
            
        contexts = []
        for i, doc in enumerate(documents, 1):  # ëª¨ë“  ë¬¸ì„œ ì‚¬ìš©
            metadata = doc.metadata
            content = doc.page_content  # ì „ì²´ ë‚´ìš© ì‚¬ìš©
            
            context = f"""[Doc {i}]
Source: {metadata.get('source', 'Unknown')}
Page: {metadata.get('page', 'N/A')}
Category: {metadata.get('category', 'Unknown')}
Content: {content}
---"""
            contexts.append(context)
            
        return "\n".join(contexts)
    
    async def _calculate_confidence(self, documents: list, answer: str, query: str) -> float:
        """LLM ê¸°ë°˜ ì‹ ë¢°ë„ ê³„ì‚°"""
        if not documents:
            return 0.0
        
        confidence_prompt = ChatPromptTemplate.from_messages([
            ("system", """Evaluate the confidence of the answer based on the provided documents.
Consider:
1. How well the documents support the answer
2. The relevance and quality of the documents
3. Coverage of the query requirements

Return a confidence score between 0.0 and 1.0."""),
            ("human", """Query: {query}
Answer: {answer}
Number of documents: {doc_count}
Unique sources: {unique_sources}

Evaluate confidence:""")
        ])
        
        class ConfidenceScore(BaseModel):
            score: float = Field(description="Confidence score (0.0-1.0)")
            reasoning: str = Field(description="Brief explanation")
        
        try:
            structured_llm = self.llm.with_structured_output(ConfidenceScore)
            result = await structured_llm.ainvoke(
                confidence_prompt.format_messages(
                    query=query,
                    answer=answer,
                    doc_count=len(documents),
                    unique_sources=len(set(d.metadata.get("source") for d in documents))
                )
            )
            return result.score
        except:
            # ì˜¤ë¥˜ ì‹œ ê¸°ë³¸ ê³„ì‚°
            return min(1.0, len(documents) * 0.1)
```

#### 2.2.5 Hallucination Check Node (CRAG)

```python
# workflow/nodes/hallucination.py
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from pydantic import BaseModel, Field
from typing import Dict, List
import os

class HallucinationCheck(BaseModel):
    """í™˜ê° ì²´í¬ ê²°ê³¼ ìŠ¤í‚¤ë§ˆ"""
    is_grounded: bool = Field(description="ë‹µë³€ì´ ë¬¸ì„œì— ê·¼ê±°í•˜ëŠ”ì§€")
    problematic_claims: List[str] = Field(default_factory=list, description="ë¬¸ì œê°€ ìˆëŠ” ì£¼ì¥ë“¤")
    confidence: float = Field(description="í‰ê°€ ì‹ ë¢°ë„ (0-1)")

class HallucinationCheckerNode:
    """ë‹µë³€ì˜ í™˜ê° ì—¬ë¶€ë¥¼ ì²´í¬í•˜ëŠ” ë…¸ë“œ (CRAG ë¡œì§)"""
    
    def __init__(self):
        self.llm = ChatOpenAI(
            model=os.getenv("OPENAI_MODEL", "gpt-4o-mini"),
            temperature=0,
            openai_api_key=os.getenv("OPENAI_API_KEY")
        )
        
        self.check_prompt = ChatPromptTemplate.from_messages([
            ("system", """You are a fact-checking expert. 
Verify if the answer is grounded in the provided documents.

Check each claim in the answer against the documents.
Identify any hallucinations or unsupported statements."""),
            ("human", """Documents:
{documents}

Answer:
{answer}

Check for hallucinations:""")
        ])
    
    async def __call__(self, state: MVPWorkflowState) -> Dict:
        """ë…¸ë“œ ì‹¤í–‰"""
        answer = state.get("final_answer", "")
        documents = state.get("documents", [])
        
        if not answer or not documents:
            return {
                "hallucination_check": {
                    "is_grounded": False,
                    "confidence": 0.0
                }
            }
        
        # ë¬¸ì„œ ì»¨í…ìŠ¤íŠ¸ ì¤€ë¹„
        doc_context = self._prepare_documents(documents)
        
        # í™˜ê° ì²´í¬
        structured_llm = self.llm.with_structured_output(HallucinationCheck)
        
        check_result = await structured_llm.ainvoke(
            self.check_prompt.format_messages(
                documents=doc_context,
                answer=answer
            )
        )
        
        return {
            "hallucination_check": {
                "is_grounded": check_result.is_grounded,
                "problematic_claims": check_result.problematic_claims,
                "confidence": check_result.confidence
            }
        }
    
    def _prepare_documents(self, documents: list) -> str:
        """ë¬¸ì„œ ì»¨í…ìŠ¤íŠ¸ ì¤€ë¹„ (ì „ì²´ ë‚´ìš© ì‚¬ìš©)"""
        contexts = []
        for doc in documents:  # ëª¨ë“  ë¬¸ì„œ ì‚¬ìš©
            contexts.append(doc.page_content)  # ì „ì²´ ë‚´ìš© ì‚¬ìš©
        return "\n---\n".join(contexts)
```

#### 2.2.6 Answer Grader Node (CRAG)

```python
# workflow/nodes/answer_grader.py
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from pydantic import BaseModel, Field
from typing import Dict
import os

class AnswerGrade(BaseModel):
    """ë‹µë³€ í‰ê°€ ê²°ê³¼ ìŠ¤í‚¤ë§ˆ"""
    relevance: float = Field(description="ì¿¼ë¦¬ì™€ì˜ ê´€ë ¨ì„± (0-1)")
    completeness: float = Field(description="ë‹µë³€ì˜ ì™„ì „ì„± (0-1)")
    clarity: float = Field(description="ëª…í™•ì„± (0-1)")
    overall_score: float = Field(description="ì „ì²´ ì ìˆ˜ (0-1)")

class AnswerGraderNode:
    """ë‹µë³€ í’ˆì§ˆì„ í‰ê°€í•˜ëŠ” ë…¸ë“œ (CRAG ë¡œì§)"""
    
    def __init__(self):
        self.llm = ChatOpenAI(
            model=os.getenv("OPENAI_MODEL", "gpt-4o-mini"),
            temperature=0,
            openai_api_key=os.getenv("OPENAI_API_KEY")
        )
        
        self.grading_prompt = ChatPromptTemplate.from_messages([
            ("system", """You are an answer quality evaluator.

Grade the answer based on:
1. Relevance to the query
2. Completeness of information
3. Clarity and structure
4. Overall quality

Provide scores from 0.0 to 1.0."""),
            ("human", """Query: {query}

Answer:
{answer}

Document Count: {doc_count}

Grade this answer:""")
        ])
    
    async def __call__(self, state: MVPWorkflowState) -> Dict:
        """ë…¸ë“œ ì‹¤í–‰"""
        query = state["query"]
        answer = state.get("final_answer", "")
        documents = state.get("documents", [])
        
        if not answer:
            return {
                "answer_grade": {
                    "overall_score": 0.0,
                    "needs_improvement": True
                }
            }
        
        # ë‹µë³€ í‰ê°€
        structured_llm = self.llm.with_structured_output(AnswerGrade)
        
        grade = await structured_llm.ainvoke(
            self.grading_prompt.format_messages(
                query=query,
                answer=answer,
                doc_count=len(documents)
            )
        )
        
        return {
            "answer_grade": {
                "relevance": grade.relevance,
                "completeness": grade.completeness,
                "clarity": grade.clarity,
                "overall_score": grade.overall_score,
                "needs_improvement": grade.overall_score < 0.7
            }
        }
```

### 2.3 Filter Generation ì˜ˆì‹œ

#### ì‚¬ìš© ì‚¬ë¡€ë³„ í•„í„° ìƒì„± ì˜ˆì‹œ

```python
# ì˜ˆì‹œ 1: íŠ¹ì • í˜ì´ì§€ì˜ í…Œì´ë¸” ìš”ì²­
Query: "3í˜ì´ì§€ì— ìˆëŠ” í‘œì¤€ ê·œê²©ì— ëŒ€í•œ í…Œì´ë¸”ì„ ì„¤ëª…í•´ì¤˜"

# Step 1: Query Extraction
{
    "page_numbers": [3],
    "categories_mentioned": ["table"],
    "entity_type": "table",
    "keywords": ["í‘œì¤€", "ê·œê²©"],
    "specific_requirements": "ì„¤ëª… ìš”ì²­"
}

# Step 2: Generated Filter
{
    "pages": [3],
    "categories": ["table", "caption"],  # captionë„ í¬í•¨ (í…Œì´ë¸” ì„¤ëª…)
    "entity": {
        "type": "table",
        "keywords": ["í‘œì¤€", "ê·œê²©", "specification", "standard"]
    }
}

# ì˜ˆì‹œ 2: ë²”ìœ„ í˜ì´ì§€ì˜ ê·¸ë¦¼ ìš”ì²­
Query: "10í˜ì´ì§€ì—ì„œ 15í˜ì´ì§€ ì‚¬ì´ì˜ ì—”ì§„ ê´€ë ¨ ê·¸ë¦¼ë“¤ì„ ë³´ì—¬ì¤˜"

# Step 1: Query Extraction
{
    "page_numbers": [10, 11, 12, 13, 14, 15],
    "categories_mentioned": ["figure"],
    "entity_type": "image",
    "keywords": ["ì—”ì§„", "engine"],
    "specific_requirements": "ê·¸ë¦¼ í‘œì‹œ"
}

# Step 2: Generated Filter
{
    "pages": [10, 11, 12, 13, 14, 15],
    "categories": ["figure", "chart"],  # chartë„ í¬í•¨ (ì—”ì§„ ì„±ëŠ¥ ì°¨íŠ¸ ê°€ëŠ¥ì„±)
    "entity": {
        "type": "image",
        "keywords": ["ì—”ì§„", "engine", "motor", "ë™ë ¥"]
    }
}

# ì˜ˆì‹œ 3: í‚¤ì›Œë“œ ê¸°ë°˜ ê²€ìƒ‰
Query: "ì•ˆì „ ê¸°ëŠ¥ì— ëŒ€í•œ ëª¨ë“  ì°¨íŠ¸ì™€ í‘œë¥¼ ì°¾ì•„ì¤˜"

# Step 1: Query Extraction
{
    "page_numbers": [],
    "categories_mentioned": ["chart", "table"],
    "entity_type": null,  # ë³µìˆ˜ íƒ€ì…
    "keywords": ["ì•ˆì „", "ê¸°ëŠ¥", "safety", "feature"],
    "specific_requirements": "ëª¨ë“  ì°¨íŠ¸ì™€ í‘œ"
}

# Step 2: Generated Filter
{
    "categories": ["table", "chart", "figure"],
    "entity": {
        "keywords": ["ì•ˆì „", "safety", "ë³´ì•ˆ", "security", "ê¸°ëŠ¥", "feature"]
    }
}
```

#### Filter Validation Rules

```python
def validate_filter(filter_dict: Dict, metadata: Dict) -> Tuple[bool, List[str]]:
    """í•„í„° ìœ íš¨ì„± ê²€ì¦"""
    
    errors = []
    
    # 1. í˜ì´ì§€ ë²”ìœ„ ê²€ì¦
    if filter_dict.get("pages"):
        for page in filter_dict["pages"]:
            if page < metadata["page_range"]["min"] or page > metadata["page_range"]["max"]:
                errors.append(f"Invalid page {page}: out of range")
    
    # 2. ì¹´í…Œê³ ë¦¬ ê²€ì¦
    if filter_dict.get("categories"):
        valid_categories = set(metadata["categories"].keys())
        for cat in filter_dict["categories"]:
            if cat not in valid_categories:
                errors.append(f"Invalid category '{cat}': not in database")
    
    # 3. Entity type ê²€ì¦
    if filter_dict.get("entity", {}).get("type"):
        entity_type = filter_dict["entity"]["type"]
        if entity_type not in ["image", "table"]:
            errors.append(f"Invalid entity type '{entity_type}'")
    
    # 4. í•„ìˆ˜ í•„ë“œ í™•ì¸
    if not any([filter_dict.get("pages"), 
                filter_dict.get("categories"),
                filter_dict.get("entity"),
                filter_dict.get("caption")]):
        errors.append("At least one filter field must be specified")
    
    return len(errors) == 0, errors
```

### 2.4 Tavily ì›¹ ê²€ìƒ‰ ë„êµ¬

```python
# workflow/tools/tavily_search.py
from tavily import TavilyClient
from typing import Dict, List
import os

class TavilySearchTool:
    """Tavily ì›¹ ê²€ìƒ‰ ë„êµ¬"""
    
    def __init__(self):
        self.client = TavilyClient(api_key=os.getenv("TAVILY_API_KEY"))
    
    async def search(self, query: str, max_results: int = 3) -> List[Dict]:
        """ì›¹ ê²€ìƒ‰ ìˆ˜í–‰"""
        try:
            response = self.client.search(
                query=query,
                max_results=max_results,
                search_depth="advanced",
                include_answer=True
            )
            
            results = []
            for result in response.get("results", []):
                results.append({
                    "title": result.get("title"),
                    "content": result.get("content"),
                    "url": result.get("url"),
                    "score": result.get("score", 0.0)
                })
                
            return results
            
        except Exception as e:
            print(f"Tavily search error: {e}")
            return []

# ì›¹ ê²€ìƒ‰ ë…¸ë“œ
async def web_search_node(state: MVPWorkflowState) -> Dict:
    """ì›¹ ê²€ìƒ‰ ë…¸ë“œ"""
    query = state["query"]
    
    tool = TavilySearchTool()
    results = await tool.search(query)
    
    # ê²°ê³¼ë¥¼ Document í˜•íƒœë¡œ ë³€í™˜
    from langchain_core.documents import Document
    
    web_documents = []
    for result in results:
        web_documents.append(
            Document(
                page_content=result["content"],
                metadata={
                    "source": result["url"],
                    "title": result["title"],
                    "category": "web_search"
                }
            )
        )
    
    return {"documents": web_documents}
```

### 2.4 ë©”ì¸ ì›Œí¬í”Œë¡œìš° ê·¸ë˜í”„

```python
# workflow/graph.py
from langgraph.graph import StateGraph, END, START
from langgraph.errors import GraphRecursionError
from typing import Literal
from .state import MVPWorkflowState
from .nodes.planning_agent import PlanningAgentNode
from .nodes.subtask_executor import SubtaskExecutorNode
from .nodes.retrieval import RetrievalNode
from .nodes.synthesis import SynthesisNode
from .nodes.hallucination import HallucinationCheckerNode
from .nodes.answer_grader import AnswerGraderNode
from .tools.tavily_search import web_search_node

class MVPWorkflow:
    """MVP RAG ì›Œí¬í”Œë¡œìš° (ë©”íƒ€ë°ì´í„° í™œìš© ë²„ì „)"""
    
    def __init__(self, db_connection_string: str):
        self.db_connection = db_connection_string
        self._build_graph()
    
    def _build_graph(self):
        """ì›Œí¬í”Œë¡œìš° ê·¸ë˜í”„ êµ¬ì„±"""
        
        # StateGraph ìƒì„±
        workflow = StateGraph(MVPWorkflowState)
        
        # ë…¸ë“œ ì´ˆê¸°í™”
        planning_node = PlanningAgentNode()
        executor_node = SubtaskExecutorNode(self.db_connection)  # DB ë©”íƒ€ë°ì´í„° í™œìš© ë° Multi-Query í¬í•¨
        retrieval_node = RetrievalNode(self.db_connection)  # Dual Search Strategy í¬í•¨
        synthesis_node = SynthesisNode()
        hallucination_node = HallucinationCheckerNode()
        grader_node = AnswerGraderNode()
        
        # ë…¸ë“œ ì¶”ê°€
        workflow.add_node("planning", planning_node)
        workflow.add_node("executor", executor_node)
        workflow.add_node("retrieval", retrieval_node)
        workflow.add_node("synthesis", synthesis_node)
        workflow.add_node("hallucination", hallucination_node)
        workflow.add_node("grader", grader_node)
        workflow.add_node("web_search", web_search_node)
        
        # ì—£ì§€ ì •ì˜
        workflow.add_edge(START, "planning")
        workflow.add_edge("planning", "executor")
        
        # ì„œë¸ŒíƒœìŠ¤í¬ ì‹¤í–‰ ë£¨í”„ (ìˆœì°¨ì )
        workflow.add_conditional_edges(
            "executor",
            self._should_continue_subtasks,
            {
                "continue": "retrieval",
                "done": "synthesis"
            }
        )
        
        workflow.add_edge("retrieval", "executor")
        workflow.add_edge("synthesis", "hallucination")
        
        # í™˜ê° ì²´í¬ í›„ ë¼ìš°íŒ…
        workflow.add_conditional_edges(
            "hallucination",
            self._check_hallucination,
            {
                "pass": "grader",
                "use_web": "web_search"
            }
        )
        
        workflow.add_edge("web_search", "synthesis")
        
        # ë‹µë³€ í‰ê°€ í›„ ë¼ìš°íŒ…
        workflow.add_conditional_edges(
            "grader",
            self._check_grade,
            {
                "accept": END,
                "revise": "synthesis",
                "restart": "planning"
            }
        )
        
        # ê·¸ë˜í”„ ì»´íŒŒì¼
        self.graph = workflow.compile()
    
    def _should_continue_subtasks(self, state: MVPWorkflowState) -> Literal["continue", "done"]:
        """ì„œë¸ŒíƒœìŠ¤í¬ ê³„ì† ì‹¤í–‰ ì—¬ë¶€ ê²°ì •"""
        subtasks = state.get("subtasks", [])
        current_idx = state.get("current_subtask_idx", 0)
        
        # ë‹¤ìŒ ì„œë¸ŒíƒœìŠ¤í¬ë¡œ ì´ë™
        next_idx = current_idx + 1
        
        if next_idx < len(subtasks):
            state["current_subtask_idx"] = next_idx
            return "continue"
        
        return "done"
    
    def _check_hallucination(self, state: MVPWorkflowState) -> Literal["pass", "use_web"]:
        """í™˜ê° ì²´í¬ ê²°ê³¼ í‰ê°€"""
        hallucination_check = state.get("hallucination_check", {})
        confidence_score = state.get("confidence_score", 0.0)
        documents = state.get("documents", [])
        
        # ë¬¸ì„œê°€ ìˆì§€ë§Œ ì‹ ë¢°ë„ê°€ ë‚®ì„ ë•Œ ì›¹ ê²€ìƒ‰
        if documents and (
            not hallucination_check.get("is_grounded", False) or 
            confidence_score < 0.5
        ):
            return "use_web"
        
        return "pass"
    
    def _check_grade(self, state: MVPWorkflowState) -> Literal["accept", "revise", "restart"]:
        """ë‹µë³€ í’ˆì§ˆ í‰ê°€ ê²°ê³¼ í™•ì¸"""
        grade = state.get("answer_grade", {})
        iteration_count = state.get("iteration_count", 0)
        
        overall_score = grade.get("overall_score", 0.0)
        
        # ì ìˆ˜ê°€ ë†’ìœ¼ë©´ ìˆ˜ë½
        if overall_score >= 0.7:
            return "accept"
        
        # ë°˜ë³µ íšŸìˆ˜ ì œí•œ
        if iteration_count >= 3:
            return "accept"  # ê°•ì œ ì¢…ë£Œ
        
        # ì ìˆ˜ê°€ ì¤‘ê°„ì´ë©´ ìˆ˜ì •
        if overall_score >= 0.4:
            state["iteration_count"] = iteration_count + 1
            return "revise"
        
        # ì ìˆ˜ê°€ ë‚®ìœ¼ë©´ ì¬ì‹œì‘
        state["iteration_count"] = iteration_count + 1
        return "restart"
    
    async def run(self, query: str) -> Dict:
        """ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ (ì¼ë°˜ ì‹¤í–‰)"""
        
        # ì´ˆê¸° ìƒíƒœ
        initial_state = {
            "query": query,
            "subtasks": [],
            "current_subtask_idx": 0,
            "subtask_results": [],
            "query_variations": [],
            "documents": [],
            "iteration_count": 0,
            "should_use_web": False,
            "confidence_score": 0.0,
            "metadata": {}
        }
        
        try:
            # LangGraph recursion limit ì„¤ì •
            config = {"recursion_limit": 50}
            
            # ì›Œí¬í”Œë¡œìš° ì‹¤í–‰
            result = await self.graph.ainvoke(initial_state, config)
            
            return {
                "answer": result.get("final_answer", ""),
                "confidence": result.get("confidence_score", 0.0),
                "documents_used": len(result.get("documents", [])),
                "web_search_used": result.get("should_use_web", False)
            }
            
        except GraphRecursionError:
            print("ì›Œí¬í”Œë¡œìš°ê°€ ìµœëŒ€ ë°˜ë³µ íšŸìˆ˜(50)ì— ë„ë‹¬í–ˆìŠµë‹ˆë‹¤.")
            return {
                "answer": "ì²˜ë¦¬ ì¤‘ ì‹œê°„ ì´ˆê³¼ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.",
                "error": "recursion_limit_exceeded"
            }
        except Exception as e:
            print(f"ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
            return {
                "answer": "ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.",
                "error": str(e)
            }
    
    async def stream(self, query: str, stream_mode: str = "updates"):
        """ì›Œí¬í”Œë¡œìš° ìŠ¤íŠ¸ë¦¬ë° ì‹¤í–‰
        
        Args:
            query: ì‚¬ìš©ì ì¿¼ë¦¬
            stream_mode: ìŠ¤íŠ¸ë¦¼ ëª¨ë“œ
                - "values": ì „ì²´ ìƒíƒœ ìŠ¤íŠ¸ë¦¬ë°
                - "updates": ì—…ë°ì´íŠ¸ë§Œ ìŠ¤íŠ¸ë¦¬ë°
                - "debug": ë””ë²„ê·¸ ì •ë³´ í¬í•¨
        
        Yields:
            ìƒíƒœ ì—…ë°ì´íŠ¸ ë˜ëŠ” ê°’
        """
        
        # ì´ˆê¸° ìƒíƒœ
        initial_state = {
            "query": query,
            "subtasks": [],
            "current_subtask_idx": 0,
            "subtask_results": [],
            "query_variations": [],
            "documents": [],
            "iteration_count": 0,
            "should_use_web": False,
            "confidence_score": 0.0,
            "metadata": {}
        }
        
        try:
            # LangGraph recursion limit ì„¤ì •
            config = {"recursion_limit": 50}
            
            # ìŠ¤íŠ¸ë¦¬ë° ì‹¤í–‰
            async for event in self.graph.astream(initial_state, config, stream_mode=stream_mode):
                yield event
                
        except GraphRecursionError:
            yield {
                "error": "recursion_limit_exceeded",
                "message": "ì›Œí¬í”Œë¡œìš°ê°€ ìµœëŒ€ ë°˜ë³µ íšŸìˆ˜(50)ì— ë„ë‹¬í–ˆìŠµë‹ˆë‹¤."
            }
        except Exception as e:
            yield {
                "error": str(e),
                "message": f"ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}"
            }
```

---

## ğŸš€ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸

### 3.1 ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”

```python
# scripts/1_setup_database.py
import asyncio
import asyncpg
import os
from dotenv import load_dotenv

load_dotenv()

async def setup_database():
    """MVP ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”"""
    
    # DB ì—°ê²°
    conn = await asyncpg.connect(
        host=os.getenv("DB_HOST", "localhost"),
        port=int(os.getenv("DB_PORT", 5432)),
        user=os.getenv("DB_USER"),
        password=os.getenv("DB_PASSWORD"),
        database=os.getenv("DB_NAME")
    )
    
    # pgvector í™•ì¥ í™œì„±í™”
    await conn.execute("CREATE EXTENSION IF NOT EXISTS vector")
    
    # MVP í…Œì´ë¸” ìƒì„±
    await conn.execute("""
        CREATE TABLE IF NOT EXISTS mvp_ddu_documents (
            id SERIAL PRIMARY KEY,
            source TEXT NOT NULL,
            page INTEGER,
            category TEXT NOT NULL,
            page_content TEXT,
            translation_text TEXT,
            contextualize_text TEXT,
            caption TEXT,
            entity JSONB,
            image_path TEXT,
            human_feedback TEXT DEFAULT '',
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            embedding_korean vector(1536),
            embedding_english vector(1536),
            search_vector_korean tsvector,
            search_vector_english tsvector
        )
    """)
    
    # ì¸ë±ìŠ¤ ìƒì„±
    await conn.execute("""
        CREATE INDEX IF NOT EXISTS idx_korean_embedding 
        ON mvp_ddu_documents USING ivfflat (embedding_korean vector_cosine_ops)
    """)
    
    await conn.execute("""
        CREATE INDEX IF NOT EXISTS idx_english_embedding
        ON mvp_ddu_documents USING ivfflat (embedding_english vector_cosine_ops)
    """)
    
    await conn.execute("""
        CREATE INDEX IF NOT EXISTS idx_korean_fts
        ON mvp_ddu_documents USING gin(search_vector_korean)
    """)
    
    await conn.execute("""
        CREATE INDEX IF NOT EXISTS idx_english_fts
        ON mvp_ddu_documents USING gin(search_vector_english)
    """)
    
    await conn.close()
    print("âœ… ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ")

if __name__ == "__main__":
    asyncio.run(setup_database())
```

### 3.2 ë¬¸ì„œ ì¸ì œìŠ¤íŠ¸

```python
# scripts/2_ingest_documents.py
import asyncio
import pickle
import asyncpg
from pathlib import Path
from dotenv import load_dotenv
import os

load_dotenv()

# í”„ë¡œì íŠ¸ ê²½ë¡œ ì¶”ê°€
import sys
sys.path.append(str(Path(__file__).parent.parent))

from ingest.embeddings import DualLanguageEmbeddings
from tqdm.asyncio import tqdm

async def ingest_documents():
    """DDU ë¬¸ì„œ ì¸ì œìŠ¤íŠ¸"""
    
    # Pickle íŒŒì¼ ê²½ë¡œ
    pickle_path = "/mnt/e/MyProject2/multimodal-rag-wsl/data/gv80_owners_manual_TEST6P_documents.pkl"
    
    # íŒŒì¼ ë¡œë“œ
    print(f"ğŸ“ Loading: {pickle_path}")
    with open(pickle_path, 'rb') as f:
        documents = pickle.load(f)
    
    print(f"âœ… Loaded {len(documents)} documents")
    
    # DB ì—°ê²°
    conn = await asyncpg.connect(
        host=os.getenv("DB_HOST", "localhost"),
        port=int(os.getenv("DB_PORT", 5432)),
        user=os.getenv("DB_USER"),
        password=os.getenv("DB_PASSWORD"),
        database=os.getenv("DB_NAME")
    )
    
    # ì„ë² ë”© ìƒì„±ê¸°
    embeddings = DualLanguageEmbeddings()
    
    # ë°°ì¹˜ ì²˜ë¦¬
    batch_size = 10
    for i in tqdm(range(0, len(documents), batch_size)):
        batch = documents[i:i+batch_size]
        
        for doc in batch:
            # ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
            metadata = doc.metadata
            
            # ì„ë² ë”© ìƒì„±
            korean_emb, english_emb = await embeddings.embed_document(metadata)
            
            # DB ì €ì¥
            await conn.execute("""
                INSERT INTO mvp_ddu_documents (
                    source, page, category, page_content,
                    translation_text, contextualize_text, caption,
                    entity, image_path, embedding_korean, embedding_english,
                    search_vector_korean, search_vector_english
                ) VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10, $11,
                         to_tsvector('simple', COALESCE($12, '')),
                         to_tsvector('english', COALESCE($13, '')))
            """,
                metadata.get("source"),
                metadata.get("page"),
                metadata.get("category"),
                doc.page_content,
                metadata.get("translation_text"),
                metadata.get("contextualize_text"),
                metadata.get("caption"),
                metadata.get("entity"),
                metadata.get("image_path"),
                korean_emb,
                english_emb,
                metadata.get("contextualize_text", "") + " " + doc.page_content,
                metadata.get("translation_text", "")
            )
    
    await conn.close()
    print("âœ… ì¸ì œìŠ¤íŠ¸ ì™„ë£Œ")

if __name__ == "__main__":
    asyncio.run(ingest_documents())
```

### 3.3 ì›Œí¬í”Œë¡œìš° í…ŒìŠ¤íŠ¸

```python
# scripts/3_test_workflow.py
import asyncio
import os
from dotenv import load_dotenv
import sys
from pathlib import Path

load_dotenv()

# í”„ë¡œì íŠ¸ ê²½ë¡œ ì¶”ê°€
sys.path.append(str(Path(__file__).parent.parent))

from workflow.graph import MVPWorkflow

async def test_workflow():
    """ì›Œí¬í”Œë¡œìš° í…ŒìŠ¤íŠ¸"""
    
    # DB ì—°ê²° ë¬¸ìì—´
    db_connection = (
        f"postgresql://{os.getenv('DB_USER')}:{os.getenv('DB_PASSWORD')}"
        f"@{os.getenv('DB_HOST')}:{os.getenv('DB_PORT')}/{os.getenv('DB_NAME')}"
    )
    
    # ì›Œí¬í”Œë¡œìš° ì´ˆê¸°í™”
    workflow = MVPWorkflow(db_connection)
    
    # í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬ë“¤
    test_queries = [
        "GV80ì˜ ì—°ë¹„ëŠ” ì–´ë–»ê²Œ ë˜ë‚˜ìš”?",
        "What are the safety features of GV80?",
        "GV80ì˜ ì—”ì§„ ì‚¬ì–‘ê³¼ ì„±ëŠ¥ì— ëŒ€í•´ ì•Œë ¤ì£¼ì„¸ìš”"
    ]
    
    for query in test_queries:
        print(f"\n{'='*60}")
        print(f"Query: {query}")
        print(f"{'='*60}")
        
        # ì›Œí¬í”Œë¡œìš° ì‹¤í–‰
        result = await workflow.run(query)
        
        print(f"\nğŸ“ Answer:")
        print(result["answer"])
        print(f"\nğŸ“Š Metadata:")
        print(f"- Confidence: {result.get('confidence', 0):.2%}")
        print(f"- Documents Used: {result.get('documents_used', 0)}")
        print(f"- Web Search Used: {result.get('web_search_used', False)}")
        
        if result.get("error"):
            print(f"âš ï¸ Error: {result['error']}")

if __name__ == "__main__":
    asyncio.run(test_workflow())
```

### 3.4 ìŠ¤íŠ¸ë¦¬ë° í…ŒìŠ¤íŠ¸

```python
# scripts/4_test_streaming.py
import asyncio
import os
from dotenv import load_dotenv
import sys
from pathlib import Path
import json

load_dotenv()

# í”„ë¡œì íŠ¸ ê²½ë¡œ ì¶”ê°€
sys.path.append(str(Path(__file__).parent.parent))

from workflow.graph import MVPWorkflow

async def test_streaming():
    """ì›Œí¬í”Œë¡œìš° ìŠ¤íŠ¸ë¦¬ë° í…ŒìŠ¤íŠ¸"""
    
    # DB ì—°ê²° ë¬¸ìì—´
    db_connection = (
        f"postgresql://{os.getenv('DB_USER')}:{os.getenv('DB_PASSWORD')}"
        f"@{os.getenv('DB_HOST')}:{os.getenv('DB_PORT')}/{os.getenv('DB_NAME')}"
    )
    
    # ì›Œí¬í”Œë¡œìš° ì´ˆê¸°í™”
    workflow = MVPWorkflow(db_connection)
    
    query = "GV80ì˜ ì£¼ìš” ì•ˆì „ ê¸°ëŠ¥ê³¼ ì—°ë¹„ë¥¼ ì•Œë ¤ì£¼ì„¸ìš”"
    
    print(f"\n{'='*60}")
    print(f"Query: {query}")
    print(f"{'='*60}\n")
    
    # 1. updates ëª¨ë“œ í…ŒìŠ¤íŠ¸
    print("ğŸ“Š Stream Mode: updates")
    print("-" * 40)
    
    async for event in workflow.stream(query, stream_mode="updates"):
        if "error" in event:
            print(f"âŒ Error: {event['error']}")
            break
            
        # ë…¸ë“œ ì—…ë°ì´íŠ¸ í‘œì‹œ
        for node, update in event.items():
            print(f"âœ… Node: {node}")
            
            # ì£¼ìš” ì •ë³´ë§Œ í‘œì‹œ
            if isinstance(update, dict):
                if "subtasks" in update:
                    print(f"   - Subtasks: {len(update['subtasks'])} created")
                if "documents" in update:
                    print(f"   - Documents: {len(update['documents'])} retrieved")
                if "final_answer" in update:
                    print(f"   - Answer: {update['final_answer'][:100]}...")
                if "confidence_score" in update:
                    print(f"   - Confidence: {update['confidence_score']:.2%}")
    
    print("\n" + "="*60)
    
    # 2. values ëª¨ë“œ í…ŒìŠ¤íŠ¸
    print("\nğŸ“Š Stream Mode: values")
    print("-" * 40)
    
    last_state = None
    async for state in workflow.stream(query, stream_mode="values"):
        if "error" in state:
            print(f"âŒ Error: {state['error']}")
            break
            
        last_state = state
        
        # ìƒíƒœ ìš”ì•½ í‘œì‹œ
        print(f"ğŸ“ State Update:")
        print(f"   - Subtasks: {len(state.get('subtasks', []))}")
        print(f"   - Documents: {len(state.get('documents', []))}")
        print(f"   - Iteration: {state.get('iteration_count', 0)}")
    
    # ìµœì¢… ê²°ê³¼ í‘œì‹œ
    if last_state and "final_answer" in last_state:
        print(f"\nğŸ“ Final Answer:")
        print(last_state["final_answer"])
        print(f"\nğŸ“Š Final Confidence: {last_state.get('confidence_score', 0):.2%}")

async def test_streaming_with_callbacks():
    """ì½œë°±ì„ ì‚¬ìš©í•œ ìŠ¤íŠ¸ë¦¬ë° í…ŒìŠ¤íŠ¸"""
    
    db_connection = (
        f"postgresql://{os.getenv('DB_USER')}:{os.getenv('DB_PASSWORD')}"
        f"@{os.getenv('DB_HOST')}:{os.getenv('DB_PORT')}/{os.getenv('DB_NAME')}"
    )
    
    workflow = MVPWorkflow(db_connection)
    query = "GV80 ì°¨ëŸ‰ì˜ íŠ¹ì§•ì„ ìš”ì•½í•´ì£¼ì„¸ìš”"
    
    print(f"\n{'='*60}")
    print(f"Query: {query}")
    print(f"{'='*60}\n")
    
    # ì§„í–‰ ìƒí™© ì¶”ì 
    progress = {
        "planning": False,
        "retrieval": False,
        "synthesis": False,
        "validation": False
    }
    
    async for event in workflow.stream(query, stream_mode="updates"):
        if "error" in event:
            print(f"âŒ Error: {event['error']}")
            break
        
        # ë…¸ë“œë³„ ì²˜ë¦¬
        for node, update in event.items():
            if node == "planning" and not progress["planning"]:
                print("ğŸ¯ Planning phase started...")
                progress["planning"] = True
                
            elif node == "retrieval" and not progress["retrieval"]:
                print("ğŸ” Retrieving documents...")
                progress["retrieval"] = True
                
            elif node == "synthesis" and not progress["synthesis"]:
                print("âœï¸ Generating answer...")
                progress["synthesis"] = True
                
            elif node in ["hallucination", "grader"] and not progress["validation"]:
                print("âœ… Validating answer quality...")
                progress["validation"] = True
            
            # ìµœì¢… ë‹µë³€ ì²˜ë¦¬
            if isinstance(update, dict) and "final_answer" in update:
                print("\n" + "="*60)
                print("ğŸ“ Final Answer:")
                print(update["final_answer"])
                print(f"\nğŸ“Š Confidence: {update.get('confidence_score', 0):.2%}")

if __name__ == "__main__":
    print("ğŸš€ Starting streaming tests...\n")
    
    # ê¸°ë³¸ ìŠ¤íŠ¸ë¦¬ë° í…ŒìŠ¤íŠ¸
    asyncio.run(test_streaming())
    
    # ì½œë°± ê¸°ë°˜ ìŠ¤íŠ¸ë¦¬ë° í…ŒìŠ¤íŠ¸
    asyncio.run(test_streaming_with_callbacks())
```

### 3.5 ë…¸ë“œë³„ ë‹¨ìœ„ í…ŒìŠ¤íŠ¸

#### 3.5.1 Subtask Executor í…ŒìŠ¤íŠ¸

```python
# scripts/test_nodes/test_subtask_executor.py
import asyncio
import os
from dotenv import load_dotenv
import sys
from pathlib import Path

load_dotenv()
sys.path.append(str(Path(__file__).parent.parent.parent))

from workflow.nodes.subtask_executor import SubtaskExecutorNode

async def test_query_extraction():
    """ì¿¼ë¦¬ ì •ë³´ ì¶”ì¶œ í…ŒìŠ¤íŠ¸"""
    
    test_queries = [
        "3í˜ì´ì§€ì˜ ì•ˆì „ ê¸°ëŠ¥ í‘œë¥¼ ë³´ì—¬ì¤˜",
        "ì—”ì§„ ì„±ëŠ¥ì— ëŒ€í•œ ì°¨íŠ¸ê°€ ìˆë‚˜ìš”?",
        "10-15í˜ì´ì§€ ì‚¬ì´ì˜ ëª¨ë“  ê·¸ë¦¼",
        "ì—°ë¹„ ì •ë³´ê°€ ìˆëŠ” í‘œ"
    ]
    
    db_connection = (
        f"postgresql://{os.getenv('DB_USER')}:{os.getenv('DB_PASSWORD')}"
        f"@{os.getenv('DB_HOST')}:{os.getenv('DB_PORT')}/{os.getenv('DB_NAME')}"
    )
    
    node = SubtaskExecutorNode(db_connection)
    
    for query in test_queries:
        print(f"\n{'='*50}")
        print(f"Query: {query}")
        print('='*50)
        
        # Mock state
        state = {
            "subtasks": [{"id": "test_1", "query": query}],
            "current_subtask_idx": 0
        }
        
        result = await node(state)
        
        extracted = result["metadata"]["extracted_info"]
        filter_generated = result["metadata"]["current_filter"]
        
        print(f"\nğŸ“ Extracted Information:")
        print(f"  - Pages: {extracted.get('page_numbers', [])}")
        print(f"  - Entity Type: {extracted.get('entity_type')}")
        print(f"  - Keywords: {extracted.get('keywords', [])}")
        
        print(f"\nğŸ” Generated Filter:")
        print(f"  - Categories: {filter_generated.get('categories', [])}")
        print(f"  - Pages: {filter_generated.get('pages', [])}")
        print(f"  - Entity: {filter_generated.get('entity', {})}")

async def test_metadata_usage():
    """DB ë©”íƒ€ë°ì´í„° í™œìš© í…ŒìŠ¤íŠ¸"""
    
    db_connection = (
        f"postgresql://{os.getenv('DB_USER')}:{os.getenv('DB_PASSWORD')}"
        f"@{os.getenv('DB_HOST')}:{os.getenv('DB_PORT')}/{os.getenv('DB_NAME')}"
    )
    
    node = SubtaskExecutorNode(db_connection)
    
    # ë©”íƒ€ë°ì´í„° ì¡°íšŒ
    metadata = await node.metadata_helper.get_basic_metadata()
    
    print("\nğŸ“Š Database Metadata:")
    print(f"  - Sources: {metadata['sources'][:3]}...")
    print(f"  - Page Range: {metadata['page_min']} - {metadata['page_max']}")
    print(f"  - Categories: {metadata['categories']}")
    print(f"  - Entity Types: {metadata['entity_types']}")

if __name__ == "__main__":
    print("ğŸ§ª Testing Subtask Executor Node")
    asyncio.run(test_query_extraction())
    asyncio.run(test_metadata_usage())
```

#### 3.5.2 Dual Search Strategy í…ŒìŠ¤íŠ¸

```python
# scripts/test_nodes/test_dual_search.py
import asyncio
import os
from dotenv import load_dotenv
import sys
from pathlib import Path

load_dotenv()
sys.path.append(str(Path(__file__).parent.parent.parent))

from workflow.nodes.retrieval import RetrievalNode

async def test_dual_search_strategy():
    """ì´ì¤‘ ê²€ìƒ‰ ì „ëµ í…ŒìŠ¤íŠ¸"""
    
    db_connection = (
        f"postgresql://{os.getenv('DB_USER')}:{os.getenv('DB_PASSWORD')}"
        f"@{os.getenv('DB_HOST')}:{os.getenv('DB_PORT')}/{os.getenv('DB_NAME')}"
    )
    
    node = RetrievalNode(db_connection)
    
    # í…ŒìŠ¤íŠ¸ í•„í„°ë“¤
    test_filters = [
        {
            "name": "Entity + General",
            "filter": {
                "categories": ["paragraph", "list"],
                "pages": [5],
                "entity": {"keywords": ["ì•ˆì „", "ê¸°ëŠ¥"]}
            }
        },
        {
            "name": "Entity Only",
            "filter": {
                "entity": {"type": "table", "keywords": ["ì—°ë¹„"]}
            }
        },
        {
            "name": "General Only",
            "filter": {
                "categories": ["paragraph"],
                "pages": [1, 2, 3]
            }
        }
    ]
    
    for test_case in test_filters:
        print(f"\n{'='*50}")
        print(f"Test: {test_case['name']}")
        print('='*50)
        
        state = {
            "query_variations": ["í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬"],
            "metadata": {"current_filter": test_case['filter']}
        }
        
        result = await node(state)
        documents = result["documents"]
        
        # ê²€ìƒ‰ íƒ€ì…ë³„ ë¶„ë¥˜
        general_docs = [d for d in documents if d.metadata.get("search_type") == "general"]
        entity_docs = [d for d in documents if d.metadata.get("search_type") == "entity"]
        
        print(f"\nğŸ“Š Search Results:")
        print(f"  - Total Documents: {len(documents)}")
        print(f"  - General Search: {len(general_docs)} docs")
        print(f"  - Entity Search: {len(entity_docs)} docs")
        
        # ì¹´í…Œê³ ë¦¬ë³„ ë¶„í¬
        categories = {}
        for doc in documents:
            cat = doc.metadata.get("category")
            categories[cat] = categories.get(cat, 0) + 1
        
        print(f"\nğŸ“‚ Category Distribution:")
        for cat, count in categories.items():
            print(f"  - {cat}: {count}")

if __name__ == "__main__":
    print("ğŸ§ª Testing Dual Search Strategy")
    asyncio.run(test_dual_search_strategy())
```

#### 3.5.3 ì „ì²´ ë…¸ë“œ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ê¸°

```python
# scripts/test_nodes/run_all_tests.py
import asyncio
import sys
from pathlib import Path
import importlib
import traceback

async def run_all_node_tests():
    """ëª¨ë“  ë…¸ë“œ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
    
    test_modules = [
        ("test_planning_agent", "Planning Agent"),
        ("test_subtask_executor", "Subtask Executor"),
        ("test_dual_search", "Dual Search Strategy"),
        ("test_retrieval", "Retrieval Node"),
        ("test_synthesis", "Synthesis Node"),
        ("test_hallucination", "Hallucination Checker"),
        ("test_answer_grader", "Answer Grader"),
        ("test_metadata_helper", "Metadata Helper")
    ]
    
    results = {}
    
    for module_name, display_name in test_modules:
        print(f"\n{'='*60}")
        print(f"ğŸ§ª Testing: {display_name}")
        print('='*60)
        
        try:
            # ë™ì  import
            module = importlib.import_module(module_name)
            
            # main í•¨ìˆ˜ê°€ ìˆìœ¼ë©´ ì‹¤í–‰
            if hasattr(module, 'main'):
                await module.main()
            else:
                print(f"âš ï¸ No main() function in {module_name}")
            
            results[display_name] = "âœ… PASSED"
            
        except ImportError as e:
            results[display_name] = f"âš ï¸ SKIPPED (not implemented)"
            print(f"Module not found: {e}")
            
        except Exception as e:
            results[display_name] = f"âŒ FAILED"
            print(f"Error: {e}")
            traceback.print_exc()
    
    # ê²°ê³¼ ìš”ì•½
    print(f"\n{'='*60}")
    print("ğŸ“Š TEST SUMMARY")
    print('='*60)
    
    passed = sum(1 for r in results.values() if "PASSED" in r)
    failed = sum(1 for r in results.values() if "FAILED" in r)
    skipped = sum(1 for r in results.values() if "SKIPPED" in r)
    
    for name, result in results.items():
        print(f"{name:30} {result}")
    
    print(f"\nğŸ“ˆ Statistics:")
    print(f"  - Passed: {passed}/{len(test_modules)}")
    print(f"  - Failed: {failed}/{len(test_modules)}")
    print(f"  - Skipped: {skipped}/{len(test_modules)}")
    
    # Exit code
    return 0 if failed == 0 else 1

if __name__ == "__main__":
    exit_code = asyncio.run(run_all_node_tests())
    sys.exit(exit_code)
```

---

## ğŸ”§ í™˜ê²½ ì„¤ì •

### .env íŒŒì¼

```env
# OpenAI
OPENAI_API_KEY=your_openai_api_key_here
OPENAI_MODEL=gpt-4o-mini
OPENAI_EMBEDDING_MODEL=text-embedding-3-small

# PostgreSQL
DB_HOST=localhost
DB_PORT=5432
DB_NAME=multimodal_rag
DB_USER=multimodal_user
DB_PASSWORD=multimodal_pass123

# Tavily Search
TAVILY_API_KEY=your_tavily_api_key_here

# LangSmith (Optional)
LANGCHAIN_TRACING_V2=true
LANGCHAIN_API_KEY=your_langsmith_api_key_here
LANGCHAIN_PROJECT=mvp-rag
```

### requirements.txt

```txt
# Core
langchain>=0.3.14
langchain-core>=0.3.72
langgraph>=0.3.6
langchain-openai>=0.3.1

# Database
asyncpg>=0.29.0
pgvector>=0.2.5
psycopg2-binary>=2.9.10

# Korean NLP
kiwipiepy>=0.21.0

# Search
tavily-python>=0.6.0

# Utils
python-dotenv>=1.0.1
pydantic>=2.10.4
tqdm>=4.66.0
numpy>=2.0.2

# Development
pytest>=8.4.1
pytest-asyncio>=1.1.0
```

---

## ğŸ“ˆ ì„±ëŠ¥ ìµœì í™” ì „ëµ

### 1. ì¸ë±ì‹± ìµœì í™”
- IVFFlat ì¸ë±ìŠ¤ì˜ lists íŒŒë¼ë¯¸í„° ì¡°ì • (í˜„ì¬: 100)
- ë¬¸ì„œ ìˆ˜ì— ë”°ë¼ ë™ì ìœ¼ë¡œ ì¡°ì •: `lists = sqrt(n_documents)`

### 2. ë°°ì¹˜ ì²˜ë¦¬
- ì¸ì œìŠ¤íŠ¸: 10ê°œ ë¬¸ì„œì”© ë°°ì¹˜ ì²˜ë¦¬
- ê²€ìƒ‰: ì¿¼ë¦¬ ë³€í˜•ì„ ë³‘ë ¬ë¡œ ì²˜ë¦¬ (asyncio.gather)

### 3. ìºì‹± ì „ëµ
- LLM ì‘ë‹µ ìºì‹± (ë™ì¼ ì¿¼ë¦¬)
- ì„ë² ë”© ìºì‹± (ìì£¼ ì‚¬ìš©ë˜ëŠ” ì¿¼ë¦¬)

### 4. RRF íŒŒë¼ë¯¸í„° íŠœë‹
- k ê°’ ì¡°ì • (í˜„ì¬: 60)
- ë°ì´í„°ì…‹ì— ë”°ë¼ ìµœì ê°’ ì‹¤í—˜

---

## ğŸ§ª í…ŒìŠ¤íŠ¸ ê³„íš

### ë‹¨ìœ„ í…ŒìŠ¤íŠ¸
1. SearchFilter ë³€í™˜ í…ŒìŠ¤íŠ¸
2. RRF ë³‘í•© ë¡œì§ í…ŒìŠ¤íŠ¸
3. ê° ë…¸ë“œ ê°œë³„ í…ŒìŠ¤íŠ¸

### í†µí•© í…ŒìŠ¤íŠ¸
1. ì¸ì œìŠ¤íŠ¸ íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸
2. í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
3. ì›Œí¬í”Œë¡œìš° ì¢…ë‹¨ê°„ í…ŒìŠ¤íŠ¸

### ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
1. ê²€ìƒ‰ ì†ë„ ì¸¡ì •
2. LLM ì‘ë‹µ ì‹œê°„ ì¸¡ì •
3. ì „ì²´ ì›Œí¬í”Œë¡œìš° ì²˜ë¦¬ ì‹œê°„

---

## ğŸ“… ê°œë°œ ì¼ì •

### Week 1: ì¸í”„ë¼ êµ¬ì¶•
- [x] DB ìŠ¤í‚¤ë§ˆ ì„¤ê³„
- [x] í™˜ê²½ ì„¤ì •
- [x] UV ì˜ì¡´ì„± ì„¤ì • (2025-01-10)
- [x] í”„ë¡œì íŠ¸ ë””ë ‰í† ë¦¬ êµ¬ì¡° ìƒì„± (2025-01-10)
- [x] ê¸°ë³¸ ëª¨ë¸ êµ¬í˜„ (2025-01-10)
- [x] ì¸ì œìŠ¤íŠ¸ íŒŒì´í”„ë¼ì¸ (2025-01-10)

## ğŸš§ ì‘ì—… ì§„í–‰ ìƒí™© (2025-01-11)

### âœ… Phase 1 ì™„ë£Œ (2025-01-10)
- **Ingest Module**
  - database.py - DB ì—°ê²° ë° í…Œì´ë¸” ìƒì„± âœ…
  - models.py - ê°„ì†Œí™”ëœ DDU ëª¨ë¸ (14ê°œ ì¹´í…Œê³ ë¦¬) âœ…
  - embeddings.py - ì´ì¤‘ ì–¸ì–´ ì„ë² ë”© ì²˜ë¦¬ âœ…
  - loader.py - Pickle íŒŒì¼ ë¡œë” âœ…
  
- **Retrieval Module**
  - search_filter.py - MVP SearchFilter (5ê°œ í•„ë“œ) âœ…
  - hybrid_search.py - RRF ê¸°ë°˜ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ âœ…
  - Kiwi í•œêµ­ì–´ í† í¬ë‚˜ì´ì € í†µí•© âœ…
  - ë²„ê·¸ ìˆ˜ì •: HybridSearch ë²¡í„° ë³€í™˜, í•œêµ­ì–´ í‚¤ì›Œë“œ ì¶”ì¶œ, asyncpg í˜¸í™˜ì„± âœ…

### âœ… Phase 2 ì™„ë£Œ (2025-01-11)
- **Workflow Module (LangGraph)**
  - state.py - ì›Œí¬í”Œë¡œìš° ìƒíƒœ ì •ì˜ âœ…
  - planning_agent.py - ê³„íš ìˆ˜ë¦½ ë…¸ë“œ (ì„œë¸ŒíƒœìŠ¤í¬ ìƒì„±) âœ…
  - subtask_executor.py - ì„œë¸ŒíƒœìŠ¤í¬ ì‹¤í–‰ ë…¸ë“œ (ì¿¼ë¦¬ ë³€í˜•, í•„í„° ìƒì„±) âœ…
  - retrieval.py - ê²€ìƒ‰ ë…¸ë“œ (Multi-Query, ì´ì¤‘ ì–¸ì–´ ê²€ìƒ‰) âœ…
  - synthesis.py - ë‹µë³€ ìƒì„± ë…¸ë“œ âœ…
  - hallucination.py - í™˜ê° ì²´í¬ ë…¸ë“œ (CRAG) âœ…
  - answer_grader.py - ë‹µë³€ í‰ê°€ ë…¸ë“œ (CRAG) âœ…
  - tavily_search.py - ì›¹ ê²€ìƒ‰ ë„êµ¬ âœ…
  - graph.py - ë©”ì¸ ì›Œí¬í”Œë¡œìš° ê·¸ë˜í”„ âœ…
  
- **ê°œì„ ì‚¬í•­**
  - State ì˜ì¡´ì„± ë¬¸ì œ í•´ê²° (current_subtask_idx, retry_count ê´€ë¦¬)
  - Multi-Query êµ¬í˜„ (asyncio.gatherë¡œ ë³‘ë ¬ ì²˜ë¦¬)
  - ì¿¼ë¦¬ ë³€í˜• í•„ìˆ˜í™” (ì—†ìœ¼ë©´ ì—ëŸ¬ ë°œìƒ)
  - DB ë©”íƒ€ë°ì´í„° ìµœì í™” (ë¶€ì •í™•í•œ ì •ë³´ ì œê±°)
  - í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§ ê°•í™” (ë³´ìˆ˜ì  í•„í„° ìƒì„±)

### âœ… Phase 3 ì™„ë£Œ (2025-01-11)
- **Scripts**
  - 3_test_workflow.py - ì›Œí¬í”Œë¡œìš° í†µí•© í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸ âœ…
  - ê¸°ë³¸ ì¿¼ë¦¬, ë³µì¡í•œ ì¿¼ë¦¬, ì—ëŸ¬ ì²˜ë¦¬, ìŠ¤íŠ¸ë¦¬ë° í…ŒìŠ¤íŠ¸ í¬í•¨ âœ…

### ğŸ”„ ë‹¤ìŒ ë‹¨ê³„
- [ ] ì‹¤ì œ ë°ì´í„°ë¡œ í†µí•© í…ŒìŠ¤íŠ¸
- [ ] ì„±ëŠ¥ ìµœì í™” ë° íŠœë‹
- [ ] í”„ë¡œë•ì…˜ ë°°í¬ ì¤€ë¹„

### Week 4: í†µí•© ë° í…ŒìŠ¤íŠ¸
- [ ] Tavily ê²€ìƒ‰ í†µí•©
- [ ] ì¢…ë‹¨ê°„ í…ŒìŠ¤íŠ¸
- [ ] ì„±ëŠ¥ íŠœë‹
- [ ] ë¬¸ì„œí™”

---

## ğŸ“ ì£¼ìš” ê²°ì • ì‚¬í•­

1. **RRF ì‚¬ìš©**: ê°„ë‹¨í•˜ê³  íš¨ê³¼ì ì¸ ë³‘í•© ë°©ì‹
2. **ìˆœì°¨ì  ì„œë¸ŒíƒœìŠ¤í¬ ì‹¤í–‰**: MVP ë‹¨ìˆœì„± ìš°ì„ 
3. **ì›¹ ê²€ìƒ‰ íŠ¸ë¦¬ê±°**: ë¬¸ì„œ ì¡´ì¬ + ë‚®ì€ ì‹ ë¢°ë„
4. **Pickle íŒŒì¼ ì§€ì›**: ê¸°ì¡´ ë°ì´í„° í˜•ì‹ ìœ ì§€
5. **Recursion Limit 50**: GraphRecursionError ë°©ì§€

---

## ğŸ¯ ì„±ê³µ ì§€í‘œ

- **ê²€ìƒ‰ ì •í™•ë„**: P@10 > 0.7
- **ì‘ë‹µ ì‹œê°„**: < 5ì´ˆ (í‰ê· )
- **í™˜ê° ë¹„ìœ¨**: < 10%
- **ë‹µë³€ í’ˆì§ˆ**: > 0.7 (í‰ê·  ìŠ¤ì½”ì–´)
- **ì‹œìŠ¤í…œ ì•ˆì •ì„±**: 99% ê°€ë™ë¥ 

---

## ğŸ“š ì°¸ê³  ìë£Œ

- [LangGraph Documentation](https://langchain-ai.github.io/langgraph/)
- [PostgreSQL Full Text Search](https://www.postgresql.org/docs/current/textsearch.html)
- [pgvector Documentation](https://github.com/pgvector/pgvector)
- [Tavily API Documentation](https://tavily.com/docs)
- [CRAG Paper](https://arxiv.org/pdf/2401.15884.pdf)

---

## ğŸ”„ ë²„ì „ íˆìŠ¤í† ë¦¬

| ë²„ì „ | ë‚ ì§œ | ë³€ê²½ì‚¬í•­ |
|------|------|----------|
| 1.0.0 | 2025-01-10 | ì´ˆê¸° PRD ì‘ì„± |

---

## ğŸ“ ë¬¸ì˜ì‚¬í•­

í”„ë¡œì íŠ¸ ê´€ë ¨ ë¬¸ì˜ì‚¬í•­ì´ë‚˜ ì¶”ê°€ ìš”êµ¬ì‚¬í•­ì´ ìˆìœ¼ì‹œë©´ ì–¸ì œë“  ë§ì”€í•´ ì£¼ì„¸ìš”.