# ğŸš€ Multimodal RAG Workflow Enhancement Plan

> **ì‘ì„±ì¼**: 2025-01-14
> **ì‘ì„±ì**: Claude Code Assistant
> **ë²„ì „**: 2.0.0
> **ìƒíƒœ**: ê³„íš ìˆ˜ì • ì™„ë£Œ (ë‹¨ìˆœí™” ë° ìµœì í™”)

## ğŸ“‹ ëª©ì°¨

1. [ê°œìš”](#ê°œìš”)
2. [í˜„í™© ë¶„ì„](#í˜„í™©-ë¶„ì„)
3. [ìš”êµ¬ì‚¬í•­ ì •ë¦¬](#ìš”êµ¬ì‚¬í•­-ì •ë¦¬)
4. [ì•„í‚¤í…ì²˜ ë³€ê²½ ê³„íš](#ì•„í‚¤í…ì²˜-ë³€ê²½-ê³„íš)
5. [êµ¬í˜„ ìƒì„¸ ê³„íš](#êµ¬í˜„-ìƒì„¸-ê³„íš)
6. [í…ŒìŠ¤íŠ¸ ê³„íš](#í…ŒìŠ¤íŠ¸-ê³„íš)
7. [ì‘ì—… ì¶”ì ](#ì‘ì—…-ì¶”ì )

---

## ê°œìš”

### ëª©ì 
100% RAG ì˜ì¡´ì ì¸ ì‹œìŠ¤í…œì„ LLM reasoning ê¸°ë°˜ ì§€ëŠ¥í˜• ë¼ìš°íŒ… ì‹œìŠ¤í…œìœ¼ë¡œ ì—…ê·¸ë ˆì´ë“œ:
- LLMì´ ì¿¼ë¦¬ë¥¼ ë¶„ì„í•˜ì—¬ ì ì ˆí•œ ì²˜ë¦¬ ê²½ë¡œ ê²°ì •
- ëŒ€í™” íˆìŠ¤í† ë¦¬ ì°¸ì¡°ê°€ í•„ìš”í•œ ê²½ìš° ìë™ ì»¨í…ìŠ¤íŠ¸ ë³´ê°•
- Chat UI ìŠ¤íŠ¸ë¦¬ë°ì„ ìœ„í•œ ë©”ì‹œì§€ ê¸°ë°˜ ì•„í‚¤í…ì²˜

### í•µì‹¬ ë³€ê²½ì‚¬í•­ (ìµœì†Œ í•„ìˆ˜ë§Œ)
1. **Query Routing**: LLM reasoningìœ¼ë¡œ 3ê°€ì§€ íƒ€ì… ë¶„ë¥˜ (simple/rag_required/history_required)
2. **Message-based State**: MessagesState ìƒì†ìœ¼ë¡œ ìŠ¤íŠ¸ë¦¬ë° ì§€ì›
3. **Context Enhancement**: íˆìŠ¤í† ë¦¬ ì°¸ì¡° ì¿¼ë¦¬ì˜ ìë™ ê°œì„ 
4. **Domain Update**: Vehicle manual â†’ Automobile manufacturing

---

## í˜„í™© ë¶„ì„

### í˜„ì¬ ë¬¸ì œì 

#### 1. RAG ì˜ì¡´ì„±
- âŒ ëª¨ë“  ì¿¼ë¦¬ê°€ RAG íŒŒì´í”„ë¼ì¸ í†µê³¼
- âŒ "ì•ˆë…•", "ë‚ ì”¨" ê°™ì€ ê°„ë‹¨í•œ ì¿¼ë¦¬ë„ ë¬¸ì„œ ê²€ìƒ‰ ìˆ˜í–‰
- âŒ ë¶ˆí•„ìš”í•œ ë¦¬ì†ŒìŠ¤ ë‚­ë¹„ (í† í°, API í˜¸ì¶œ, ì‹œê°„)

#### 2. ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ ë¶€ì¬
- âŒ ì´ì „ ëŒ€í™” íˆìŠ¤í† ë¦¬ í™œìš© ë¶ˆê°€
- âŒ "ì•„ê¹Œ ë§í•œ ë‚´ìš©"ê°™ì€ ì°¸ì¡° ë¶ˆê°€ëŠ¥
- âŒ ë§¤ ì¿¼ë¦¬ê°€ ë…ë¦½ì ìœ¼ë¡œ ì²˜ë¦¬

#### 3. ë„ë©”ì¸ ì œí•œ
- âš ï¸ í”„ë¡¬í”„íŠ¸ê°€ "vehicle manual"ë¡œ í•˜ë“œì½”ë”©
- âš ï¸ Automobile manufacturing ì „ë°˜ìœ¼ë¡œ í™•ì¥ í•„ìš”

#### 4. ìŠ¤íŠ¸ë¦¬ë° ë¯¸ì§€ì›
- âŒ ì¤‘ê°„ ì§„í–‰ ìƒí™© í‘œì‹œ ë¶ˆê°€
- âŒ Chat UI í†µí•© ì œí•œì 

### ê¸°ìˆ  ìŠ¤íƒ
- **Framework**: LangGraph 0.2.x
- **LLM**: OpenAI GPT-4o-mini
- **Database**: PostgreSQL + pgvector
- **State Management**: MVPWorkflowState (61 fields)
- **Current Entry Point**: PlanningAgentNode

---

## ìš”êµ¬ì‚¬í•­ ì •ë¦¬

### 1. ì¿¼ë¦¬ ë¶„ë¥˜ ì²´ê³„ (LLM Reasoning ê¸°ë°˜)
- **3ê°€ì§€ íƒ€ì…ë§Œ**: simple / rag_required / history_required
- LLMì´ ì¿¼ë¦¬ì™€ ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ë¥¼ ë³´ê³  reasoningí•˜ì—¬ ë¶„ë¥˜
- ë‹¨ìˆœ ê·œì¹™ì´ë‚˜ íŒ¨í„´ ë§¤ì¹­ ì‚¬ìš© ì•ˆ í•¨

### 2. Messages State í†µí•© (ìµœì†Œ ë³€ê²½)
- MessagesState ìƒì†ìœ¼ë¡œ messages í•„ë“œ ìë™ í¬í•¨
- ê° ë…¸ë“œì—ì„œ AIMessage ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
- ê¸°ì¡´ í•„ë“œëŠ” ìµœëŒ€í•œ ìœ ì§€

### 3. ë…¸ë“œ êµ¬í˜„ ë°©í–¥
- **QueryRouterNode**: LLM reasoningìœ¼ë¡œ ì¿¼ë¦¬ íƒ€ì… íŒë‹¨
- **DirectResponseNode**: ë‹¨ìˆœ LLM í˜¸ì¶œë§Œ (í…œí”Œë¦¿ ì—†ìŒ)
- **ContextEnhancementNode**: íˆìŠ¤í† ë¦¬ ì°¸ì¡° í•´ê²°

### 4. ìœ ì§€ë³´ìˆ˜ ìµœì í™”
- í™˜ê²½ë³€ìˆ˜ ì¶”ê°€ ìµœì†Œí™” (ê¸°ì¡´ ê²ƒ í™œìš©)
- State í•„ë“œ ì¶”ê°€ ìµœì†Œí™” (í•„ìˆ˜ë§Œ)
- ì½”ë“œ ë³€ê²½ ìµœì†Œí™” (í•µì‹¬ë§Œ)

---

## ì•„í‚¤í…ì²˜ ë³€ê²½ ê³„íš

### ì „ì²´ ì›Œí¬í”Œë¡œìš° êµ¬ì¡°

```mermaid
graph TD
    START --> QueryRouter{LLM Query Router}
    
    QueryRouter -->|simple| DirectResponse
    QueryRouter -->|history_required| ContextEnhancement
    QueryRouter -->|rag_required| Planning
    
    DirectResponse --> END
    ContextEnhancement --> Planning
    
    Planning --> SubtaskExecutor
    SubtaskExecutor --> Retrieval
    Retrieval --> SubtaskExecutor
    SubtaskExecutor -->|complete| Synthesis
    
    Synthesis --> HallucinationCheck
    HallucinationCheck -->|valid| AnswerGrader
    HallucinationCheck -->|retry| Synthesis
    AnswerGrader -->|accept| END
    AnswerGrader -->|retry| Synthesis
    
    style QueryRouter fill:#ff9999,stroke:#333,stroke-width:4px
    style DirectResponse fill:#99ff99,stroke:#333,stroke-width:2px
    style ContextEnhancement fill:#9999ff,stroke:#333,stroke-width:2px
```

### ì¿¼ë¦¬ íƒ€ì…ë³„ ì²˜ë¦¬ í”Œë¡œìš°

1. **simple (ë‹¨ìˆœ ì¿¼ë¦¬)**
   - ì¸ì‚¬, ì¼ë°˜ ëŒ€í™”, ì¼ë°˜ ì§€ì‹ ì§ˆë¬¸
   - Flow: QueryRouter â†’ DirectResponse(LLM í˜¸ì¶œ) â†’ END

2. **rag_required (RAG í•„ìš”)**
   - ìë™ì°¨ ì œì¡° ê´€ë ¨ ê¸°ìˆ  ì§ˆë¬¸
   - Flow: QueryRouter â†’ Planning â†’ RAG Pipeline â†’ END

3. **history_required (íˆìŠ¤í† ë¦¬ ì°¸ì¡°)**
   - "ì•„ê¹Œ ë§í•œ", "ì´ì „ì— ì–¸ê¸‰í•œ" ë“± ì°¸ì¡° í•„ìš”
   - Flow: QueryRouter â†’ ContextEnhancement â†’ Planning â†’ RAG Pipeline â†’ END
   - ì²˜ë¦¬: LLMì´ ëŒ€í™” íˆìŠ¤í† ë¦¬ë¥¼ ë³´ê³  ì°¸ì¡° í•´ê²° â†’ ìê¸°ì™„ê²°ì  ì¿¼ë¦¬ ìƒì„±

### State ë³€ê²½ì‚¬í•­ (ìµœì†Œ í•„ìˆ˜ë§Œ)

```python
from langgraph.graph import MessagesState
from typing import List, Dict, Optional, Annotated, Any
from langchain_core.documents import Document
from operator import add

class MVPWorkflowState(MessagesState):
    """MessagesState ìƒì†ìœ¼ë¡œ messages í•„ë“œ ìë™ í¬í•¨"""
    
    # === ê¸°ì¡´ í•„ë“œë“¤ ëª¨ë‘ ìœ ì§€ (61ê°œ) ===
    query: str
    subtasks: List[Dict[str, Any]]
    current_subtask_idx: int
    documents: Annotated[List[Document], add]
    # ... (ë‚˜ë¨¸ì§€ ê¸°ì¡´ í•„ë“œë“¤)
    
    # === í•„ìˆ˜ ì¶”ê°€ í•„ë“œë§Œ (3ê°œ) ===
    query_type: Optional[str] = None  # simple/rag_required/history_required
    enhanced_query: Optional[str] = None  # ì»¨í…ìŠ¤íŠ¸ ê°œì„ ëœ ì¿¼ë¦¬
    current_node: Optional[str] = None  # ë””ë²„ê¹…ìš©
```

---

## êµ¬í˜„ ìƒì„¸ ê³„íš

### Phase 1: State ë° ê¸°ë³¸ êµ¬ì¡° ë³€ê²½

#### 1.1 State í´ë˜ìŠ¤ ìˆ˜ì • (`workflow/state.py`)

```python
from langgraph.graph import MessagesState
from langchain_core.messages import BaseMessage, AIMessage, HumanMessage

class MVPWorkflowState(MessagesState):
    """
    MessagesStateë¥¼ ìƒì†í•˜ì—¬ messages í•„ë“œ ìë™ í¬í•¨
    Chat UI ìŠ¤íŠ¸ë¦¬ë° ì§€ì›
    """
    # ê¸°ì¡´ í•„ë“œë“¤...
    
    # ìƒˆë¡œìš´ í•„ë“œ
    query_type: Optional[str] = None
    requires_context: bool = False
    session_id: Optional[str] = None
    streaming_enabled: bool = True
    current_node: Optional[str] = None
```

#### 1.2 í”„ë¡¬í”„íŠ¸ ìˆ˜ì • (Vehicle Manual â†’ Automobile Manufacturing)

**ìˆ˜ì • ëŒ€ìƒ íŒŒì¼ ë° ìœ„ì¹˜**:

1. **planning_agent.py:56**
   ```python
   # ë³€ê²½ ì „
   "You are a query planning expert for a RAG system about vehicle manuals"
   # ë³€ê²½ í›„
   "You are a query planning expert for a RAG system about automobile manufacturing documents including manuals, technical specifications, reports, and industry documentation"
   ```

2. **synthesis.py:49**
   ```python
   # ë³€ê²½ ì „
   "You are an expert assistant for a vehicle manual RAG system"
   # ë³€ê²½ í›„
   "You are an expert assistant for an automobile manufacturing RAG system handling diverse technical documents"
   ```

3. **answer_grader.py:56**
   ```python
   # ë³€ê²½ ì „
   "You are a quality evaluator for a RAG system specializing in vehicle manuals"
   # ë³€ê²½ í›„
   "You are a quality evaluator for a RAG system specializing in automobile manufacturing documentation"
   ```

4. **subtask_executor.py:131, 157, 179**
   - ëª¨ë“  "vehicle manual" â†’ "automobile manufacturing documents"

### Phase 2: Query Router Node êµ¬í˜„ (LLM Reasoning)

#### 2.1 Query Router Node (`workflow/nodes/query_router.py`)

```python
from langchain_core.messages import AIMessage, HumanMessage
from langchain_openai import ChatOpenAI
from pydantic import BaseModel, Field
from typing import Literal, Dict, Any, List, Optional
import logging
import os

logger = logging.getLogger(__name__)

class QueryClassification(BaseModel):
    """LLMì´ ì¶”ë¡ í•œ ì¿¼ë¦¬ ë¶„ë¥˜ ê²°ê³¼"""
    type: Literal[
        "simple",              # RAG ë¶ˆí•„ìš”, LLM ì§ì ‘ ë‹µë³€ ê°€ëŠ¥
        "rag_required",        # ìë™ì°¨ ì œì¡° ë¬¸ì„œ ê²€ìƒ‰ í•„ìš”
        "history_required"     # ì´ì „ ëŒ€í™” ì°¸ì¡° í•„ìš”
    ]
    reasoning: str  # LLMì˜ ì¶”ë¡  ê³¼ì •
    confidence: float = Field(ge=0.0, le=1.0)

class QueryRouterNode:
    """LLM reasoningìœ¼ë¡œ ì¿¼ë¦¬ë¥¼ ë¶„ë¥˜í•˜ê³  ë¼ìš°íŒ…í•˜ëŠ” ë…¸ë“œ"""
    
    def __init__(self):
        self.llm = ChatOpenAI(
            model=os.getenv("OPENAI_MODEL", "gpt-4o-mini"),
            temperature=0
        )
        
        self.classification_prompt = ChatPromptTemplate.from_messages([
            ("system", """You are an intelligent query classifier using reasoning, not pattern matching.

Analyze the query and recent conversation to determine the type:

1. **simple**: Query that can be answered with general knowledge without searching documents
   - Greetings, casual chat, general knowledge questions
   - Questions unrelated to automobile manufacturing
   
2. **rag_required**: Query that needs to search automobile manufacturing documents
   - Technical specifications, manufacturing processes
   - Quality standards, safety procedures
   - Any domain-specific information
   
3. **history_required**: Query that references previous conversation
   - Contains references like "ì´ì „ì—", "ì•„ê¹Œ", "ìœ„ì—ì„œ", "that", "it", etc.
   - Needs context from earlier messages to be fully understood
   - After resolving references, might still need RAG

Use reasoning to decide, not keyword matching. Consider:
- Does this require domain-specific knowledge from documents?
- Are there unresolved references to previous conversation?
- Can I answer this with general knowledge alone?

Provide your reasoning process in the 'reasoning' field."""),
            ("human", """Query: {query}

Recent messages (for context):
{recent_messages}

Classify this query.""")
        ])
    
    async def __call__(self, state: MVPWorkflowState) -> Dict[str, Any]:
        """ë…¸ë“œ ì‹¤í–‰"""
        logger.info(f"[QUERY_ROUTER] Node started")
        
        try:
            query = state["query"]
            messages = state.get("messages", [])
            
            # ì§„í–‰ ìƒí™© ë©”ì‹œì§€
            progress_messages = [
                AIMessage(content="ğŸ” ì¿¼ë¦¬ë¥¼ ë¶„ì„í•˜ê³  ìˆìŠµë‹ˆë‹¤...")
            ]
            
            # ìµœê·¼ ë©”ì‹œì§€ ì»¨í…ìŠ¤íŠ¸ ì¤€ë¹„ (ìµœëŒ€ 5ê°œ)
            recent_messages = []
            for msg in messages[-10:]:  # ìµœê·¼ 10ê°œì—ì„œ
                if isinstance(msg, (HumanMessage, AIMessage)):
                    recent_messages.append(f"{msg.__class__.__name__}: {msg.content[:100]}")
            recent_context = "\n".join(recent_messages) if recent_messages else "No previous messages"
            
            # LLMìœ¼ë¡œ ë¶„ë¥˜
            structured_llm = self.llm.with_structured_output(QueryClassification)
            classification = await structured_llm.ainvoke(
                self.classification_prompt.format_messages(
                    query=query,
                    recent_messages=recent_context
                )
            )
            
            logger.info(f"[QUERY_ROUTER] Classification: {classification.type} (confidence: {classification.confidence:.2f})")
            
            # ì™„ë£Œ ë©”ì‹œì§€
            type_emoji = {
                "greeting": "ğŸ‘‹",
                "chitchat": "ğŸ’¬",
                "weather": "ğŸŒ¤ï¸",
                "follow_up": "ğŸ”„",
                "automobile_manufacturing": "ğŸ­"
            }
            
            progress_messages.append(
                AIMessage(content=f"{type_emoji.get(classification.type, 'â“')} ì¿¼ë¦¬ íƒ€ì…: {classification.type}")
            )
            
            # State ì—…ë°ì´íŠ¸
            result = {
                "messages": progress_messages,
                "query_type": classification.type,
                "requires_context": classification.requires_context,
                "current_node": "query_router",
                "metadata": {
                    **state.get("metadata", {}),
                    "query_classification": {
                        "type": classification.type,
                        "confidence": classification.confidence,
                        "reasoning": classification.reasoning,
                        "suggested_response": classification.suggested_response
                    }
                }
            }
            
            logger.info(f"[QUERY_ROUTER] Node completed successfully")
            return result
            
        except Exception as e:
            logger.error(f"[QUERY_ROUTER] Failed: {str(e)}")
            return {
                "messages": [
                    AIMessage(content=f"âŒ ì¿¼ë¦¬ ë¶„ë¥˜ ì‹¤íŒ¨: {str(e)}")
                ],
                "error": str(e),
                "query_type": "automobile_manufacturing"  # ê¸°ë³¸ê°’
            }
    
    def invoke(self, state: MVPWorkflowState) -> Dict[str, Any]:
        """ë™ê¸° ì‹¤í–‰ (LangGraph í˜¸í™˜)"""
        import asyncio
        return asyncio.run(self.__call__(state))
```

### Phase 3: Direct Response Node êµ¬í˜„ (ë‹¨ìˆœ LLM í˜¸ì¶œ)

#### 3.1 Direct Response Node (`workflow/nodes/direct_response.py`)

```python
class DirectResponseNode:
    """ë‹¨ìˆœ ì¿¼ë¦¬ì— ëŒ€í•´ LLMì´ ì§ì ‘ ì‘ë‹µí•˜ëŠ” ë…¸ë“œ"""
    
    def __init__(self):
        self.llm = ChatOpenAI(
            model=os.getenv("OPENAI_MODEL", "gpt-4o-mini"),
            temperature=0.7  # ìì—°ìŠ¤ëŸ¬ìš´ ëŒ€í™”ë¥¼ ìœ„í•´ ì•½ê°„ ë†’ì„
        )
        
        self.response_prompt = ChatPromptTemplate.from_messages([
            ("system", """You are a helpful AI assistant. 
            Answer the user's query directly and naturally.
            If asked about automobile manufacturing, mention that you can help with technical questions if they search the documentation.
            Keep responses concise and friendly."""),
            ("human", "{query}")
        ])
    
    async def __call__(self, state: MVPWorkflowState) -> Dict[str, Any]:
        """ë…¸ë“œ ì‹¤í–‰ - ë‹¨ìˆœ LLM í˜¸ì¶œ"""
        logger.info(f"[DIRECT_RESPONSE] Node started")
        
        query = state.get("query", "")
        
        messages = [
            AIMessage(content="ğŸ’¬ ì‘ë‹µì„ ìƒì„±í•˜ê³  ìˆìŠµë‹ˆë‹¤...")
        ]
        
        # LLM í˜¸ì¶œí•˜ì—¬ ì‘ë‹µ ìƒì„±
        response = await self.llm.ainvoke(
            self.response_prompt.format_messages(query=query)
        )
        
        messages.append(
            AIMessage(content=response.content)
        )
        
        logger.info(f"[DIRECT_RESPONSE] Generated response")
        
        return {
            "messages": messages,
            "final_answer": response.content,
            "workflow_status": "completed",
            "current_node": "direct_response"
        }
    
    def invoke(self, state: MVPWorkflowState) -> Dict[str, Any]:
        """ë™ê¸° ì‹¤í–‰"""
        import asyncio
        return asyncio.run(self.__call__(state))
```

### Phase 4: Context Enhancement Node êµ¬í˜„ (LLM ê¸°ë°˜)

#### 4.1 Context Enhancement Node (`workflow/nodes/context_enhancement.py`)

```python
class ContextEnhancementNode:
    """LLMì´ ëŒ€í™” íˆìŠ¤í† ë¦¬ë¥¼ ë³´ê³  ì°¸ì¡°ë¥¼ í•´ê²°í•˜ëŠ” ë…¸ë“œ"""
    
    def __init__(self):
        self.llm = ChatOpenAI(
            model=os.getenv("OPENAI_MODEL", "gpt-4o-mini"),
            temperature=0
        )
        
        self.enhancement_prompt = ChatPromptTemplate.from_messages([
            ("system", """You are analyzing a follow-up query that references previous conversation.

Your task: Create a self-contained query by resolving references using conversation history.

Process:
1. Identify what the user is referring to (ì´ì „ì—, ì•„ê¹Œ, that, it, etc.)
2. Find the relevant context from previous messages
3. Create a complete query that doesn't need context to understand
4. Preserve the original intent and question

Important:
- Only add necessary context, don't over-elaborate
- Keep technical terms accurate
- The enhanced query should be searchable in documents

Example:
- History: "GV80 ì—”ì§„ì˜ ì¡°ë¦½ ê³µì •ì„ ì•Œë ¤ì¤˜"
- Follow-up: "ê±°ê¸°ì— í•„ìš”í•œ ë„êµ¬ëŠ”?"
- Enhanced: "GV80 ì—”ì§„ ì¡°ë¦½ ê³µì •ì— í•„ìš”í•œ ë„êµ¬"
"""),
            ("human", """Original query: {query}

Recent conversation:
{conversation_history}

Enhance this follow-up query with necessary context:""")
        ])
    
    async def __call__(self, state: MVPWorkflowState) -> Dict[str, Any]:
        """ë…¸ë“œ ì‹¤í–‰"""
        logger.info(f"[CONTEXT_ENHANCEMENT] Node started")
        
        try:
            query = state["query"]
            messages = state.get("messages", [])
            
            progress_messages = [
                AIMessage(content="ğŸ”„ ì´ì „ ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ë¥¼ ë¶„ì„í•˜ê³  ìˆìŠµë‹ˆë‹¤...")
            ]
            
            # ëŒ€í™” íˆìŠ¤í† ë¦¬ ì¤€ë¹„
            conversation_history = []
            for msg in messages[-10:]:  # ìµœê·¼ 10ê°œ ë©”ì‹œì§€
                if isinstance(msg, HumanMessage):
                    conversation_history.append(f"User: {msg.content}")
                elif isinstance(msg, AIMessage) and not msg.content.startswith("ğŸ”„"):
                    # ì§„í–‰ ìƒí™© ë©”ì‹œì§€ ì œì™¸
                    conversation_history.append(f"Assistant: {msg.content[:200]}")
            
            history_text = "\n".join(conversation_history) if conversation_history else "No previous conversation"
            
            # ì¿¼ë¦¬ ê°œì„ 
            response = await self.llm.ainvoke(
                self.enhancement_prompt.format_messages(
                    query=query,
                    conversation_history=history_text
                )
            )
            
            enhanced_query = response.content
            logger.info(f"[CONTEXT_ENHANCEMENT] Enhanced: '{query}' â†’ '{enhanced_query}'")
            
            progress_messages.append(
                AIMessage(content=f"âœ… ì»¨í…ìŠ¤íŠ¸ ì ìš© ì™„ë£Œ: {enhanced_query}")
            )
            
            return {
                "messages": progress_messages,
                "query": enhanced_query,  # ì›ë³¸ ì¿¼ë¦¬ë¥¼ ê°œì„ ëœ ì¿¼ë¦¬ë¡œ êµì²´
                "current_node": "context_enhancement",
                "metadata": {
                    **state.get("metadata", {}),
                    "context_enhancement": {
                        "original_query": query,
                        "enhanced_query": enhanced_query,
                        "history_used": len(conversation_history)
                    }
                }
            }
            
        except Exception as e:
            logger.error(f"[CONTEXT_ENHANCEMENT] Failed: {str(e)}")
            return {
                "messages": [
                    AIMessage(content=f"âš ï¸ ì»¨í…ìŠ¤íŠ¸ ê°œì„  ì‹¤íŒ¨, ì›ë³¸ ì¿¼ë¦¬ ì‚¬ìš©: {query}")
                ],
                "current_node": "context_enhancement"
            }
    
    def invoke(self, state: MVPWorkflowState) -> Dict[str, Any]:
        """ë™ê¸° ì‹¤í–‰"""
        import asyncio
        return asyncio.run(self.__call__(state))
```

### Phase 5: ê¸°ì¡´ ë…¸ë“œë“¤ì˜ Messages ì§€ì› ì¶”ê°€

#### 5.1 ê° ë…¸ë“œì— ìŠ¤íŠ¸ë¦¬ë° ë©”ì‹œì§€ ì¶”ê°€

**PlanningAgentNode ìˆ˜ì • ì˜ˆì‹œ**:
```python
async def __call__(self, state: MVPWorkflowState) -> Dict[str, Any]:
    logger.info(f"[PLANNING] Node started")
    
    # ìŠ¤íŠ¸ë¦¬ë° ë©”ì‹œì§€ ì¶”ê°€
    messages = [
        AIMessage(content="ğŸ”„ ì¿¼ë¦¬ë¥¼ ë¶„ì„í•˜ì—¬ ì‘ì—… ê³„íšì„ ìˆ˜ë¦½í•˜ê³  ìˆìŠµë‹ˆë‹¤...")
    ]
    
    try:
        query = state["query"]
        # ... ê¸°ì¡´ ë¡œì§ ...
        
        # ì„œë¸ŒíƒœìŠ¤í¬ ìƒì„± í›„
        messages.append(
            AIMessage(content=f"ğŸ“‹ {len(subtasks)}ê°œì˜ ì„œë¸ŒíƒœìŠ¤í¬ë¡œ ë¶„í•´ ì™„ë£Œ")
        )
        
        # ì„œë¸ŒíƒœìŠ¤í¬ ìƒì„¸ ì •ë³´
        task_list = "\n".join([
            f"  {i+1}. {task['query'][:50]}..." 
            for i, task in enumerate(subtasks)
        ])
        messages.append(
            AIMessage(content=f"ğŸ“ ì‘ì—… ê³„íš:\n{task_list}")
        )
        
        result = {
            "messages": messages,  # ë©”ì‹œì§€ ì¶”ê°€
            "subtasks": subtasks,
            "current_subtask_idx": 0,
            "metadata": metadata,
            "workflow_status": "running",
            "current_node": "planning"
        }
        
        return result
    except Exception as e:
        # ... ì—ëŸ¬ ì²˜ë¦¬ ...
```

**ëª¨ë“  ë…¸ë“œì— ë™ì¼í•œ íŒ¨í„´ ì ìš©**:
1. SubtaskExecutorNode
2. RetrievalNode
3. SynthesisNode
4. HallucinationCheckNode
5. AnswerGraderNode

### Phase 6: Graph ì¬êµ¬ì„± (ìµœì†Œ ë³€ê²½)

#### 6.1 Graph ìˆ˜ì • (`workflow/graph.py`)

```python
from workflow.nodes.query_router import QueryRouterNode
from workflow.nodes.direct_response import DirectResponseNode
from workflow.nodes.context_enhancement import ContextEnhancementNode

class MVPWorkflowGraph:
    def __init__(self, checkpointer_path: Optional[str] = None):
        # ê¸°ì¡´ ë…¸ë“œë“¤ ìœ ì§€...
        
        # Query Routing í™œì„±í™” ì²´í¬
        self.enable_routing = os.getenv("ENABLE_QUERY_ROUTING", "true").lower() == "true"
        
        if self.enable_routing:
            # ìƒˆë¡œìš´ ë…¸ë“œ ì¶”ê°€
            self.query_router = QueryRouterNode()
            self.direct_response = DirectResponseNode()
            self.context_enhancement = ContextEnhancementNode()
        
        # ê¸°ì¡´ checkpointer ë¡œì§ ìœ ì§€
        if checkpointer_path:
            self.checkpointer = SqliteSaver.from_conn_string(checkpointer_path)
        else:
            self.checkpointer = None
        
        # ê·¸ë˜í”„ êµ¬ì„±
        self.graph = self._build_graph()
        
        # ê¸°ì¡´ recursion limit ë¡œì§ ìœ ì§€
        max_subtasks = int(os.getenv("LANGGRAPH_PLANNING_MAX_SUBTASKS", "5"))
        max_retries = int(os.getenv("CRAG_MAX_RETRIES", "3"))
        recursion_limit = (max_subtasks * 3) + (max_retries * 4) + 10
        
        if self.enable_routing:
            recursion_limit += 5  # ë¼ìš°íŒ… ë…¸ë“œë“¤ ê³ ë ¤
        
        self.app = self.graph.compile(
            checkpointer=self.checkpointer
        ).with_config(recursion_limit=recursion_limit)
    
    def _build_graph(self) -> StateGraph:
        """ì›Œí¬í”Œë¡œìš° ê·¸ë˜í”„ êµ¬ì„±"""
        workflow = StateGraph(MVPWorkflowState)
        
        if self.enable_routing:
            # === ë¼ìš°íŒ… í™œì„±í™” ì‹œ ===
            # ìƒˆë¡œìš´ ë…¸ë“œë“¤
            workflow.add_node("query_router", self.query_router.invoke)
            workflow.add_node("direct_response", self.direct_response.invoke)
            workflow.add_node("context_enhancement", self.context_enhancement.invoke)
            
            # ì—”íŠ¸ë¦¬ í¬ì¸íŠ¸ë¥¼ query_routerë¡œ
            workflow.set_entry_point("query_router")
            
            # ì¡°ê±´ë¶€ ë¼ìš°íŒ…
            def route_query(state: MVPWorkflowState) -> str:
                """ì¿¼ë¦¬ íƒ€ì…ì— ë”°ë¥¸ ë¼ìš°íŒ…"""
                query_type = state.get("query_type", "rag_required")
                
                if query_type == "simple":
                    return "direct_response"
                elif query_type == "history_required":
                    return "context_enhancement"
                else:  # rag_required
                    return "planning"
            
            workflow.add_conditional_edges(
                "query_router",
                route_query,
                {
                    "direct_response": "direct_response",
                    "context_enhancement": "context_enhancement",
                    "planning": "planning"
                }
            )
            
            # Direct ResponseëŠ” ë°”ë¡œ ì¢…ë£Œ
            workflow.add_edge("direct_response", END)
            
            # Context EnhancementëŠ” Planningìœ¼ë¡œ (enhanced_query ì‚¬ìš©)
            workflow.add_edge("context_enhancement", "planning")
        else:
            # === ë¼ìš°íŒ… ë¹„í™œì„±í™” ì‹œ (ê¸°ì¡´ ë™ì‘) ===
            workflow.set_entry_point("planning")
        
        # === ê¸°ì¡´ ë…¸ë“œë“¤ (ê³µí†µ) ===
        workflow.add_node("planning", self.planning_node.invoke)
        workflow.add_node("subtask_executor", self.subtask_executor.invoke)
        workflow.add_node("retrieval", self.retrieval_node.invoke)
        workflow.add_node("synthesis", self.synthesis_node.invoke)
        workflow.add_node("hallucination_check", self.hallucination_check.invoke)
        workflow.add_node("answer_grader", self.answer_grader.invoke)
        
        if self.use_tavily:
            workflow.add_node("web_search", self._web_search_node_sync)
        
        # ê¸°ì¡´ ì—£ì§€ë“¤ ìœ ì§€
        workflow.add_edge("planning", "subtask_executor")
        # ... (ë‚˜ë¨¸ì§€ ê¸°ì¡´ ì—£ì§€ êµ¬ì¡°)
        
        return workflow
```

### Phase 7: í™˜ê²½ë³€ìˆ˜ (ìµœì†Œ ì¶”ê°€)

#### 7.1 .env íŒŒì¼ ì—…ë°ì´íŠ¸

```bash
# === ê¸°ì¡´ í™˜ê²½ë³€ìˆ˜ í™œìš© ===
# OPENAI_MODEL=gpt-4o-mini  (ì´ë¯¸ ìˆìŒ)
# OPENAI_API_KEY=sk-...     (ì´ë¯¸ ìˆìŒ)

# === í•„ìˆ˜ ì¶”ê°€ë§Œ ===
ENABLE_QUERY_ROUTING=true  # Query routing ê¸°ëŠ¥ on/off (ê¸°ë³¸ê°’: true)
```

ê¸°ì¡´ í™˜ê²½ë³€ìˆ˜ë“¤ì„ ìµœëŒ€í•œ í™œìš©:
- `OPENAI_MODEL`: ëª¨ë“  LLM ë…¸ë“œì—ì„œ ê³µí†µ ì‚¬ìš©
- `OPENAI_API_KEY`: ì´ë¯¸ ì„¤ì •ë˜ì–´ ìˆìŒ
- `LANGGRAPH_PLANNING_MAX_SUBTASKS`: ì´ë¯¸ ìˆìŒ
- `CRAG_MAX_RETRIES`: ì´ë¯¸ ìˆìŒ

---

## í…ŒìŠ¤íŠ¸ ê³„íš (í•µì‹¬ë§Œ)

### ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸

1. **3ê°€ì§€ ì¿¼ë¦¬ íƒ€ì… í…ŒìŠ¤íŠ¸**
   ```python
   test_queries = [
       "ì•ˆë…•í•˜ì„¸ìš”",  # simple â†’ DirectResponse
       "ì—”ì§„ ì¡°ë¦½ ê³µì •ì„ ì•Œë ¤ì¤˜",  # rag_required â†’ Full RAG
       "ì•„ê¹Œ ë§í•œ ê³µì •ì˜ í’ˆì§ˆ ê¸°ì¤€ì€?",  # history_required â†’ Context + RAG
   ]
   ```

2. **LLM Reasoning ê²€ì¦**
   - Query Routerê°€ LLM reasoningìœ¼ë¡œ ì˜¬ë°”ë¥´ê²Œ ë¶„ë¥˜í•˜ëŠ”ì§€
   - Context Enhancementê°€ ì°¸ì¡°ë¥¼ ì œëŒ€ë¡œ í•´ê²°í•˜ëŠ”ì§€

3. **ë©”ì‹œì§€ ìŠ¤íŠ¸ë¦¬ë° í™•ì¸**
   - ê° ë…¸ë“œê°€ AIMessage ë°˜í™˜í•˜ëŠ”ì§€
   - ìŠ¤íŠ¸ë¦¬ë°ì´ ì‘ë™í•˜ëŠ”ì§€

---

## ì‘ì—… ì¶”ì 

### êµ¬í˜„ ì²´í¬ë¦¬ìŠ¤íŠ¸ (ìš°ì„ ìˆœìœ„ë³„)

#### Phase 1: í•µì‹¬ ê¸°ë°˜ ì‘ì—…
- [ ] Stateì— MessagesState ìƒì† + 3ê°œ í•„ë“œ ì¶”ê°€
- [ ] í”„ë¡¬í”„íŠ¸ ìˆ˜ì • (vehicle manual â†’ automobile manufacturing)
  - [ ] planning_agent.py
  - [ ] synthesis.py  
  - [ ] answer_grader.py
  - [ ] subtask_executor.py

#### Phase 2: Query Routing êµ¬í˜„
- [ ] QueryRouterNode êµ¬í˜„ (LLM reasoning)
- [ ] DirectResponseNode êµ¬í˜„ (ë‹¨ìˆœ LLM í˜¸ì¶œ)
- [ ] ContextEnhancementNode êµ¬í˜„ (íˆìŠ¤í† ë¦¬ ì°¸ì¡° í•´ê²°)

#### Phase 3: ê¸°ì¡´ ë…¸ë“œ ë©”ì‹œì§€ ì§€ì› (ê°„ë‹¨)
- [ ] ê° ë…¸ë“œì— messages ë°˜í™˜ 1-2ì¤„ ì¶”ê°€
  - [ ] PlanningAgentNode
  - [ ] SubtaskExecutorNode
  - [ ] RetrievalNode
  - [ ] SynthesisNode
  - [ ] HallucinationCheckNode
  - [ ] AnswerGraderNode

#### Phase 4: Graph ì¬êµ¬ì„±
- [ ] _build_graph() ë©”ì„œë“œ ìˆ˜ì •
- [ ] ì¡°ê±´ë¶€ ë¼ìš°íŒ… ì¶”ê°€
- [ ] ENABLE_QUERY_ROUTING í”Œë˜ê·¸ ì²˜ë¦¬

#### Phase 5: í…ŒìŠ¤íŠ¸
- [ ] 3ê°€ì§€ ì¿¼ë¦¬ íƒ€ì… í…ŒìŠ¤íŠ¸
- [ ] ìŠ¤íŠ¸ë¦¬ë° ë™ì‘ í™•ì¸

### ì§„í–‰ ìƒí™© ê¸°ë¡

| ë‚ ì§œ | ì‘ì—… ë‚´ìš© | ìƒíƒœ | ë‹´ë‹¹ì | ë¹„ê³  |
|------|----------|------|--------|------|
| 2025-01-14 | ê³„íš ìˆ˜ë¦½ ì™„ë£Œ | âœ… | Claude | ì´ˆê¸° ê³„íš |
| | | | | |
| | | | | |

### ì´ìŠˆ ë° ë¦¬ìŠ¤í¬

1. **ìµœì†Œ ë¦¬ìŠ¤í¬ ì ‘ê·¼**
   - MessagesState ìƒì†ì€ ê¸°ì¡´ í•„ë“œì— ì˜í–¥ ì—†ìŒ (messages í•„ë“œë§Œ ì¶”ê°€)
   - ENABLE_QUERY_ROUTING í”Œë˜ê·¸ë¡œ ê¸°ëŠ¥ on/off ê°€ëŠ¥
   - ê¸°ì¡´ ë™ì‘ ì™„ì „ ë³´ì¡´ (ë¼ìš°íŒ… ë¹„í™œì„±í™” ì‹œ)

2. **LLM ì˜ì¡´ì„±**
   - Query Routerì™€ Context Enhancementê°€ LLM í˜¸ì¶œ ì¶”ê°€
   - ë ˆì´í„´ì‹œ ì•½ê°„ ì¦ê°€ ì˜ˆìƒ (simple ì¿¼ë¦¬ëŠ” ì˜¤íˆë ¤ ë¹¨ë¼ì§)

3. **í…ŒìŠ¤íŠ¸ ì „ëµ**
   - ë¼ìš°íŒ… ë¹„í™œì„±í™” ìƒíƒœë¡œ ë¨¼ì € ë°°í¬
   - ì¶©ë¶„í•œ í…ŒìŠ¤íŠ¸ í›„ í™œì„±í™”

---

## ì°¸ê³  ìë£Œ

### ë‚´ë¶€ ë¬¸ì„œ
- `/LANGGRAPH_CHAT_UI_STREAMING_GUIDE.md` - ìŠ¤íŠ¸ë¦¬ë° ê°€ì´ë“œ
- `/CLAUDE.md` - í”„ë¡œì íŠ¸ ê·œì¹™ ë° ì»¨í…ìŠ¤íŠ¸
- `/workflow/state.py` - í˜„ì¬ State ì •ì˜

### ì™¸ë¶€ ì°¸ì¡°
- [LangGraph Documentation](https://langchain-ai.github.io/langgraph/)
- [LangGraph Adaptive RAG](https://github.com/langchain-ai/langgraph/examples/rag)
- [Chat UI Repository](https://github.com/langchain-ai/agent-chat-ui)

---

## í•µì‹¬ ë³€ê²½ ìš”ì•½

### ë‹¨ìˆœí™”ëœ ì•„í‚¤í…ì²˜
1. **3ê°€ì§€ ì¿¼ë¦¬ íƒ€ì…ë§Œ**: simple / rag_required / history_required
2. **LLM Reasoning ê¸°ë°˜**: ëª¨ë“  íŒë‹¨ì„ LLMì´ ìˆ˜í–‰ (íŒ¨í„´ ë§¤ì¹­ X)
3. **ìµœì†Œ ì½”ë“œ ë³€ê²½**: ê¸°ì¡´ ì‹œìŠ¤í…œ ìœ ì§€í•˜ë©° ì ì§„ì  ê°œì„ 
4. **ë‹¨ì¼ í”Œë˜ê·¸ ì œì–´**: ENABLE_QUERY_ROUTINGìœ¼ë¡œ ì „ì²´ ê¸°ëŠ¥ on/off

### ì£¼ìš” ì¥ì 
- **ìœ ì§€ë³´ìˆ˜ ìš©ì´**: ì½”ë“œ ë³€ê²½ ìµœì†Œí™”, í™˜ê²½ë³€ìˆ˜ ë‹¨ìˆœ
- **ì ì§„ì  ì ìš©**: ê¸°ì¡´ ë™ì‘ ë³´ì¡´í•˜ë©° ìƒˆ ê¸°ëŠ¥ ì¶”ê°€
- **ì„±ëŠ¥ ê°œì„ **: simple ì¿¼ë¦¬ëŠ” RAG íŒŒì´í”„ë¼ì¸ ê±´ë„ˆë›°ì–´ ë¹ ë¥¸ ì‘ë‹µ
- **í™•ì¥ ê°€ëŠ¥**: í–¥í›„ ë” ë³µì¡í•œ ë¼ìš°íŒ… ë¡œì§ ì¶”ê°€ ê°€ëŠ¥

## ë‹¤ìŒ ë‹¨ê³„

1. **Phase 1 ì‹œì‘**: State ìˆ˜ì •ê³¼ í”„ë¡¬í”„íŠ¸ ì—…ë°ì´íŠ¸
2. **Phase 2 êµ¬í˜„**: 3ê°œ ìƒˆ ë…¸ë“œ êµ¬í˜„
3. **í…ŒìŠ¤íŠ¸ í›„ ë°°í¬**: ë¼ìš°íŒ… ë¹„í™œì„±í™” ìƒíƒœë¡œ ë¨¼ì € ë°°í¬

---

*ë¬¸ì„œ ë²„ì „: 2.0.0 (ë‹¨ìˆœí™” ë° ìµœì í™” ì™„ë£Œ)*
*ì‘ì„±ì¼: 2025-01-14*