"""
Retrieval Node
Phase 1ì˜ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì‹œìŠ¤í…œê³¼ ì—°ë™í•˜ì—¬ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•˜ëŠ” ë…¸ë“œ
ì–¸ì–´ ê°ì§€, Dual Search Strategy, ì´ì¤‘ ì–¸ì–´ ê²€ìƒ‰ ì§€ì›
"""

import os
import logging
from typing import Dict, Any, List, Optional
from concurrent.futures import ThreadPoolExecutor
from langchain_core.documents import Document
from langchain_core.messages import AIMessage
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from pydantic import BaseModel, Field
from dotenv import load_dotenv
import time


from workflow.state import MVPWorkflowState, SearchResult
from ingest.database import DatabaseManager
from retrieval.hybrid_search import HybridSearch
from retrieval.search_filter import MVPSearchFilter

load_dotenv()

# ë¡œê¹… ì„¤ì •
logger = logging.getLogger(__name__)


class LanguageDetection(BaseModel):
    """ì–¸ì–´ ê°ì§€ ê²°ê³¼"""
    language: str = Field(description="Detected language: 'korean' or 'english'")
    confidence: float = Field(description="Detection confidence (0.0-1.0)")
    reason: str = Field(description="Reason for detection")


class RerankResult(BaseModel):
    """ë¬¸ì„œ ì¬ìˆœìœ„í™” ê²°ê³¼"""
    ranked_doc_ids: List[str] = Field(
        description="ë¬¸ì„œ ID ë¦¬ìŠ¤íŠ¸ (ê´€ë ¨ì„± ë†’ì€ ìˆœì„œ)"
    )
    reasoning: str = Field(
        description="ìˆœìœ„ ê²°ì • ì´ìœ "
    )


class RetrievalNode:
    """ê²€ìƒ‰ ë…¸ë“œ - Phase 1 ì‹œìŠ¤í…œê³¼ ì—°ë™, ì–¸ì–´ ê°ì§€ ë° Dual Search ì§€ì›"""
    
    def __init__(self):
        """ì´ˆê¸°í™”"""
        self.db_manager = None
        self.hybrid_search = None
        self.initialized = False
        
        # LLM for language detection
        self.llm = ChatOpenAI(
            model=os.getenv("OPENAI_MODEL", "gpt-4o-mini"),
            temperature=0,
            openai_api_key=os.getenv("OPENAI_API_KEY")
        )
        
        # ì–¸ì–´ ê°ì§€ í”„ë¡¬í”„íŠ¸
        self.language_detection_prompt = ChatPromptTemplate.from_messages([
            ("system", """Detect the primary language of the query.
            
Rules:
1. If the query contains Korean characters (í•œê¸€), it's 'korean'
2. If the query is entirely in English, it's 'english'
3. If mixed, determine the dominant language
4. Consider technical terms that might be in English even in Korean queries

Examples:
- "ì•ˆì „ë²¨íŠ¸ ì°©ìš© ë°©ë²•" â†’ korean (confidence: 1.0)
- "How to wear seatbelt" â†’ english (confidence: 1.0)
- "ì—”ì§„ ì˜¤ì¼ êµì²´ ë°©ë²•" â†’ korean (confidence: 0.9, English term 'oil' doesn't change primary language)
- "brake system ì ê²€" â†’ korean (confidence: 0.7, mixed but Korean context)"""),
            ("human", "Query: {query}\n\nDetect the language:")
        ])
        
        # í™˜ê²½ë³€ìˆ˜ì—ì„œ ì„¤ì • ì½ê¸°
        self.default_top_k = int(os.getenv("SEARCH_DEFAULT_TOP_K", "10"))
        self.max_results = int(os.getenv("SEARCH_MAX_RESULTS", "20"))
        
    
    def _initialize(self):
        """ë™ê¸° ì´ˆê¸°í™” (í•œ ë²ˆë§Œ ì‹¤í–‰)"""
        if not self.initialized:
            self.db_manager = DatabaseManager()
            self.db_manager.initialize()
            self.hybrid_search = HybridSearch(self.db_manager.pool)
            self.initialized = True
    
    def _detect_language(self, query: str) -> LanguageDetection:
        """
        ì¿¼ë¦¬ ì–¸ì–´ ê°ì§€
        
        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            
        Returns:
            ì–¸ì–´ ê°ì§€ ê²°ê³¼
        """
        structured_llm = self.llm.with_structured_output(
            LanguageDetection
        )
        
        result = structured_llm.invoke(
            self.language_detection_prompt.format_messages(query=query)
        )
        
        return result
    
    def _dual_search_strategy(
        self,
        query: str,
        filter_dict: Optional[Dict],
        language: str = 'korean',
        top_k: int = 10
    ) -> List[Document]:
        """
        ì´ì¤‘ ê²€ìƒ‰ ì „ëµ ì‹¤í–‰
        
        1. Entity í•„í„°ë¥¼ ë¶„ë¦¬
        2. ì¼ë°˜ í•„í„°ë¡œ ëª¨ë“  ì¹´í…Œê³ ë¦¬ ê²€ìƒ‰
        3. Entity í•„í„°ê°€ ìˆìœ¼ë©´ image/tableë§Œ ì¶”ê°€ ê²€ìƒ‰
        4. ê²°ê³¼ ë³‘í•© (ì¤‘ë³µ ì œê±°)
        
        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            filter_dict: í•„í„° ë”•ì…”ë„ˆë¦¬
            language: ê²€ìƒ‰ ì–¸ì–´
            top_k: ë°˜í™˜í•  ë¬¸ì„œ ìˆ˜
            
        Returns:
            ê²€ìƒ‰ëœ ë¬¸ì„œë“¤
        """
        all_documents = []
        seen_ids = set()
        
        # Entity í•„í„° ë¶„ë¦¬ (ì›ë³¸ dict ë³€ì¡° ë°©ì§€)
        entity_filter = None
        general_filter_dict = {}
        
        if filter_dict:
            entity_filter = filter_dict.get("entity", None)  # pop() ëŒ€ì‹  get() ì‚¬ìš© (ì›ë³¸ ë³´ì¡´)
            # entityë¥¼ ì œì™¸í•œ ë‚˜ë¨¸ì§€ í•„í„°ë§Œ ë³µì‚¬
            general_filter_dict = {k: v for k, v in filter_dict.items() if k != "entity"}
        
        # Entity í•„í„°ê°€ ìˆìœ¼ë©´ entity ê²€ìƒ‰ì„ ìš°ì„ ì ìœ¼ë¡œ ìˆ˜í–‰
        if entity_filter:
            # 1. Entity í•„í„°ë¡œ ë¨¼ì € ê²€ìƒ‰ (ìš°ì„ ìˆœìœ„ ë†’ìŒ)
            entity_filter_dict = general_filter_dict.copy()
            entity_filter_dict["entity"] = entity_filter
            # Entityê°€ ìˆì„ ìˆ˜ ìˆëŠ” ëª¨ë“  ì¹´í…Œê³ ë¦¬ í¬í•¨ (image/table + ë˜‘ë”±ì´ê°€ ìˆëŠ” text ì¹´í…Œê³ ë¦¬)
            entity_filter_dict["categories"] = ["figure", "table", "paragraph", "heading1", "heading2", "heading3"]
            
            entity_search_filter = MVPSearchFilter(**entity_filter_dict)
            
            entity_results = self.hybrid_search.search(
                query=query,
                filter=entity_search_filter,
                language=language,
                top_k=top_k  # ì „ì²´ top_k ì‚¬ìš© (ìš°ì„ ìˆœìœ„)
            )
            
            # Entity ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë¨¼ì € ì¶”ê°€
            for result in entity_results:
                doc_id = result.get("id")
                if doc_id and doc_id not in seen_ids:
                    seen_ids.add(doc_id)
                    all_documents.append(self._convert_to_document(result))
            
            # 2. ì¼ë°˜ í•„í„°ë¡œ ë³´ì¶© ê²€ìƒ‰ (Entity ì—†ì´) - ë¶€ì¡±í•œ ê²½ìš°ì—ë§Œ
            if len(all_documents) < top_k:
                general_filter = MVPSearchFilter(**general_filter_dict) if general_filter_dict else MVPSearchFilter()
                
                general_results = self.hybrid_search.search(
                    query=query,
                    filter=general_filter,
                    language=language,
                    top_k=top_k - len(all_documents)  # ë¶€ì¡±í•œ ë§Œí¼ë§Œ
                )
                
                # ì¼ë°˜ ê²€ìƒ‰ ê²°ê³¼ ì¶”ê°€ (ì¤‘ë³µ ì œê±°)
                for result in general_results:
                    doc_id = result.get("id")
                    if doc_id and doc_id not in seen_ids:
                        seen_ids.add(doc_id)
                        all_documents.append(self._convert_to_document(result))
        else:
            # Entity í•„í„°ê°€ ì—†ìœ¼ë©´ ì¼ë°˜ ê²€ìƒ‰ë§Œ ìˆ˜í–‰
            general_filter = MVPSearchFilter(**general_filter_dict) if general_filter_dict else MVPSearchFilter()
            
            general_results = self.hybrid_search.search(
                query=query,
                filter=general_filter,
                language=language,
                top_k=top_k
            )
            
            # ê²°ê³¼ë¥¼ Documentë¡œ ë³€í™˜
            for result in general_results:
                doc_id = result.get("id")
                if doc_id and doc_id not in seen_ids:
                    seen_ids.add(doc_id)
                    all_documents.append(self._convert_to_document(result))
        
        return all_documents[:top_k]  # ìµœëŒ€ top_kê°œë§Œ ë°˜í™˜
    
    def _bilingual_search(
        self,
        query: str,
        filter_dict: Optional[Dict],
        primary_language: str,
        top_k: int = 10
    ) -> List[Document]:
        """
        ë‹¨ì¼ ì–¸ì–´ ê²€ìƒ‰ (ê°ì§€ëœ ì–¸ì–´ë¡œë§Œ ê²€ìƒ‰)
        
        ê° ì¿¼ë¦¬ë³„ë¡œ ì–¸ì–´ë¥¼ ì •í™•íˆ ê°ì§€í–ˆìœ¼ë¯€ë¡œ, 
        í•´ë‹¹ ì–¸ì–´ë¡œë§Œ ê²€ìƒ‰í•˜ì—¬ ì •í™•ë„ë¥¼ ë†’ì„
        
        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            filter_dict: í•„í„° ë”•ì…”ë„ˆë¦¬
            primary_language: ê°ì§€ëœ ì–¸ì–´
            top_k: ë°˜í™˜í•  ë¬¸ì„œ ìˆ˜
            
        Returns:
            ê²€ìƒ‰ ê²°ê³¼
        """
        # ê°ì§€ëœ ì–¸ì–´ë¡œë§Œ ê²€ìƒ‰ (ì´ì¤‘ ì–¸ì–´ ê²€ìƒ‰ ì œê±°)
        # í•œêµ­ì–´ ì¿¼ë¦¬ëŠ” í•œêµ­ì–´ë¡œë§Œ, ì˜ì–´ ì¿¼ë¦¬ëŠ” ì˜ì–´ë¡œë§Œ ê²€ìƒ‰
        results = self._dual_search_strategy(
            query=query,
            filter_dict=filter_dict,
            language=primary_language,
            top_k=top_k
        )
        
        logger.debug(f"[RETRIEVAL] Single language search completed: {primary_language}, {len(results)} results")
        
        return results
    
    def _convert_to_document(self, result: Dict) -> Document:
        """
        ê²€ìƒ‰ ê²°ê³¼ë¥¼ LangChain Documentë¡œ ë³€í™˜
        
        Args:
            result: ê²€ìƒ‰ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
            
        Returns:
            LangChain Document
        """
        # sourceì™€ page ì •ë³´ ì¶”ì¶œ
        source = result.get("source", "")
        page = result.get("page", 0)
        
        # page_image_path ìƒì„± ë¡œì§
        page_image_path = ""
        if source and page > 0:  # pageê°€ 1 ì´ìƒì¼ ë•Œë§Œ
            # íŒŒì¼ëª… ì¶”ì¶œ (ê²½ë¡œì™€ í™•ì¥ì ì œê±°)
            # ì˜ˆ: "data/gv80_owners_manual_TEST6P.pdf" â†’ "gv80_owners_manual_TEST6P"
            filename = os.path.splitext(os.path.basename(source))[0]
            page_image_path = f"data/images/{filename}-page-{page}.png"
        
        # ë©”íƒ€ë°ì´í„° êµ¬ì„±
        metadata = {
            "source": source,
            "page": page,
            "category": result.get("category", ""),
            "id": result.get("id", ""),
            "caption": result.get("caption", ""),
            "entity": result.get("entity"),
            "page_image_path": page_image_path,  # í˜ì´ì§€ ì´ë¯¸ì§€ ê²½ë¡œ ì¶”ê°€
            "similarity": result.get("similarity"),
            "rank": result.get("rank"),
            "rrf_score": result.get("rrf_score"),
            "human_feedback": result.get("human_feedback", ""),  # Human feedback ì¶”ê°€
            # í†µí•© score í•„ë“œ - Noneì´ ì•„ë‹Œ ê°’ ìš°ì„  (ìš°ì„ ìˆœìœ„: rrf > similarity > rank)
            # RRFëŠ” ì´ë¯¸ ì •ê·œí™”ë¨ (0.0-1.0)
            "score": (
                result.get("rrf_score") or 
                result.get("similarity") or 
                result.get("rank") or 
                0.0
            )
        }
        
        # í˜ì´ì§€ ì½˜í…ì¸  ê²°ì • (ìš°ì„ ìˆœìœ„: contextualize_text > page_content > translation_text)
        page_content = (
            result.get("contextualize_text") or 
            result.get("page_content") or 
            result.get("translation_text") or 
            ""
        )
        
        return Document(
            page_content=page_content,
            metadata=metadata
        )
    
    def _calculate_confidence(self, documents: List[Document]) -> float:
        """
        ê²€ìƒ‰ ê²°ê³¼ì˜ ì‹ ë¢°ë„ ê³„ì‚°
        
        Args:
            documents: ê²€ìƒ‰ëœ ë¬¸ì„œë“¤
            
        Returns:
            ì‹ ë¢°ë„ ì ìˆ˜ (0.0-1.0)
        """
        if not documents:
            return 0.0
        
        confidence = 0.0
        
        # í†µí•© score ê¸°ë°˜ ì‹ ë¢°ë„ ê³„ì‚°
        scores = []
        for doc in documents[:5]:  # ìƒìœ„ 5ê°œë§Œ ê³ ë ¤
            score = doc.metadata.get("score", 0.0)
            if score and score > 0:
                scores.append(score)
        
        if scores:
            avg_score = sum(scores) / len(scores)
            confidence += avg_score

        return min(confidence, 1.0)
    
    def _rerank_documents(self, query: str, documents: List[Document], top_k: int = 20) -> List[Document]:
        """
        LLMì„ ì‚¬ìš©í•œ ë¬¸ì„œ ì¬ìˆœìœ„í™” - ëª¨ë“  ë¬¸ì„œ í‰ê°€
        
        Args:
            query: ì‚¬ìš©ì ì¿¼ë¦¬
            documents: ê²€ìƒ‰ëœ ë¬¸ì„œë“¤
            top_k: ë°˜í™˜í•  ìƒìœ„ ë¬¸ì„œ ìˆ˜
        
        Returns:
            ì¬ìˆœìœ„í™”ëœ ìƒìœ„ ë¬¸ì„œë“¤
        """
        if len(documents) <= top_k:
            return documents
        
        # ë™ì  preview ê¸¸ì´ ê³„ì‚° (í† í° ì œí•œ ê³ ë ¤)
        # ì „ì²´ í† í°ì„ ì•½ 50000ìë¡œ ì œí•œ
        preview_length = min(2000, max(1000, 50000 // len(documents)))
        logger.info(f"[RERANK] Evaluating {len(documents)} docs with {preview_length} chars preview each")
        
        # ëª¨ë“  ë¬¸ì„œ ìš”ì•½ ìƒì„±
        doc_summaries = []
        for i, doc in enumerate(documents):
            summary = {
                "id": doc.metadata.get("id", f"doc_{i}"),
                "page": doc.metadata.get("page", 0),
                "category": doc.metadata.get("category", ""),
                "content_preview": doc.page_content[:preview_length],  # ë™ì  ê¸¸ì´
                "score": doc.metadata.get("score", 0)
            }
            doc_summaries.append(summary)
        
        # Reranking í”„ë¡¬í”„íŠ¸
        rerank_prompt = ChatPromptTemplate.from_messages([
            ("system", """You are a document relevance expert for automotive manuals.
Your task is to rerank ALL provided documents based on their relevance to the user query.

CRITICAL RULES:
1. Evaluate ALL documents, not just a subset
2. Prioritize documents with SPECIFIC information matching the query
3. Deprioritize generic or vague content
4. Consider document category (table, figure, heading1 are often important)
5. Consider original retrieval score as a hint but not absolute
6. Return ONLY the document IDs WITHOUT brackets or prefixes

IMPORTANT: Return IDs exactly as shown between the brackets in [ID: xxx]
For example, if you see [ID: doc_0], return "doc_0"
If you see [ID: gv80_owners_manual_0001], return "gv80_owners_manual_0001"
DO NOT include "[ID: " or "]" in your response."""),
            
            ("human", """Query: {query}

Total documents to evaluate: {doc_count}

Documents:
{documents}

Return the top {top_k} most relevant document IDs in order.
Return ONLY the IDs, without any brackets or prefixes.
Focus on documents that directly answer the query.""")
        ])
        
        # LLMìœ¼ë¡œ ì¬ìˆœìœ„í™”
        llm = ChatOpenAI(
            model=os.getenv("OPENAI_MODEL", "gpt-4o-mini"),
            temperature=0
        )
        
        structured_llm = llm.with_structured_output(RerankResult)
        
        # ë¬¸ì„œ í…ìŠ¤íŠ¸ í¬ë§·íŒ…
        doc_text = "\n".join([
            f"[ID: {d['id']}] Page {d['page']}, {d['category']}, Score: {d['score']:.2f}\nContent: {d['content_preview']}..."
            for d in doc_summaries
        ])
        
        result = structured_llm.invoke(
            rerank_prompt.format_messages(
                query=query,
                doc_count=len(documents),
                documents=doc_text,
                top_k=top_k
            )
        )
        
        # ë””ë²„ê¹…: LLMì´ ë°˜í™˜í•œ IDë“¤ ë¡œê¹…
        logger.info(f"[RERANK] LLM returned {len(result.ranked_doc_ids)} IDs: {result.ranked_doc_ids[:5]}...")
        
        # ì¬ìˆœìœ„í™”ëœ ë¬¸ì„œ ë°˜í™˜
        doc_map = {doc.metadata.get("id", f"doc_{i}"): doc for i, doc in enumerate(documents)}
        
        # ë””ë²„ê¹…: ì‹¤ì œ ë¬¸ì„œ IDë“¤ ë¡œê¹…
        actual_ids = list(doc_map.keys())[:5]
        logger.info(f"[RERANK] Actual document IDs: {actual_ids}...")
        
        reranked_docs = []
        missing_ids = []
        
        for doc_id in result.ranked_doc_ids[:top_k]:
            if doc_id in doc_map:
                reranked_docs.append(doc_map[doc_id])
            else:
                # ID ë§¤ì¹­ ì‹¤íŒ¨ - ë‹¤ì–‘í•œ í˜•ì‹ ì‹œë„
                # 1. ëŒ€ê´„í˜¸ ì œê±°
                cleaned_id = doc_id.strip("[]")
                if cleaned_id in doc_map:
                    reranked_docs.append(doc_map[cleaned_id])
                    continue
                
                # 2. "ID: " í”„ë¦¬í”½ìŠ¤ ì œê±°
                if doc_id.startswith("ID: "):
                    cleaned_id = doc_id[4:]
                    if cleaned_id in doc_map:
                        reranked_docs.append(doc_map[cleaned_id])
                        continue
                
                # 3. "[ID: " í”„ë¦¬í”½ìŠ¤ì™€ "]" ì œê±°
                if doc_id.startswith("[ID: ") and doc_id.endswith("]"):
                    cleaned_id = doc_id[5:-1]
                    if cleaned_id in doc_map:
                        reranked_docs.append(doc_map[cleaned_id])
                        continue
                
                # 4. íƒ€ì… ë³€í™˜ ì‹œë„ - ë¬¸ìì—´ì„ ì •ìˆ˜ë¡œ
                if isinstance(doc_id, str) and doc_id.isdigit():
                    int_id = int(doc_id)
                    if int_id in doc_map:
                        reranked_docs.append(doc_map[int_id])
                        continue
                
                # 5. íƒ€ì… ë³€í™˜ ì‹œë„ - ì •ìˆ˜ë¥¼ ë¬¸ìì—´ë¡œ
                if isinstance(doc_id, int):
                    str_id = str(doc_id)
                    if str_id in doc_map:
                        reranked_docs.append(doc_map[str_id])
                        continue
                
                # 6. cleaned_idì— ëŒ€í•´ì„œë„ íƒ€ì… ë³€í™˜ ì‹œë„
                if 'cleaned_id' in locals():
                    # cleaned_idê°€ ìˆ«ì ë¬¸ìì—´ì¸ ê²½ìš° ì •ìˆ˜ë¡œ ë³€í™˜
                    if isinstance(cleaned_id, str) and cleaned_id.isdigit():
                        int_cleaned_id = int(cleaned_id)
                        if int_cleaned_id in doc_map:
                            reranked_docs.append(doc_map[int_cleaned_id])
                            continue
                    # cleaned_idê°€ ì •ìˆ˜ì¸ ê²½ìš° ë¬¸ìì—´ë¡œ ë³€í™˜
                    elif isinstance(cleaned_id, int):
                        str_cleaned_id = str(cleaned_id)
                        if str_cleaned_id in doc_map:
                            reranked_docs.append(doc_map[str_cleaned_id])
                            continue
                
                missing_ids.append(doc_id)
        
        if missing_ids:
            logger.warning(f"[RERANK] Could not match {len(missing_ids)} IDs: {missing_ids[:3]}...")
        
        logger.info(f"[RERANK] {len(documents)} â†’ {len(reranked_docs)} documents (reasoning: {result.reasoning[:100]}...)")
        return reranked_docs
    
    def __call__(self, state: MVPWorkflowState) -> Dict[str, Any]:
        """
        ë…¸ë“œ ì‹¤í–‰
        
        Args:
            state: ì›Œí¬í”Œë¡œìš° ìƒíƒœ
            
        Returns:
            ì—…ë°ì´íŠ¸ëœ ìƒíƒœ í•„ë“œ
        """
        logger.info(f"[RETRIEVAL] Node started")
        
        # Multi-turn ë¬¸ì„œ ì´ˆê¸°í™” ê²€ì¦ ë¡œì§ (ì²« ë²ˆì§¸ subtaskì—ì„œë§Œ)
        current_subtask_idx = state.get("current_subtask_idx", 0)
        existing_docs = state.get("documents", [])
        logger.info(f"[RETRIEVAL] Subtask index: {current_subtask_idx}, Existing documents: {len(existing_docs)}")
        
        if current_subtask_idx == 0:  # ì²« ë²ˆì§¸ subtask ì²˜ë¦¬ ì‹œ
            if len(existing_docs) > 0:
                logger.warning(f"[RETRIEVAL] Documents not cleared properly: {len(existing_docs)} existing documents found")
                logger.warning(f"[RETRIEVAL] This may cause multi-turn document accumulation issues")
                # Log first few document IDs for debugging
                doc_ids = [doc.metadata.get('id', 'unknown') for doc in existing_docs[:5]]
                logger.warning(f"[RETRIEVAL] First few document IDs: {doc_ids}")
            else:
                logger.info(f"[RETRIEVAL] Document state properly cleared for new RAG query")
        
        try:
            start_time = time.time()
            
            # ì´ˆê¸°í™”
            logger.debug(f"[RETRIEVAL] Initializing database and search components...")
            self._initialize()
            logger.debug(f"[RETRIEVAL] Initialization completed")
            
            # í˜„ì¬ ì„œë¸ŒíƒœìŠ¤í¬ ê°€ì ¸ì˜¤ê¸°
            subtasks = state.get("subtasks", [])
            current_idx = state.get("current_subtask_idx", 0)
            logger.debug(f"[RETRIEVAL] Subtasks: {len(subtasks)}, current_idx: {current_idx}")
            
            # ì¿¼ë¦¬ ë³€í˜• í™•ì¸ (í•„ìˆ˜)
            query_variations = state.get("query_variations")  # state.pyì˜ í•„ë“œëª…ê³¼ ì¼ì¹˜
            
            # None ì²´í¬ - ì¦‰ì‹œ ì‹¤íŒ¨
            if query_variations is None:
                logger.error(f"[RETRIEVAL] CRITICAL: query_variations is None in state")
                logger.error(f"[RETRIEVAL] State keys: {list(state.keys())}")
                raise ValueError(
                    "CRITICAL ERROR: query_variations is None. "
                    "This should never happen - SubtaskExecutor must set query_variations."
                )
            
            # ë¹ˆ ë¦¬ìŠ¤íŠ¸ ì²´í¬ - ì¦‰ì‹œ ì‹¤íŒ¨
            if not query_variations:
                logger.error(f"[RETRIEVAL] CRITICAL: query_variations is empty list")
                logger.error(f"[RETRIEVAL] State keys: {list(state.keys())}")
                raise ValueError(
                    "CRITICAL ERROR: query_variations is empty. "
                    "SubtaskExecutor must generate at least one query variation."
                )
            
            logger.debug(f"[RETRIEVAL] Query variations in state: {len(query_variations)} items")
            
            # ë©”ì¸ ì¿¼ë¦¬ ê²°ì •
            if not subtasks or current_idx >= len(subtasks):
                # ì„œë¸ŒíƒœìŠ¤í¬ê°€ ì—†ìœ¼ë©´ ì›ë³¸ ì¿¼ë¦¬
                query = state["query"]
                logger.debug(f"[RETRIEVAL] Using original query: '{query}'")
            else:
                # í˜„ì¬ ì„œë¸ŒíƒœìŠ¤í¬ì˜ ì¿¼ë¦¬ ì‚¬ìš©
                current_subtask = subtasks[current_idx]
                query = current_subtask["query"]
                subtask_id = current_subtask.get("id", "no-id")[:8]
                logger.info(f"[RETRIEVAL] Using subtask [{subtask_id}] query: '{query}'")
                
                # ì„œë¸ŒíƒœìŠ¤í¬ì˜ ì¿¼ë¦¬ ë³€í˜•ì´ ìˆìœ¼ë©´ ìš°ì„  ì‚¬ìš©
                if current_subtask.get("query_variations"):
                    subtask_variations = current_subtask["query_variations"]
                    query_variations = subtask_variations
                    logger.debug(f"[RETRIEVAL] Using subtask query variations: {len(subtask_variations)} items")
            
            # ê¸°ë³¸ ì–¸ì–´ ê°ì§€ (ì›ë³¸ ì¿¼ë¦¬ ê¸°ì¤€) - í´ë°±ìš©
            logger.debug(f"[RETRIEVAL] Detecting language for query: '{query}'")
            language_detection = self._detect_language(query)
            logger.info(f"[RETRIEVAL] Default language detected: {language_detection.language} (confidence: {language_detection.confidence:.2f})")
            
            # ê²€ìƒ‰ í•„í„° ìƒì„± (stateì—ì„œ ê°€ì ¸ì˜¤ê±°ë‚˜ ê¸°ë³¸ê°’)
            filter_dict = state.get("search_filter")
            if filter_dict:
                logger.info(f"[RETRIEVAL] Search filter received:")
                for key, value in filter_dict.items():
                    if value is not None:
                        logger.info(f"  - {key}: {value}")
            else:
                logger.info(f"[RETRIEVAL] No search filter (will search all documents)")
            
            # Multi-Query ë³‘ë ¬ ê²€ìƒ‰ ì‹¤í–‰ (ë³‘ë ¬ì„± í–¥ìƒ)
            logger.info(f"[RETRIEVAL] Preparing {len(query_variations)} parallel search tasks")
            
            # ThreadPoolExecutorë¡œ ë³‘ë ¬ ì‹¤í–‰ (DB pool sizeì˜ 30%)
            max_workers = 3  # DB poolì´ 10ê°œì´ë¯€ë¡œ 3ê°œ ì •ë„ê°€ ì ì ˆ
            
            def search_task(idx: int, query_variant: str):
                """ë³‘ë ¬ ê²€ìƒ‰ íƒœìŠ¤í¬ - ê° ì¿¼ë¦¬ë³„ ê°œë³„ ì–¸ì–´ ê°ì§€"""
                try:
                    logger.debug(f"[RETRIEVAL] Executing task {idx}: '{query_variant[:50]}...'")
                    
                    # ê° ì¿¼ë¦¬ ë³€í˜•ë³„ë¡œ ê°œë³„ ì–¸ì–´ ê°ì§€
                    variant_language_detection = self._detect_language(query_variant)
                    logger.info(f"[RETRIEVAL] Task {idx} language: {variant_language_detection.language} (confidence: {variant_language_detection.confidence:.2f}) for query: '{query_variant[:50]}...'")
                    
                    # ê°ì§€ëœ ì–¸ì–´ë¡œ ê²€ìƒ‰ ì‹¤í–‰
                    result = self._bilingual_search(
                        query=query_variant,
                        filter_dict=filter_dict,
                        primary_language=variant_language_detection.language,  # ê°œë³„ ê°ì§€ëœ ì–¸ì–´ ì‚¬ìš©
                        top_k=self.default_top_k
                    )
                    
                    # ê²€ìƒ‰ í†µê³„ ìˆ˜ì§‘
                    stats = None
                    if hasattr(self.hybrid_search, 'last_search_stats'):
                        stats = self.hybrid_search.last_search_stats.copy()
                        # ì–¸ì–´ ì •ë³´ ì¶”ê°€ (ì§‘ê³„ì— í•„ìš”)
                        if stats:
                            stats['detected_language'] = variant_language_detection.language
                    
                    return (result, stats)  # íŠœí”Œë¡œ ë°˜í™˜
                    
                except Exception as e:
                    # ì˜ˆì™¸ë¥¼ ë¡œê¹…í•˜ê³  ë‹¤ì‹œ ë°œìƒì‹œí‚´ (executorì—ì„œ ì²˜ë¦¬í•˜ë„ë¡)
                    error_type = type(e).__name__
                    logger.error(f"[RETRIEVAL] Task {idx} encountered error: {error_type}: {str(e)}")
                    raise  # ì˜ˆì™¸ë¥¼ ë‹¤ì‹œ ë°œìƒì‹œì¼œ executorì—ì„œ ì²˜ë¦¬
            
            # ê²€ìƒ‰ íƒœìŠ¤í¬ íŒŒë¼ë¯¸í„° ì €ì¥
            search_tasks = []
            for idx, query_variant in enumerate(query_variations):
                search_tasks.append((idx, query_variant))  # íŒŒë¼ë¯¸í„°ë§Œ ì €ì¥
            
            # ëª¨ë“  ê²€ìƒ‰ì„ ë³‘ë ¬ë¡œ ì‹¤í–‰ (í–¥ìƒëœ ë³‘ë ¬ì„±)
            logger.info(f"[RETRIEVAL] Executing {len(search_tasks)} searches (max {max_workers} workers)...")
            
            # ThreadPoolExecutorë¡œ ë³‘ë ¬ ì‹¤í–‰
            results_or_errors = []
            all_search_stats = []  # ëª¨ë“  ê²€ìƒ‰ í†µê³„ ìˆ˜ì§‘
            with ThreadPoolExecutor(max_workers=max_workers) as executor:
                futures = [executor.submit(search_task, idx, query_variant) 
                          for idx, query_variant in search_tasks]
                
                for future in futures:
                    try:
                        result_tuple = future.result()
                        results_or_errors.append(result_tuple)
                    except Exception as e:
                        results_or_errors.append(e)
            
            # Process results and handle errors with better exception handling
            results = []
            connection_errors = []
            
            for idx, result_or_error in enumerate(results_or_errors):
                if isinstance(result_or_error, Exception):
                    error_type = type(result_or_error).__name__
                    error_msg = str(result_or_error)
                    
                    # Connection ê´€ë ¨ ì—ëŸ¬ íŠ¹ë³„ ì²˜ë¦¬
                    if "ConnectionDoesNotExistError" in error_type or "connection" in error_msg.lower():
                        connection_errors.append(idx)
                        logger.error(f"[RETRIEVAL] Task {idx} failed with CONNECTION ERROR: {error_msg}")
                    else:
                        logger.error(f"[RETRIEVAL] Task {idx} failed with {error_type}: {error_msg}")
                    
                    results.append([])  # ì‹¤íŒ¨í•œ ë³€í˜•ì€ ë¹ˆ ë¦¬ìŠ¤íŠ¸
                else:
                    # íŠœí”Œ ë¶„í•´: (result, stats)
                    if isinstance(result_or_error, tuple) and len(result_or_error) == 2:
                        result, stats = result_or_error
                        logger.debug(f"[RETRIEVAL] Task {idx} succeeded with {len(result)} documents")
                        results.append(result)
                        if stats:
                            all_search_stats.append(stats)
                    else:
                        # ì´ì „ ë²„ì „ í˜¸í™˜ì„± (íŠœí”Œì´ ì•„ë‹Œ ê²½ìš°)
                        logger.debug(f"[RETRIEVAL] Task {idx} succeeded with {len(result_or_error)} documents")
                        results.append(result_or_error)
            
            # Connection ì—ëŸ¬ê°€ ìˆìœ¼ë©´ ê²½ê³ 
            if connection_errors:
                logger.warning(f"[RETRIEVAL] Connection errors detected in tasks: {connection_errors}. Pool may be corrupted.")
            
            # Log overall status
            successful_tasks = sum(1 for r in results_or_errors if not isinstance(r, Exception))
            if successful_tasks == len(search_tasks):
                logger.info(f"[RETRIEVAL] All {len(search_tasks)} searches completed successfully")
            else:
                logger.warning(f"[RETRIEVAL] {successful_tasks}/{len(search_tasks)} searches succeeded")
            
            # í•„í„°ê°€ ìˆì„ ë•Œ ëª¨ë“  ê²°ê³¼ê°€ ë¹„ì–´ìˆìœ¼ë©´ í•„í„° ì—†ì´ ì¬ì‹œë„
            total_docs_with_filter = sum(len(r) for r in results if r)
            if filter_dict and total_docs_with_filter == 0:
                logger.warning(f"[RETRIEVAL] All searches with filter returned 0 documents. Retrying without filter...")
                logger.info(f"[RETRIEVAL] Original filter was: {filter_dict}")
                
                # í•„í„° ì—†ì´ ì¬ì‹¤í–‰ (filter_dictë¥¼ Noneìœ¼ë¡œ ë³€ê²½)
                original_filter = filter_dict
                filter_dict = None  # í•„í„° ì œê±°
                
                # í•„í„° ì—†ì´ ì¬ì‹œë„í•˜ëŠ” ê²€ìƒ‰ í•¨ìˆ˜
                def retry_search_task(idx: int, query_variant: str):
                    """í•„í„° ì—†ì´ ì¬ì‹œë„í•˜ëŠ” ê²€ìƒ‰"""
                    logger.debug(f"[RETRIEVAL] Retrying task {idx} without filter: '{query_variant[:50]}...'")
                    result = self._bilingual_search(
                        query=query_variant,
                        filter_dict=None,  # í•„í„° ì—†ì´
                        primary_language=language_detection.language,
                        top_k=self.default_top_k
                    )
                    
                    # HybridSearchì˜ last_search_stats ê°€ì ¸ì˜¤ê¸° (ìˆëŠ” ê²½ìš°)
                    stats = None
                    if hasattr(self.hybrid_search, 'last_search_stats'):
                        stats = self.hybrid_search.last_search_stats.copy() if self.hybrid_search.last_search_stats else None
                        # ì–¸ì–´ ì •ë³´ ì¶”ê°€ (ì§‘ê³„ì— í•„ìš”)
                        if stats:
                            # ì–¸ì–´ ê°ì§€
                            variant_language_detection = detect_language(query_variant)
                            stats['detected_language'] = variant_language_detection.language
                    
                    return (result, stats)  # íŠœí”Œë¡œ ë°˜í™˜
                
                logger.info(f"[RETRIEVAL] Retrying {len(search_tasks)} searches without filter...")
                
                # ì¬ì‹œë„ ì‹¤í–‰ (ë™ì¼í•œ ë³‘ë ¬ì„± ìœ ì§€)
                retry_results_or_errors = []
                with ThreadPoolExecutor(max_workers=max_workers) as executor:
                    retry_futures = [executor.submit(retry_search_task, idx, query_variant) 
                                   for idx, query_variant in search_tasks]
                    
                    for future in retry_futures:
                        try:
                            result = future.result()
                            retry_results_or_errors.append(result)
                        except Exception as e:
                            retry_results_or_errors.append(e)
                
                # ì¬ì‹œë„ ê²°ê³¼ ì²˜ë¦¬
                retry_results = []
                retry_stats = []
                for idx, result_or_error in enumerate(retry_results_or_errors):
                    if isinstance(result_or_error, Exception):
                        logger.error(f"[RETRIEVAL] Retry task {idx} failed: {str(result_or_error)}")
                        retry_results.append([])
                    else:
                        # íŠœí”Œ ì²˜ë¦¬: (result, stats)
                        if isinstance(result_or_error, tuple) and len(result_or_error) == 2:
                            result, stats = result_or_error
                            logger.debug(f"[RETRIEVAL] Retry task {idx} succeeded with {len(result)} documents")
                            retry_results.append(result)
                            if stats:
                                retry_stats.append(stats)
                        else:
                            # ì´ì „ ë²„ì „ í˜¸í™˜ì„± (íŠœí”Œì´ ì•„ë‹Œ ê²½ìš°)
                            logger.debug(f"[RETRIEVAL] Retry task {idx} succeeded with {len(result_or_error)} documents")
                            retry_results.append(result_or_error)
                
                # ì¬ì‹œë„ ì„±ê³µ ì—¬ë¶€ ë¡œê¹…
                retry_total_docs = sum(len(r) for r in retry_results if r)
                if retry_total_docs > 0:
                    logger.info(f"[RETRIEVAL] Retry without filter succeeded: {retry_total_docs} documents found")
                    results = retry_results  # ì¬ì‹œë„ ê²°ê³¼ ì‚¬ìš©
                    # ì¬ì‹œë„ í†µê³„ë¡œ êµì²´
                    if retry_stats:
                        all_search_stats = retry_stats
                    # ë©”íƒ€ë°ì´í„°ì— ì¬ì‹œë„ ì •ë³´ ì¶”ê°€
                    if 'metadata' not in state:
                        state['metadata'] = {}
                    state['metadata']['retrieval_retry'] = {
                        'retried': True,
                        'original_filter': original_filter,
                        'retry_reason': 'filter_returned_zero_documents',
                        'retry_documents': retry_total_docs
                    }
                else:
                    logger.warning(f"[RETRIEVAL] Retry without filter also returned 0 documents")
            
            # ê²°ê³¼ ì²˜ë¦¬ ë° ì¤‘ë³µ ì œê±°
            logger.debug(f"[RETRIEVAL] Processing search results and removing duplicates...")
            all_documents = []
            seen_ids = set()
            
            for (idx, query_variant), variant_docs in zip(search_tasks, results):
                result_count = len(variant_docs)
                logger.debug(f"[RETRIEVAL] Query variant {idx} returned {result_count} documents")
                
                for doc in variant_docs:
                    # Document ID ìƒì„± (metadataì˜ id ë˜ëŠ” content hash)
                    doc_id = doc.metadata.get("id")
                    if not doc_id:
                        # IDê°€ ì—†ìœ¼ë©´ contentì˜ ì²˜ìŒ 100ìë¥¼ ê¸°ì¤€ìœ¼ë¡œ
                        doc_id = hash(doc.page_content[:100])
                    
                    if doc_id not in seen_ids:
                        seen_ids.add(doc_id)
                        # ë©”íƒ€ë°ì´í„°ì— ê²€ìƒ‰ ë³€í˜• ì •ë³´ ì¶”ê°€
                        doc.metadata["search_variant_idx"] = idx
                        doc.metadata["search_variant_query"] = query_variant
                        all_documents.append(doc)
            
            # ìµœì¢… ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
            documents = all_documents
            
            # documentsê°€ Noneì´ë©´ ì¦‰ì‹œ ì‹¤íŒ¨
            if documents is None:
                raise ValueError(
                    "CRITICAL ERROR: documents is None after search. "
                    "This should never happen - search must return empty list at minimum."
                )
            
            unique_count = len(seen_ids)
            total_retrieved = sum(len(result) for result in results)
            logger.info(f"[RETRIEVAL] Results: {total_retrieved} total â†’ {unique_count} unique â†’ {len(documents)} final documents")
            
            # ê²€ìƒ‰ ê²°ê³¼ ë¬¸ì„œ ìƒì„¸ ì •ë³´ ë¡œê¹… (ìƒìœ„ 3ê°œë§Œ)
            for i, doc in enumerate(documents[:3]):
                source = doc.metadata.get('source', 'unknown')[:20]
                page = doc.metadata.get('page', 'N/A')
                category = doc.metadata.get('category', 'unknown')[:12]
                content_preview = doc.page_content[:45].replace('\n', ' ').strip()
                logger.info(f"[RETRIEVAL] Doc {i+1}: {source}:P.{page}:{category} - \"{content_preview}...\"")
            
            # CRITICAL: ìµœì†Œí•œ í•˜ë‚˜ì˜ ê²°ê³¼ë¼ë„ ìˆì–´ì•¼ í•¨ (ì—ëŸ¬ ë°œìƒ)
            if not documents and query_variations:
                logger.error(f"[RETRIEVAL] CRITICAL: No documents found for any of {len(query_variations)} query variations")
                raise ValueError(
                    f"CRITICAL ERROR: No documents retrieved from {len(query_variations)} query variations. "
                    f"Retrieval must return at least one document. "
                    f"This indicates a serious issue with the search system or data."
                )
            
            execution_time = time.time() - start_time
            logger.info(f"[RETRIEVAL] Search execution completed in {execution_time:.3f}s")
            
            # ì‹¤í–‰ ì‹œê°„ ê¸°ë¡
            execution_times = state.get("execution_time", {})
            execution_times["retrieval"] = execution_time
            
            # ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸
            metadata = state.get("metadata", {})
            metadata["retrieval"] = {
                "query": query,
                "query_variations_count": len(query_variations),
                "query_variations_used": query_variations,
                "detected_language": language_detection.language,
                "language_confidence": language_detection.confidence,
                "total_documents": len(documents),
                "unique_documents": len(seen_ids),
                "search_strategy": "multi_query_bilingual",
                "confidence": self._calculate_confidence(documents)
            }
            
            # í˜„ì¬ ì„œë¸ŒíƒœìŠ¤í¬ ì—…ë°ì´íŠ¸ (ìˆëŠ” ê²½ìš°)
            if subtasks and current_idx < len(subtasks):
                subtasks[current_idx]["documents"] = documents
                subtasks[current_idx]["status"] = "retrieved"
                subtask_id = subtasks[current_idx].get("id", "no-id")[:8]
                logger.info(f"[RETRIEVAL] Updated subtask [{subtask_id}] status: 'executing' -> 'retrieved' ({len(documents)} docs)")
            
            confidence_score = self._calculate_confidence(documents)
            logger.info(f"[RETRIEVAL] Confidence score: {confidence_score:.3f}")
            
            # Reranking ì ìš© (ë¬¸ì„œê°€ 10ê°œ ì´ˆê³¼ì‹œ)
            if len(documents) > 10:
                logger.info(f"[RETRIEVAL] Applying LLM reranking to {len(documents)} documents...")
                documents = self._rerank_documents(
                    query=state["query"],
                    documents=documents,
                    top_k=int(os.getenv("RERANK_TOP_K", "10"))
                )
                # Reranking í›„ ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸
                metadata["retrieval"]["documents_after_rerank"] = len(documents)
                metadata["retrieval"]["reranking_applied"] = True
                
                # í˜„ì¬ ì„œë¸ŒíƒœìŠ¤í¬ ë¬¸ì„œë„ ì—…ë°ì´íŠ¸
                if subtasks and current_idx < len(subtasks):
                    subtasks[current_idx]["documents"] = documents
            else:
                metadata["retrieval"]["reranking_applied"] = False
            
            # ì–¸ì–´ë³„ í‚¤ì›Œë“œ ì§‘ê³„
            korean_keywords = set()
            english_keywords = set()
            total_keyword_docs = 0
            total_semantic_docs = 0
            
            for stats in all_search_stats:
                if stats:
                    # ì–¸ì–´ë³„ í‚¤ì›Œë“œ ìˆ˜ì§‘
                    detected_lang = stats.get('detected_language', stats.get('language', ''))
                    keywords = stats.get('extracted_keywords', [])
                    
                    if detected_lang == 'korean':
                        korean_keywords.update(keywords)
                    elif detected_lang == 'english':
                        english_keywords.update(keywords)
                    
                    # ê²€ìƒ‰ ê²°ê³¼ ìˆ˜ ì§‘ê³„
                    total_keyword_docs += stats.get('keyword_count', 0)
                    total_semantic_docs += stats.get('semantic_count', 0)
            
            # í†µê³„ ë¡œê¹…
            if korean_keywords or english_keywords:
                logger.info(f"[RETRIEVAL] Aggregated keywords - Korean: {list(korean_keywords)[:5]}, English: {list(english_keywords)[:5]}")
                logger.info(f"[RETRIEVAL] Total search results - Keyword: {total_keyword_docs}, Semantic: {total_semantic_docs}")
            
            # ë©”ì‹œì§€ ìƒì„± - ê²€ìƒ‰ ê³¼ì • ìƒì„¸ ì •ë³´
            messages = []
            
            # 1. ì„œë¸ŒíƒœìŠ¤í¬ ì •ë³´ (ê°„ì†Œí™”)
            if subtasks and current_idx < len(subtasks):
                current_subtask = subtasks[current_idx]
                subtask_desc = current_subtask.get("description", current_subtask.get("query", ""))
                messages.append(
                    AIMessage(content=f"ğŸ” [{current_idx+1}/{len(subtasks)}] {subtask_desc[:80]}... ê²€ìƒ‰ ì¤‘")
                )
            else:
                messages.append(
                    AIMessage(content=f"ğŸ” ê²€ìƒ‰ ì¤‘: {query[:80]}...")
                )
            
            # ì–¸ì–´ë³„ í‚¤ì›Œë“œ í‘œì‹œ (ì§‘ê³„ëœ í†µê³„ ì‚¬ìš©)
            if korean_keywords:
                korean_kw_list = list(korean_keywords)[:4]
                korean_display = ', '.join(korean_kw_list)
                if len(korean_keywords) > 4:
                    korean_display += f" ì™¸ {len(korean_keywords)-4}ê°œ"
                messages.append(
                    AIMessage(content=f"ğŸ”‘ í•œêµ­ì–´: {korean_display}")
                )
            
            if english_keywords:
                english_kw_list = list(english_keywords)[:4]
                english_display = ', '.join(english_kw_list)
                if len(english_keywords) > 4:
                    english_display += f" ì™¸ {len(english_keywords)-4}ê°œ"
                messages.append(
                    AIMessage(content=f"ğŸ”‘ ì˜ì–´: {english_display}")
                )
            
            # ê²€ìƒ‰ ê²°ê³¼ í†µê³„ í‘œì‹œ
            if len(all_search_stats) > 0:
                if total_keyword_docs > 0:
                    messages.append(
                        AIMessage(content=f"ğŸ” {len(all_search_stats)}ê°œ ë³€í˜• ê²€ìƒ‰ (í‚¤ì›Œë“œ {total_keyword_docs}ê°œ + ì˜ë¯¸ {total_semantic_docs}ê°œ ë¬¸ì„œ)")
                    )
                elif total_semantic_docs > 0:
                    messages.append(
                        AIMessage(content=f"ğŸ” {len(all_search_stats)}ê°œ ë³€í˜• ê²€ìƒ‰ (í‚¤ì›Œë“œ ë§¤ì¹­ ì—†ìŒ, ì˜ë¯¸ ê²€ìƒ‰ {total_semantic_docs}ê°œ)")
                    )
            
            # ì–¸ì–´ ê°ì§€, ì¿¼ë¦¬ ë³€í˜•, ê²€ìƒ‰ ì „ëµ ë©”ì‹œì§€ ì œê±°
            # í•„í„° ì •ë³´ëŠ” ì¤‘ìš”í•œ ê²½ìš°ë§Œ í‘œì‹œ
            if filter_dict:
                # ì¤‘ìš”í•œ í•„í„°ë§Œ í‘œì‹œ (ì˜ˆ: íŠ¹ì • í˜ì´ì§€, ì¹´í…Œê³ ë¦¬)
                important_filters = []
                if filter_dict.get("page"):
                    important_filters.append(f"í˜ì´ì§€ {filter_dict['page']}")
                if filter_dict.get("category"):
                    important_filters.append(f"ì¹´í…Œê³ ë¦¬ {filter_dict['category']}")
                if important_filters:
                    messages.append(
                        AIMessage(content=f"ğŸ” í•„í„°: {', '.join(important_filters)}")
                    )
            
            # 2. ê²€ìƒ‰ ê²°ê³¼ í‘œì‹œ
            if documents:
                unique_count = metadata.get("unique_documents", len(documents))
                # í†µí•© score í•„ë“œ ì‚¬ìš© - None ì•ˆì „ ì²˜ë¦¬
                scores = [doc.metadata.get("score", 0.0) for doc in documents]
                valid_scores = [s for s in scores if s and s > 0]  # Noneê³¼ 0 ì²´í¬
                
                if valid_scores:
                    avg_score = sum(valid_scores) / len(valid_scores)
                    messages.append(
                        AIMessage(content=f"ğŸ“„ {unique_count}ê°œ ê³ ìœ  ë¬¸ì„œ ë°œê²¬ (ìœ ì‚¬ë„ í‰ê·  {avg_score:.1%})")
                    )
                else:
                    messages.append(
                        AIMessage(content=f"ğŸ“„ {unique_count}ê°œ ê³ ìœ  ë¬¸ì„œ ë°œê²¬")
                    )
            else:
                messages.append(
                    AIMessage(content="âš ï¸ ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤. ì›¹ ê²€ìƒ‰ì„ ì‹œë„í•©ë‹ˆë‹¤...")
                )
            
            result = {
                "messages": messages,  # ë©”ì‹œì§€ ì¶”ê°€
                "documents": documents,  # ë¬¸ì„œ ë°˜í™˜ (stateì˜ add reducerì— ì˜í•´ ëˆ„ì ë¨, planningì—ì„œ ì´ˆê¸°í™”)
                "subtasks": subtasks,
                "search_language": language_detection.language,
                "confidence_score": confidence_score,
                "execution_time": execution_times,
                "metadata": metadata
            }
            logger.info(f"[RETRIEVAL] Node completed successfully - retrieved {len(documents)} documents")
            return result
            
        except Exception as e:
            logger.error(f"[RETRIEVAL] Node failed: {str(e)}")
            return {
                "error": f"Retrieval failed: {str(e)}",
                "workflow_status": "failed",
                "warnings": [f"Search error: {str(e)}"]
            }
    
    def invoke(self, state: MVPWorkflowState) -> Dict[str, Any]:
        """ë™ê¸° ì‹¤í–‰ (LangGraph í˜¸í™˜ì„±)"""
        logger.debug(f"[RETRIEVAL] Invoke called (sync)")
        
        # ë™ê¸° ë°©ì‹ìœ¼ë¡œ ì§ì ‘ í˜¸ì¶œ
        return self.__call__(state)
    
    def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        if self.db_manager:
            self.db_manager.close()
            self.initialized = False