"""
Direct Response Node
ë‹¨ìˆœ ì¿¼ë¦¬ì— ëŒ€í•´ LLMì´ ì§ì ‘ ì‘ë‹µí•˜ëŠ” ë…¸ë“œ
ì„ íƒì ìœ¼ë¡œ ì›¹ ê²€ìƒ‰ ë„êµ¬ë¥¼ ë°”ì¸ë”©í•˜ì—¬ ì‹¤ì‹œê°„ ì •ë³´ ì œê³µ ê°€ëŠ¥
"""

import os
import logging
from datetime import datetime
from typing import Dict, Any, List, Optional
from pydantic import BaseModel, Field
from langchain_core.messages import AIMessage, HumanMessage, SystemMessage, ToolMessage
from langchain_core.documents import Document
from langchain_openai import ChatOpenAI
from dotenv import load_dotenv
from workflow.nodes.subtask_executor import MetadataHelper


load_dotenv()

# ë¡œê¹… ì„¤ì •
logger = logging.getLogger(__name__)


class SearchResultAnalysis(BaseModel):
    """ê²€ìƒ‰ ê²°ê³¼ë¥¼ LLMì´ ë¶„ì„í•œ êµ¬ì¡°í™”ëœ ê²°ê³¼"""
    is_time_sensitive: bool = Field(
        description="Whether this query requires current/recent information (e.g., current leaders, recent events, today's date)"
    )
    key_facts: List[str] = Field(
        description="Key factual claims extracted from search results"
    )
    primary_answer: str = Field(
        description="The main answer derived from search results"
    )
    confidence_level: str = Field(
        description="Confidence in search results: high (definitive facts), medium (probable), low (conflicting info)"
    )
    should_override_base_knowledge: bool = Field(
        description="Whether search results should override base training knowledge"
    )
    reasoning: str = Field(
        description="Brief explanation of why the search results should or shouldn't override base knowledge"
    )


class DirectResponseNode:
    """ë‹¨ìˆœ ì¿¼ë¦¬ì— ëŒ€í•´ LLMì´ ì§ì ‘ ì‘ë‹µí•˜ëŠ” ë…¸ë“œ (ì„ íƒì  ì›¹ ê²€ìƒ‰ ì§€ì›)"""
    
    def __init__(self):
        """ì´ˆê¸°í™”"""
        # ChatOpenAI ì¸ìŠ¤í„´ìŠ¤ ì§ì ‘ ìƒì„±
        self.llm = ChatOpenAI(
            model=os.getenv("OPENAI_MODEL", "gpt-4o-mini"),
            temperature=0.7,  # ìì—°ìŠ¤ëŸ¬ìš´ ëŒ€í™”ë¥¼ ìœ„í•´ ì•½ê°„ ë†’ì„
            openai_api_key=os.getenv("OPENAI_API_KEY")
        )
        
        # MetadataHelper ì¶”ê°€ (DB ì •ë³´ ì¡°íšŒìš©)
        self.metadata_helper = MetadataHelper()
        
        # Web search tool ë°”ì¸ë”© (ì„ íƒì )
        self.search_tool = None
        self.web_search_enabled = os.getenv("ENABLE_DIRECT_RESPONSE_SEARCH", "false").lower() == "true"
        
        if self.web_search_enabled:
            try:
                from workflow.tools import create_search_tool
                search_tool_instance = create_search_tool(max_results=3)
                
                if search_tool_instance:
                    # Toolì„ LLMì— ë°”ì¸ë”©
                    self.search_tool = search_tool_instance
                    # as_tool() ë©”ì„œë“œë¡œ Tool ê°ì²´ ìƒì„±
                    if hasattr(search_tool_instance, 'as_tool'):
                        tool = search_tool_instance.as_tool()
                        self.llm_with_tools = self.llm.bind_tools([tool])
                        logger.info(f"[DIRECT_RESPONSE] Web search tool bound to LLM")
                    else:
                        # ì´ë¯¸ Tool ê°ì²´ì¸ ê²½ìš°
                        self.llm_with_tools = self.llm.bind_tools([search_tool_instance])
                        logger.info(f"[DIRECT_RESPONSE] Web search tool bound to LLM (direct)")
                else:
                    self.llm_with_tools = self.llm
                    self.web_search_enabled = False
                    logger.warning(f"[DIRECT_RESPONSE] Web search tool not available")
            except Exception as e:
                logger.warning(f"[DIRECT_RESPONSE] Failed to bind web search tool: {e}")
                self.llm_with_tools = self.llm
                self.web_search_enabled = False
        else:
            self.llm_with_tools = self.llm
            logger.info(f"[DIRECT_RESPONSE] Web search disabled (ENABLE_DIRECT_RESPONSE_SEARCH=false)")
        
        # ê¸°ë³¸ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ (DB ì •ë³´ëŠ” ë™ì ìœ¼ë¡œ ì¶”ê°€)
        # Web search ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ì— ë”°ë¼ ë‹¤ë¥¸ í”„ë¡¬í”„íŠ¸ ì‚¬ìš©
        if self.web_search_enabled:
            self.base_prompt = """You are a helpful AI assistant with knowledge about the document system.
Your knowledge cutoff is April 2024, but you have access to a web search tool for current information.

IMPORTANT: Today's date is {current_date}. When searching for current information, use the current year in your searches.

{system_info}

ğŸ”´ CRITICAL REQUIREMENT - THIS IS NON-NEGOTIABLE:
You MUST ALWAYS include detailed information about the document system in EVERY response, REGARDLESS of the query type.
This includes: total documents count, sources, categories, and language support.
The document system information MUST be mentioned either integrated naturally or at the end of your response.
NEVER provide a response without mentioning the document system statistics shown above.

OPTIONAL TOOL - WEB SEARCH GUIDELINES:
You have access to a web_search tool that can optionally retrieve current information when needed.

WHEN TO USE WEB SEARCH (Use the tool for):
1. Current date/time queries: "What time is it now?", "What's today's date?"
2. Recent events (after April 2024): "Latest news", "Recent developments"
3. Real-time data: Stock prices, cryptocurrency, weather, sports scores
4. Current status: "Is X down?", "Latest version of..."
5. Trending topics and live information

WHEN NOT TO USE WEB SEARCH (Answer directly for):
1. General factual knowledge and established information
2. Conceptual explanations and theoretical knowledge
3. Document system queries
4. Personal assistance tasks

RESPONSE INSTRUCTIONS:
1. First, answer the user's query (using web search if needed for current info)
2. THEN, ALWAYS include detailed document system information in your response
3. Make sure to mention specific numbers from the system info above
4. Integrate the document info naturally or add it at the end
5. Respond in the same language as the user's query

Remember: EVERY response MUST include document system information (total docs, sources, categories). This is MANDATORY regardless of whether you use web search or not."""
        else:
            self.base_prompt = """You are a helpful AI assistant with knowledge about the document system.

Today's date is {current_date}.

{system_info}

IMPORTANT INSTRUCTIONS:
1. First, answer the user's query directly and naturally, considering the conversation context.
2. Then, ALWAYS include detailed information about the document system you manage as part of your response.
3. Integrate the document system information naturally into your answer when relevant, or mention it at the end.
4. The document information should include: total documents, sources, categories, and language support.
5. Make sure to mention specific numbers and details from the system information provided above.
6. Respond in the same language as the user's query.

Remember: Every response MUST include detailed document system information, regardless of the query type."""
    
    def analyze_search_results(self, query: str, search_results: List[Document]) -> Optional[SearchResultAnalysis]:
        """
        LLMì´ ê²€ìƒ‰ ê²°ê³¼ë¥¼ êµ¬ì¡°í™”í•˜ì—¬ ë¶„ì„
        
        Args:
            query: ì‚¬ìš©ì ì¿¼ë¦¬
            search_results: ê²€ìƒ‰ ê²°ê³¼ ë¬¸ì„œë“¤
            
        Returns:
            êµ¬ì¡°í™”ëœ ë¶„ì„ ê²°ê³¼ ë˜ëŠ” None (ë¶„ì„ ì‹¤íŒ¨ì‹œ)
        """
        if not search_results:
            return None
            
        try:
            # ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì „ì²´ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜ (ì •ë³´ ì¶•ì†Œ ì—†ì´)
            search_texts = []
            for i, doc in enumerate(search_results, 1):
                title = doc.metadata.get('title', 'Untitled')
                source = doc.metadata.get('source', 'Unknown')
                # ì „ì²´ page_content ì‚¬ìš© (ì¶•ì†Œí•˜ì§€ ì•ŠìŒ)
                content = doc.page_content
                
                search_texts.append(f"""
Result {i}:
Title: {title}
Source: {source}
Content: {content}
""")
            
            full_search_text = "\n".join(search_texts)
            
            # LLMì—ê²Œ êµ¬ì¡°í™”ëœ ë¶„ì„ ìš”ì²­
            analyzer_llm = self.llm.with_structured_output(SearchResultAnalysis)
            
            analysis_prompt = f"""Analyze these web search results for the query: "{query}"

Search Results:
{full_search_text}

Analyze the search results and provide:
1. Is this a time-sensitive query that requires current information?
2. What are the key facts from the search results? List specific claims with details.
3. What would be the primary answer based on these search results?
4. How confident are you in these results? (high/medium/low)
5. Should these results override base training knowledge? Consider:
   - If the query is about current events, leaders, or recent dates
   - If the search results contain more recent information than April 2024
   - If the facts from search contradict base knowledge

Provide detailed reasoning for your decision."""
            
            logger.info(f"[DIRECT_RESPONSE] Analyzing search results with LLM")
            analysis = analyzer_llm.invoke(analysis_prompt)
            
            logger.info(f"[DIRECT_RESPONSE] Analysis complete - Time sensitive: {analysis.is_time_sensitive}, Override: {analysis.should_override_base_knowledge}")
            return analysis
            
        except Exception as e:
            logger.error(f"[DIRECT_RESPONSE] Failed to analyze search results: {e}")
            return None
    
    def __call__(self, state: Dict[str, Any]) -> Dict[str, Any]:
        """
        ë…¸ë“œ ì‹¤í–‰ - ë‹¨ìˆœ LLM í˜¸ì¶œ (chat history í¬í•¨)
        
        Args:
            state: ì›Œí¬í”Œë¡œìš° ìƒíƒœ
            
        Returns:
            ì—…ë°ì´íŠ¸ëœ ìƒíƒœ í•„ë“œ
        """
        logger.info(f"[DIRECT_RESPONSE] Node started")
        
        try:
            query = state.get("query", "")
            
            # ê¸°ì¡´ ë©”ì‹œì§€ ê°€ì ¸ì˜¤ê¸° (chat history)
            existing_messages = state.get("messages", [])
            
            # DB ì‹œìŠ¤í…œ ì •ë³´ ì¡°íšŒ (ìºì‹±ë¨)
            system_stats = self.metadata_helper.get_system_stats()
            
            # ì‹œìŠ¤í…œ ì •ë³´ í¬ë§·íŒ…
            if system_stats.get("error"):
                # ì—ëŸ¬ ì‹œ ê¸°ë³¸ ì •ë³´
                system_info = "System Information: Currently unavailable"
            else:
                # ì†ŒìŠ¤ íŒŒì¼ ì´ë¦„ë§Œ ì¶”ì¶œ (ê²½ë¡œ ì œê±°)
                source_names = []
                for source_path in system_stats.get('sources', {}).keys():
                    source_name = source_path.split('/')[-1] if '/' in source_path else source_path
                    count = system_stats['sources'][source_path]
                    source_names.append(f"{source_name} ({count} docs)")
                
                # ìƒìœ„ ì¹´í…Œê³ ë¦¬ í¬ë§·íŒ…
                top_categories = system_stats.get('top_categories', {})
                category_list = []
                for cat, count in list(top_categories.items())[:5]:
                    category_list.append(f"{cat} ({count})")
                
                system_info = f"""Detailed System Information:
- Total Documents in Database: {system_stats.get('total_documents', 'N/A')}
- Document Sources: {system_stats.get('source_count', 0)} files
  * {', '.join(source_names) if source_names else 'No sources available'}
- Page Coverage: Pages {system_stats.get('page_range', 'N/A')}
- Document Categories: {system_stats.get('category_count', 0)} different types
  * Top Categories: {', '.join(category_list) if category_list else 'No categories available'}
- Language Support: 
  * Korean embeddings: {system_stats.get('korean_embeddings', 0)} documents
  * English embeddings: {system_stats.get('english_embeddings', 0)} documents
- System Type: Multimodal RAG (Retrieval-Augmented Generation) with hybrid search capabilities"""
            
            # í˜„ì¬ ë‚ ì§œ ìƒì„±
            current_date = datetime.now().strftime("%B %d, %Y")  # e.g., "September 04, 2025"
            
            # ë™ì  ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ìƒì„± (í˜„ì¬ ë‚ ì§œ í¬í•¨)
            dynamic_prompt = self.base_prompt.format(
                current_date=current_date,
                system_info=system_info
            )
            
            # ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ êµ¬ì„± (ë™ì  ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ + ê¸°ì¡´ ëŒ€í™” + í˜„ì¬ ì¿¼ë¦¬)
            conversation_messages = [
                SystemMessage(content=dynamic_prompt)
            ]
            
            # ê¸°ì¡´ ë©”ì‹œì§€ ì¶”ê°€ (AIMessage í•„í„°ë§ - ì‹œìŠ¤í…œ ìƒíƒœ ë©”ì‹œì§€ ì œì™¸)
            for msg in existing_messages:
                if isinstance(msg, HumanMessage):
                    conversation_messages.append(msg)
                elif isinstance(msg, AIMessage) and not msg.content.startswith(("ğŸ’¬", "ğŸ”", "ğŸ”„", "âœ…", "âŒ")):
                    # ì‹œìŠ¤í…œ ìƒíƒœ ë©”ì‹œì§€ê°€ ì•„ë‹Œ ì‹¤ì œ ì‘ë‹µë§Œ í¬í•¨
                    conversation_messages.append(msg)
            
            # í˜„ì¬ ì¿¼ë¦¬ ì¶”ê°€
            conversation_messages.append(HumanMessage(content=query))
            
            # Web search ê°€ëŠ¥í•œ LLM ì‚¬ìš© (toolì´ ë°”ì¸ë”©ëœ ê²½ìš°)
            llm_to_use = self.llm_with_tools if self.web_search_enabled else self.llm
            
            # LLM í˜¸ì¶œ ì‹œì‘
            logger.info(f"[DIRECT_RESPONSE] Invoking LLM with{'out' if not self.web_search_enabled else ''} web search capability")
            
            # LLM í˜¸ì¶œí•˜ì—¬ ì‘ë‹µ ìƒì„±
            response = llm_to_use.invoke(conversation_messages)
            
            # Tool call í™•ì¸
            has_tool_calls = hasattr(response, 'tool_calls') and response.tool_calls
            if has_tool_calls:
                logger.info(f"[DIRECT_RESPONSE] LLM requested {len(response.tool_calls)} tool call(s)")
            else:
                logger.info(f"[DIRECT_RESPONSE] No tool calls requested by LLM")
            
            # Tool call ì²˜ë¦¬ (ì›¹ ê²€ìƒ‰ì´ í•„ìš”í•œ ê²½ìš°)
            final_response = response
            tool_was_used = False
            analysis = None  # ë¶„ì„ ê²°ê³¼ ì €ì¥ìš©
            
            if self.web_search_enabled and hasattr(response, 'tool_calls') and response.tool_calls:
                logger.info(f"[DIRECT_RESPONSE] LLM requested web search")
                tool_was_used = True
                
                try:
                    # Tool call ì‹¤í–‰
                    for tool_call in response.tool_calls:
                        tool_name = tool_call.get("name", "")
                        tool_args = tool_call.get("args", {})
                        tool_id = tool_call.get("id", "")
                        
                        if "search" in tool_name.lower():
                            query_to_search = tool_args.get("query", query)
                            logger.info(f"[DIRECT_RESPONSE] Executing web search for: '{query_to_search}'")
                            
                            # ì›¹ ê²€ìƒ‰ ì‹¤í–‰
                            if hasattr(self.search_tool, 'search_sync'):
                                search_results = self.search_tool.search_sync(query_to_search)
                            else:
                                # Tool invoke ì§ì ‘ í˜¸ì¶œ
                                search_results = self.search_tool.invoke({"query": query_to_search})
                            
                            # ê²€ìƒ‰ ê²°ê³¼ ì²˜ë¦¬ ë° ë¶„ì„
                            if search_results:
                                logger.info(f"[DIRECT_RESPONSE] Retrieved {len(search_results) if isinstance(search_results, list) else 1} web results")
                                
                                # 1. ê²€ìƒ‰ ê²°ê³¼ êµ¬ì¡°í™” ë¶„ì„
                                analysis = self.analyze_search_results(query, search_results if isinstance(search_results, list) else [search_results])
                                
                                # 2. ê²€ìƒ‰ ê²°ê³¼ í…ìŠ¤íŠ¸ ì¤€ë¹„ (ì „ì²´ ë‚´ìš© ì‚¬ìš©)
                                if isinstance(search_results, list):
                                    results_text = "\n\n".join([
                                        f"Source: {doc.metadata.get('source', 'Web')}\nTitle: {doc.metadata.get('title', 'N/A')}\nContent: {doc.page_content}"
                                        for doc in search_results  # ëª¨ë“  ê²°ê³¼ ì‚¬ìš©, ë‚´ìš© ì¶•ì†Œ ì—†ìŒ
                                    ])
                                else:
                                    results_text = str(search_results)
                                
                                # 3. Chain of Thought í”„ë¡¬í”„íŠ¸ ìƒì„±
                                if analysis:
                                    # ë¶„ì„ ê²°ê³¼ì— ê¸°ë°˜í•œ CoT í”„ë¡¬í”„íŠ¸
                                    cot_prompt = f"""Based on your search and analysis, think through this step by step:

QUERY: {query}

STEP 1 - Query Understanding:
This query is {'time-sensitive and requires current information' if analysis.is_time_sensitive else 'a general knowledge query'}.

STEP 2 - Search Results Analysis:
Key facts from search: {'; '.join(analysis.key_facts[:5]) if analysis.key_facts else 'No specific facts extracted'}
Primary answer from search: {analysis.primary_answer}
Confidence level: {analysis.confidence_level}

STEP 3 - Information Reconciliation:
{'IMPORTANT: This is current/recent information that supersedes your training data.' if analysis.should_override_base_knowledge else 'Consider both search results and base knowledge.'}
Reasoning: {analysis.reasoning}

STEP 4 - Final Answer:
Based on the analysis above, provide a comprehensive answer that:
1. {'Prioritizes the search results as the authoritative source' if analysis.should_override_base_knowledge else 'Balances search results with established knowledge'}
2. Includes specific details from the search results
3. Mentions the document system information as required

Remember to respond in the same language as the query."""
                                    
                                    # CoT í”„ë¡¬í”„íŠ¸ë¥¼ ì‹œìŠ¤í…œ ë©”ì‹œì§€ë¡œ ì¶”ê°€
                                    conversation_messages.append(response)  # AIì˜ tool call ë©”ì‹œì§€
                                    conversation_messages.append(ToolMessage(
                                        content=f"Web search results:\n{results_text}",
                                        tool_call_id=tool_id
                                    ))
                                    conversation_messages.append(SystemMessage(content=cot_prompt))
                                else:
                                    # ë¶„ì„ ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ ì²˜ë¦¬
                                    logger.warning(f"[DIRECT_RESPONSE] Search result analysis failed, using basic approach")
                                    conversation_messages.append(response)
                                    conversation_messages.append(ToolMessage(
                                        content=f"Web search results:\n{results_text}",
                                        tool_call_id=tool_id
                                    ))
                                
                                # ìµœì¢… ì‘ë‹µ ìƒì„± (CoT ê¸°ë°˜)
                                final_response = self.llm.invoke(conversation_messages)
                            else:
                                logger.warning(f"[DIRECT_RESPONSE] No web search results found")
                                final_response = response
                            
                except Exception as e:
                    logger.error(f"[DIRECT_RESPONSE] Web search failed: {e}")
                    # Tool ì‹¤í–‰ ì‹¤íŒ¨ ì‹œ ì›ë˜ ì‘ë‹µ ì‚¬ìš©
                    final_response = response
            
            # MessagesStateê°€ ìë™ìœ¼ë¡œ ëˆ„ì í•˜ë¯€ë¡œ ìƒˆë¡œìš´ ë©”ì‹œì§€ë§Œ ë°˜í™˜
            # HumanMessageëŠ” ì´ë¯¸ stateì— ìˆìœ¼ë¯€ë¡œ AIMessageë§Œ ì¶”ê°€
            new_messages = [
                AIMessage(content=final_response.content)
            ]
            
            # ìµœì¢… ë¡œê¹…
            logger.info(f"[DIRECT_RESPONSE] Response generation complete")
            logger.info(f"[DIRECT_RESPONSE] â”œâ”€ Context messages: {len(conversation_messages)}")
            logger.info(f"[DIRECT_RESPONSE] â”œâ”€ Web search used: {tool_was_used}")
            if tool_was_used:
                logger.info(f"[DIRECT_RESPONSE] â”œâ”€ Search analysis: {'CoT applied' if analysis else 'Basic approach'}")
            logger.info(f"[DIRECT_RESPONSE] â””â”€ Response length: {len(final_response.content)} chars")
            
            return {
                "messages": new_messages,
                "final_answer": final_response.content,
                "workflow_status": "completed",
                "current_node": "direct_response",
                "metadata": {
                    **state.get("metadata", {}),
                    "direct_response": {
                        "query_type": "simple",
                        "model": self.llm.model_name,
                        "context_messages": len(conversation_messages),
                        "system_stats_included": True,
                        "web_search_enabled": self.web_search_enabled,
                        "web_search_used": tool_was_used
                    }
                }
            }
            
        except Exception as e:
            logger.error(f"[DIRECT_RESPONSE] Failed: {str(e)}")
            
            # ì—ëŸ¬ ì‹œì—ë„ ê¸°ì¡´ ë©”ì‹œì§€ ìœ ì§€
            existing_messages = state.get("messages", [])
            error_messages = existing_messages + [
                AIMessage(content=f"âŒ ì‘ë‹µ ìƒì„± ì‹¤íŒ¨: {str(e)}")
            ]
            
            return {
                "messages": error_messages,
                "error": str(e),
                "workflow_status": "failed",
                "current_node": "direct_response"
            }
    
    def invoke(self, state: Dict[str, Any]) -> Dict[str, Any]:
        """ë™ê¸° ì‹¤í–‰ (LangGraph í˜¸í™˜ì„±)"""
        logger.debug(f"[DIRECT_RESPONSE] Invoke called (sync wrapper)")
        return self.__call__(state)