"""
Direct Response Node
ë‹¨ìˆœ ì¿¼ë¦¬ì— ëŒ€í•´ LLMì´ ì§ì ‘ ì‘ë‹µí•˜ëŠ” ë…¸ë“œ
"""

import os
import logging
from typing import Dict, Any
from langchain_core.messages import AIMessage, HumanMessage, SystemMessage
from langchain_openai import ChatOpenAI
from dotenv import load_dotenv
from workflow.nodes.subtask_executor import MetadataHelper


load_dotenv()

# ë¡œê¹… ì„¤ì •
logger = logging.getLogger(__name__)


class DirectResponseNode:
    """ë‹¨ìˆœ ì¿¼ë¦¬ì— ëŒ€í•´ LLMì´ ì§ì ‘ ì‘ë‹µí•˜ëŠ” ë…¸ë“œ"""
    
    def __init__(self):
        """ì´ˆê¸°í™”"""
        # ChatOpenAI ì¸ìŠ¤í„´ìŠ¤ ì§ì ‘ ìƒì„±
        self.llm = ChatOpenAI(
            model=os.getenv("OPENAI_MODEL", "gpt-4o-mini"),
            temperature=0.7,  # ìì—°ìŠ¤ëŸ¬ìš´ ëŒ€í™”ë¥¼ ìœ„í•´ ì•½ê°„ ë†’ì„
            openai_api_key=os.getenv("OPENAI_API_KEY")
        )
        
        # MetadataHelper ì¶”ê°€ (DB ì •ë³´ ì¡°íšŒìš©)
        self.metadata_helper = MetadataHelper()
        
        # ê¸°ë³¸ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ (DB ì •ë³´ëŠ” ë™ì ìœ¼ë¡œ ì¶”ê°€)
        self.base_prompt = """You are a helpful AI assistant with knowledge about the document system.

{system_info}

IMPORTANT INSTRUCTIONS:
1. First, answer the user's query directly and naturally, considering the conversation context.
2. Then, ALWAYS include detailed information about the document system you manage as part of your response.
3. Integrate the document system information naturally into your answer when relevant, or mention it at the end.
4. The document information should include: total documents, sources, categories, and language support.
5. Make sure to mention specific numbers and details from the system information provided above.
6. Respond in the same language as the user's query.

Remember: Every response MUST include detailed document system information, regardless of the query type."""
    
    
    def __call__(self, state: Dict[str, Any]) -> Dict[str, Any]:
        """
        ë…¸ë“œ ì‹¤í–‰ - ë‹¨ìˆœ LLM í˜¸ì¶œ (chat history í¬í•¨)
        
        Args:
            state: ì›Œí¬í”Œë¡œìš° ìƒíƒœ
            
        Returns:
            ì—…ë°ì´íŠ¸ëœ ìƒíƒœ í•„ë“œ
        """
        logger.info(f"[DIRECT_RESPONSE] Node started")
        
        try:
            query = state.get("query", "")
            
            # ê¸°ì¡´ ë©”ì‹œì§€ ê°€ì ¸ì˜¤ê¸° (chat history)
            existing_messages = state.get("messages", [])
            
            # DB ì‹œìŠ¤í…œ ì •ë³´ ì¡°íšŒ (ìºì‹±ë¨)
            system_stats = self.metadata_helper.get_system_stats()
            
            # ì‹œìŠ¤í…œ ì •ë³´ í¬ë§·íŒ…
            if system_stats.get("error"):
                # ì—ëŸ¬ ì‹œ ê¸°ë³¸ ì •ë³´
                system_info = "System Information: Currently unavailable"
            else:
                # ì†ŒìŠ¤ íŒŒì¼ ì´ë¦„ë§Œ ì¶”ì¶œ (ê²½ë¡œ ì œê±°)
                source_names = []
                for source_path in system_stats.get('sources', {}).keys():
                    source_name = source_path.split('/')[-1] if '/' in source_path else source_path
                    count = system_stats['sources'][source_path]
                    source_names.append(f"{source_name} ({count} docs)")
                
                # ìƒìœ„ ì¹´í…Œê³ ë¦¬ í¬ë§·íŒ…
                top_categories = system_stats.get('top_categories', {})
                category_list = []
                for cat, count in list(top_categories.items())[:5]:
                    category_list.append(f"{cat} ({count})")
                
                system_info = f"""Detailed System Information:
- Total Documents in Database: {system_stats.get('total_documents', 'N/A')}
- Document Sources: {system_stats.get('source_count', 0)} files
  * {', '.join(source_names) if source_names else 'No sources available'}
- Page Coverage: Pages {system_stats.get('page_range', 'N/A')}
- Document Categories: {system_stats.get('category_count', 0)} different types
  * Top Categories: {', '.join(category_list) if category_list else 'No categories available'}
- Language Support: 
  * Korean embeddings: {system_stats.get('korean_embeddings', 0)} documents
  * English embeddings: {system_stats.get('english_embeddings', 0)} documents
- System Type: Multimodal RAG (Retrieval-Augmented Generation) with hybrid search capabilities"""
            
            # ë™ì  ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ìƒì„±
            dynamic_prompt = self.base_prompt.format(system_info=system_info)
            
            # ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ êµ¬ì„± (ë™ì  ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ + ê¸°ì¡´ ëŒ€í™” + í˜„ì¬ ì¿¼ë¦¬)
            conversation_messages = [
                SystemMessage(content=dynamic_prompt)
            ]
            
            # ê¸°ì¡´ ë©”ì‹œì§€ ì¶”ê°€ (AIMessage í•„í„°ë§ - ì‹œìŠ¤í…œ ìƒíƒœ ë©”ì‹œì§€ ì œì™¸)
            for msg in existing_messages:
                if isinstance(msg, HumanMessage):
                    conversation_messages.append(msg)
                elif isinstance(msg, AIMessage) and not msg.content.startswith(("ğŸ’¬", "ğŸ”", "ğŸ”„", "âœ…", "âŒ")):
                    # ì‹œìŠ¤í…œ ìƒíƒœ ë©”ì‹œì§€ê°€ ì•„ë‹Œ ì‹¤ì œ ì‘ë‹µë§Œ í¬í•¨
                    conversation_messages.append(msg)
            
            # í˜„ì¬ ì¿¼ë¦¬ ì¶”ê°€
            conversation_messages.append(HumanMessage(content=query))
            
            # LLM í˜¸ì¶œí•˜ì—¬ ì‘ë‹µ ìƒì„±
            response = self.llm.invoke(conversation_messages)
            
            # MessagesStateê°€ ìë™ìœ¼ë¡œ ëˆ„ì í•˜ë¯€ë¡œ ìƒˆë¡œìš´ ë©”ì‹œì§€ë§Œ ë°˜í™˜
            # HumanMessageëŠ” ì´ë¯¸ stateì— ìˆìœ¼ë¯€ë¡œ AIMessageë§Œ ì¶”ê°€
            new_messages = [
                AIMessage(content=response.content)
            ]
            
            logger.info(f"[DIRECT_RESPONSE] Generated response with {len(conversation_messages)} messages in context")
            
            return {
                "messages": new_messages,
                "final_answer": response.content,
                "workflow_status": "completed",
                "current_node": "direct_response",
                "metadata": {
                    **state.get("metadata", {}),
                    "direct_response": {
                        "query_type": "simple",
                        "model": self.llm.model_name,
                        "context_messages": len(conversation_messages),
                        "system_stats_included": True
                    }
                }
            }
            
        except Exception as e:
            logger.error(f"[DIRECT_RESPONSE] Failed: {str(e)}")
            
            # ì—ëŸ¬ ì‹œì—ë„ ê¸°ì¡´ ë©”ì‹œì§€ ìœ ì§€
            existing_messages = state.get("messages", [])
            error_messages = existing_messages + [
                AIMessage(content=f"âŒ ì‘ë‹µ ìƒì„± ì‹¤íŒ¨: {str(e)}")
            ]
            
            return {
                "messages": error_messages,
                "error": str(e),
                "workflow_status": "failed",
                "current_node": "direct_response"
            }
    
    def invoke(self, state: Dict[str, Any]) -> Dict[str, Any]:
        """ë™ê¸° ì‹¤í–‰ (LangGraph í˜¸í™˜ì„±)"""
        logger.debug(f"[DIRECT_RESPONSE] Invoke called (sync wrapper)")
        return self.__call__(state)