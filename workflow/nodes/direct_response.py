"""
Direct Response Node
ë‹¨ìˆœ ì¿¼ë¦¬ì— ëŒ€í•´ LLMì´ ì§ì ‘ ì‘ë‹µí•˜ëŠ” ë…¸ë“œ
"""

import os
import logging
from typing import Dict, Any
from langchain_core.messages import AIMessage
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from dotenv import load_dotenv
import nest_asyncio

# Apply nest_asyncio to allow nested event loops
nest_asyncio.apply()

load_dotenv()

# ë¡œê¹… ì„¤ì •
logger = logging.getLogger(__name__)


class DirectResponseNode:
    """ë‹¨ìˆœ ì¿¼ë¦¬ì— ëŒ€í•´ LLMì´ ì§ì ‘ ì‘ë‹µí•˜ëŠ” ë…¸ë“œ"""
    
    def __init__(self):
        """ì´ˆê¸°í™”"""
        self.llm = ChatOpenAI(
            model=os.getenv("OPENAI_MODEL", "gpt-4o-mini"),
            temperature=0.7,  # ìì—°ìŠ¤ëŸ¬ìš´ ëŒ€í™”ë¥¼ ìœ„í•´ ì•½ê°„ ë†’ì„
            openai_api_key=os.getenv("OPENAI_API_KEY")
        )
        
        self.response_prompt = ChatPromptTemplate.from_messages([
            ("system", """You are a helpful AI assistant. 
Answer the user's query directly and naturally.
If asked about automobile manufacturing, mention that you can help with technical questions if they search the documentation.
Keep responses concise and friendly.
Respond in the same language as the user's query."""),
            ("human", "{query}")
        ])
    
    async def __call__(self, state: Dict[str, Any]) -> Dict[str, Any]:
        """
        ë…¸ë“œ ì‹¤í–‰ - ë‹¨ìˆœ LLM í˜¸ì¶œ
        
        Args:
            state: ì›Œí¬í”Œë¡œìš° ìƒíƒœ
            
        Returns:
            ì—…ë°ì´íŠ¸ëœ ìƒíƒœ í•„ë“œ
        """
        logger.info(f"[DIRECT_RESPONSE] Node started")
        
        try:
            query = state.get("query", "")
            
            messages = [
                AIMessage(content="ğŸ’¬ ì‘ë‹µì„ ìƒì„±í•˜ê³  ìˆìŠµë‹ˆë‹¤...")
            ]
            
            # LLM í˜¸ì¶œí•˜ì—¬ ì‘ë‹µ ìƒì„±
            response = await self.llm.ainvoke(
                self.response_prompt.format_messages(query=query)
            )
            
            messages.append(
                AIMessage(content=response.content)
            )
            
            logger.info(f"[DIRECT_RESPONSE] Generated response")
            
            return {
                "messages": messages,
                "final_answer": response.content,
                "workflow_status": "completed",
                "current_node": "direct_response",
                "metadata": {
                    **state.get("metadata", {}),
                    "direct_response": {
                        "query_type": "simple",
                        "model": self.llm.model_name
                    }
                }
            }
            
        except Exception as e:
            logger.error(f"[DIRECT_RESPONSE] Failed: {str(e)}")
            return {
                "messages": [
                    AIMessage(content=f"âŒ ì‘ë‹µ ìƒì„± ì‹¤íŒ¨: {str(e)}")
                ],
                "error": str(e),
                "workflow_status": "failed",
                "current_node": "direct_response"
            }
    
    def invoke(self, state: Dict[str, Any]) -> Dict[str, Any]:
        """ë™ê¸° ì‹¤í–‰ (LangGraph í˜¸í™˜ì„±)"""
        logger.debug(f"[DIRECT_RESPONSE] Invoke called (sync wrapper)")
        
        import asyncio
        from concurrent.futures import ThreadPoolExecutor
        
        try:
            # ì´ë¯¸ ì‹¤í–‰ ì¤‘ì¸ ì´ë²¤íŠ¸ ë£¨í”„ê°€ ìˆëŠ”ì§€ í™•ì¸
            loop = asyncio.get_running_loop()
            logger.debug(f"[DIRECT_RESPONSE] Event loop detected, using ThreadPoolExecutor")
        except RuntimeError:
            # ì´ë²¤íŠ¸ ë£¨í”„ê°€ ì—†ìœ¼ë©´ ìƒˆë¡œ ìƒì„±í•˜ì—¬ ì‹¤í–‰
            logger.debug(f"[DIRECT_RESPONSE] No event loop, creating new one")
            return asyncio.run(self.__call__(state))
        else:
            # ì´ë¯¸ ì´ë²¤íŠ¸ ë£¨í”„ê°€ ì‹¤í–‰ ì¤‘ì´ë©´ ë³„ë„ ìŠ¤ë ˆë“œì—ì„œ ì‹¤í–‰
            with ThreadPoolExecutor(max_workers=1) as executor:
                future = executor.submit(asyncio.run, self.__call__(state))
                return future.result()