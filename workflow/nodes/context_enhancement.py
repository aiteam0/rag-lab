"""
Context Enhancement Node
LLMì´ ëŒ€í™” íˆìŠ¤í† ë¦¬ë¥¼ ë³´ê³  ì°¸ì¡°ë¥¼ í•´ê²°í•˜ëŠ” ë…¸ë“œ
"""

import os
import logging
from typing import Dict, Any
from langchain_core.messages import AIMessage, HumanMessage
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from dotenv import load_dotenv
import nest_asyncio

# Apply nest_asyncio to allow nested event loops
nest_asyncio.apply()

load_dotenv()

# ë¡œê¹… ì„¤ì •
logger = logging.getLogger(__name__)


class ContextEnhancementNode:
    """LLMì´ ëŒ€í™” íˆìŠ¤í† ë¦¬ë¥¼ ë³´ê³  ì°¸ì¡°ë¥¼ í•´ê²°í•˜ëŠ” ë…¸ë“œ"""
    
    def __init__(self):
        """ì´ˆê¸°í™”"""
        self.llm = ChatOpenAI(
            model=os.getenv("OPENAI_MODEL", "gpt-4o-mini"),
            temperature=0,
            openai_api_key=os.getenv("OPENAI_API_KEY")
        )
        
        self.enhancement_prompt = ChatPromptTemplate.from_messages([
            ("system", """You are analyzing a follow-up query that references previous conversation.

Your task: Create a self-contained query by resolving references using conversation history.

Process:
1. Identify what the user is referring to (ì´ì „ì—, ì•„ê¹Œ, that, it, etc.)
2. Find the relevant context from previous messages
3. Create a complete query that doesn't need context to understand
4. Preserve the original intent and question

Important:
- Only add necessary context, don't over-elaborate
- Keep technical terms accurate
- The enhanced query should be searchable in documents
- If the query is already self-contained, return it as is

Example:
- History: "GV80 ì—”ì§„ì˜ ì¡°ë¦½ ê³µì •ì„ ì•Œë ¤ì¤˜"
- Follow-up: "ê±°ê¸°ì— í•„ìš”í•œ ë„êµ¬ëŠ”?"
- Enhanced: "GV80 ì—”ì§„ ì¡°ë¦½ ê³µì •ì— í•„ìš”í•œ ë„êµ¬"

Another example:
- History: "ë°°í„°ë¦¬ ìƒì‚° ë¼ì¸ì˜ íš¨ìœ¨ì„± ë³´ê³ ì„œ"
- Follow-up: "ì•„ê¹Œ ë§í•œ ê²ƒì˜ ì‘ë…„ ëŒ€ë¹„ ê°œì„ ìœ¨"
- Enhanced: "ë°°í„°ë¦¬ ìƒì‚° ë¼ì¸ íš¨ìœ¨ì„±ì˜ ì‘ë…„ ëŒ€ë¹„ ê°œì„ ìœ¨"
"""),
            ("human", """Original query: {query}

Recent conversation:
{conversation_history}

Create a self-contained enhanced query:""")
        ])
    
    async def __call__(self, state: Dict[str, Any]) -> Dict[str, Any]:
        """
        ë…¸ë“œ ì‹¤í–‰
        
        Args:
            state: ì›Œí¬í”Œë¡œìš° ìƒíƒœ
            
        Returns:
            ì—…ë°ì´íŠ¸ëœ ìƒíƒœ í•„ë“œ
        """
        logger.info(f"[CONTEXT_ENHANCEMENT] Node started")
        
        try:
            query = state.get("query", "")
            messages = state.get("messages", [])
            
            progress_messages = [
                AIMessage(content="ğŸ”„ ì´ì „ ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ë¥¼ ë¶„ì„í•˜ê³  ìˆìŠµë‹ˆë‹¤...")
            ]
            
            # ëŒ€í™” íˆìŠ¤í† ë¦¬ ì¤€ë¹„
            conversation_history = []
            for msg in messages[-10:] if messages else []:  # ìµœê·¼ 10ê°œ ë©”ì‹œì§€
                if isinstance(msg, HumanMessage):
                    conversation_history.append(f"User: {msg.content}")
                elif isinstance(msg, AIMessage) and not str(msg.content).startswith("ğŸ”„"):
                    # ì§„í–‰ ìƒí™© ë©”ì‹œì§€ ì œì™¸
                    content_preview = str(msg.content)[:200] if hasattr(msg, 'content') else str(msg)[:200]
                    conversation_history.append(f"Assistant: {content_preview}")
            
            history_text = "\n".join(conversation_history) if conversation_history else "No previous conversation"
            
            # ì¿¼ë¦¬ ê°œì„ 
            response = await self.llm.ainvoke(
                self.enhancement_prompt.format_messages(
                    query=query,
                    conversation_history=history_text
                )
            )
            
            enhanced_query = response.content.strip()
            logger.info(f"[CONTEXT_ENHANCEMENT] Enhanced: '{query}' â†’ '{enhanced_query}'")
            
            progress_messages.append(
                AIMessage(content=f"âœ… ì»¨í…ìŠ¤íŠ¸ ì ìš© ì™„ë£Œ: {enhanced_query}")
            )
            
            return {
                "messages": progress_messages,
                "enhanced_query": enhanced_query,  # ê°œì„ ëœ ì¿¼ë¦¬ ì €ì¥
                "query": enhanced_query,  # Planning ë…¸ë“œë¥¼ ìœ„í•´ queryë„ ì—…ë°ì´íŠ¸
                "current_node": "context_enhancement",
                "metadata": {
                    **state.get("metadata", {}),
                    "context_enhancement": {
                        "original_query": query,
                        "enhanced_query": enhanced_query,
                        "history_used": len(conversation_history)
                    }
                }
            }
            
        except Exception as e:
            logger.error(f"[CONTEXT_ENHANCEMENT] Failed: {str(e)}")
            # ì‹¤íŒ¨ì‹œ ì›ë³¸ ì¿¼ë¦¬ ì‚¬ìš©
            return {
                "messages": [
                    AIMessage(content=f"âš ï¸ ì»¨í…ìŠ¤íŠ¸ ê°œì„  ì‹¤íŒ¨, ì›ë³¸ ì¿¼ë¦¬ ì‚¬ìš©: {state.get('query', '')}")
                ],
                "current_node": "context_enhancement",
                "enhanced_query": state.get("query", ""),  # ì›ë³¸ ì¿¼ë¦¬ ìœ ì§€
                "error": None  # ì—ëŸ¬ë¥¼ ì „íŒŒí•˜ì§€ ì•Šê³  ê³„ì† ì§„í–‰
            }
    
    def invoke(self, state: Dict[str, Any]) -> Dict[str, Any]:
        """ë™ê¸° ì‹¤í–‰ (LangGraph í˜¸í™˜ì„±)"""
        logger.debug(f"[CONTEXT_ENHANCEMENT] Invoke called (sync wrapper)")
        
        import asyncio
        from concurrent.futures import ThreadPoolExecutor
        
        try:
            # ì´ë¯¸ ì‹¤í–‰ ì¤‘ì¸ ì´ë²¤íŠ¸ ë£¨í”„ê°€ ìˆëŠ”ì§€ í™•ì¸
            loop = asyncio.get_running_loop()
            logger.debug(f"[CONTEXT_ENHANCEMENT] Event loop detected, using ThreadPoolExecutor")
        except RuntimeError:
            # ì´ë²¤íŠ¸ ë£¨í”„ê°€ ì—†ìœ¼ë©´ ìƒˆë¡œ ìƒì„±í•˜ì—¬ ì‹¤í–‰
            logger.debug(f"[CONTEXT_ENHANCEMENT] No event loop, creating new one")
            return asyncio.run(self.__call__(state))
        else:
            # ì´ë¯¸ ì´ë²¤íŠ¸ ë£¨í”„ê°€ ì‹¤í–‰ ì¤‘ì´ë©´ ë³„ë„ ìŠ¤ë ˆë“œì—ì„œ ì‹¤í–‰
            with ThreadPoolExecutor(max_workers=1) as executor:
                future = executor.submit(asyncio.run, self.__call__(state))
                return future.result()