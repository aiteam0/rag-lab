"""
Query Router Node
LLM reasoningì„ ì‚¬ìš©í•˜ì—¬ ì¿¼ë¦¬ë¥¼ ë¶„ë¥˜í•˜ê³  ì ì ˆí•œ ì²˜ë¦¬ ê²½ë¡œë¥¼ ê²°ì •í•˜ëŠ” ë…¸ë“œ
"""

import os
import logging
from typing import Dict, Any, List, Optional
from langchain_core.messages import AIMessage, HumanMessage
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from pydantic import BaseModel, Field
from typing import Literal
from dotenv import load_dotenv
import nest_asyncio

# Apply nest_asyncio to allow nested event loops
nest_asyncio.apply()

load_dotenv()

# ë¡œê¹… ì„¤ì •
logger = logging.getLogger(__name__)


class QueryClassification(BaseModel):
    """LLMì´ ì¶”ë¡ í•œ ì¿¼ë¦¬ ë¶„ë¥˜ ê²°ê³¼"""
    type: Literal[
        "simple",              # RAG ë¶ˆí•„ìš”, LLM ì§ì ‘ ë‹µë³€ ê°€ëŠ¥
        "rag_required",        # ìë™ì°¨ ì œì¡° ë¬¸ì„œ ê²€ìƒ‰ í•„ìš”
        "history_required"     # ì´ì „ ëŒ€í™” ì°¸ì¡° í•„ìš”
    ]
    reasoning: str = Field(description="LLMì˜ ì¶”ë¡  ê³¼ì •")
    confidence: float = Field(ge=0.0, le=1.0, description="ë¶„ë¥˜ ì‹ ë¢°ë„")


class QueryRouterNode:
    """LLM reasoningìœ¼ë¡œ ì¿¼ë¦¬ë¥¼ ë¶„ë¥˜í•˜ê³  ë¼ìš°íŒ…í•˜ëŠ” ë…¸ë“œ"""
    
    def __init__(self):
        """ì´ˆê¸°í™”"""
        self.llm = ChatOpenAI(
            model=os.getenv("OPENAI_MODEL", "gpt-4o-mini"),
            temperature=0,
            openai_api_key=os.getenv("OPENAI_API_KEY")
        )
        
        self.classification_prompt = ChatPromptTemplate.from_messages([
            ("system", """You are an intelligent query classifier using reasoning, not pattern matching.

Analyze the query and recent conversation to determine the type:

1. **simple**: Query that can be answered with general knowledge without searching documents
   - Greetings, casual chat, general knowledge questions
   - Questions unrelated to automobile manufacturing
   
2. **rag_required**: Query that needs to search automobile manufacturing documents
   - Technical specifications, manufacturing processes
   - Quality standards, safety procedures
   - Any domain-specific information
   
3. **history_required**: Query that references previous conversation
   - Contains references like "ì´ì „ì—", "ì•„ê¹Œ", "ìœ„ì—ì„œ", "that", "it", etc.
   - Needs context from earlier messages to be fully understood
   - After resolving references, might still need RAG

Use reasoning to decide, not keyword matching. Consider:
- Does this require domain-specific knowledge from documents?
- Are there unresolved references to previous conversation?
- Can I answer this with general knowledge alone?

Provide your reasoning process in the 'reasoning' field."""),
            ("human", """Query: {query}

Recent messages (for context):
{recent_messages}

Classify this query.""")
        ])
    
    async def __call__(self, state: Dict[str, Any]) -> Dict[str, Any]:
        """
        ë…¸ë“œ ì‹¤í–‰
        
        Args:
            state: ì›Œí¬í”Œë¡œìš° ìƒíƒœ
            
        Returns:
            ì—…ë°ì´íŠ¸ëœ ìƒíƒœ í•„ë“œ
        """
        logger.info(f"[QUERY_ROUTER] Node started")
        
        try:
            query = state.get("query", "")
            messages = state.get("messages", [])
            
            # ì§„í–‰ ìƒí™© ë©”ì‹œì§€
            progress_messages = [
                AIMessage(content="ğŸ” ì¿¼ë¦¬ë¥¼ ë¶„ì„í•˜ê³  ìˆìŠµë‹ˆë‹¤...")
            ]
            
            # ìµœê·¼ ë©”ì‹œì§€ ì»¨í…ìŠ¤íŠ¸ ì¤€ë¹„ (ìµœëŒ€ 5ê°œ)
            recent_messages = []
            for msg in messages[-10:] if messages else []:
                if isinstance(msg, (HumanMessage, AIMessage)):
                    msg_preview = str(msg.content)[:100] if hasattr(msg, 'content') else str(msg)[:100]
                    recent_messages.append(f"{msg.__class__.__name__}: {msg_preview}")
            recent_context = "\n".join(recent_messages) if recent_messages else "No previous messages"
            
            # LLMìœ¼ë¡œ ë¶„ë¥˜
            structured_llm = self.llm.with_structured_output(QueryClassification)
            classification = await structured_llm.ainvoke(
                self.classification_prompt.format_messages(
                    query=query,
                    recent_messages=recent_context
                )
            )
            
            logger.info(f"[QUERY_ROUTER] Classification: {classification.type} (confidence: {classification.confidence:.2f})")
            logger.debug(f"[QUERY_ROUTER] Reasoning: {classification.reasoning}")
            
            # ì™„ë£Œ ë©”ì‹œì§€
            type_emoji = {
                "simple": "ğŸ’¬",
                "rag_required": "ğŸ­",
                "history_required": "ğŸ”„"
            }
            
            progress_messages.append(
                AIMessage(content=f"{type_emoji.get(classification.type, 'â“')} ì¿¼ë¦¬ íƒ€ì…: {classification.type}")
            )
            
            # State ì—…ë°ì´íŠ¸
            result = {
                "messages": progress_messages,
                "query_type": classification.type,
                "current_node": "query_router",
                "metadata": {
                    **state.get("metadata", {}),
                    "query_classification": {
                        "type": classification.type,
                        "confidence": classification.confidence,
                        "reasoning": classification.reasoning
                    }
                }
            }
            
            logger.info(f"[QUERY_ROUTER] Node completed successfully")
            return result
            
        except Exception as e:
            logger.error(f"[QUERY_ROUTER] Failed: {str(e)}")
            return {
                "messages": [
                    AIMessage(content=f"âŒ ì¿¼ë¦¬ ë¶„ë¥˜ ì‹¤íŒ¨: {str(e)}")
                ],
                "error": str(e),
                "query_type": "rag_required",  # ê¸°ë³¸ê°’ìœ¼ë¡œ RAG ì²˜ë¦¬
                "current_node": "query_router"
            }
    
    def invoke(self, state: Dict[str, Any]) -> Dict[str, Any]:
        """ë™ê¸° ì‹¤í–‰ (LangGraph í˜¸í™˜ì„±)"""
        logger.debug(f"[QUERY_ROUTER] Invoke called (sync wrapper)")
        
        import asyncio
        from concurrent.futures import ThreadPoolExecutor
        
        try:
            # ì´ë¯¸ ì‹¤í–‰ ì¤‘ì¸ ì´ë²¤íŠ¸ ë£¨í”„ê°€ ìˆëŠ”ì§€ í™•ì¸
            loop = asyncio.get_running_loop()
            logger.debug(f"[QUERY_ROUTER] Event loop detected, using ThreadPoolExecutor")
        except RuntimeError:
            # ì´ë²¤íŠ¸ ë£¨í”„ê°€ ì—†ìœ¼ë©´ ìƒˆë¡œ ìƒì„±í•˜ì—¬ ì‹¤í–‰
            logger.debug(f"[QUERY_ROUTER] No event loop, creating new one")
            return asyncio.run(self.__call__(state))
        else:
            # ì´ë¯¸ ì´ë²¤íŠ¸ ë£¨í”„ê°€ ì‹¤í–‰ ì¤‘ì´ë©´ ë³„ë„ ìŠ¤ë ˆë“œì—ì„œ ì‹¤í–‰
            with ThreadPoolExecutor(max_workers=1) as executor:
                future = executor.submit(asyncio.run, self.__call__(state))
                return future.result()