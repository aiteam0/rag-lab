"""
Answer Grader Node (CRAG)
ìƒì„±ëœ ë‹µë³€ì˜ ì „ì²´ì ì¸ í’ˆì§ˆì„ í‰ê°€í•˜ëŠ” ë…¸ë“œ
"""

import os
import logging
from typing import Dict, Any, List
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.documents import Document
from langchain_core.messages import AIMessage
from pydantic import BaseModel, Field
from dotenv import load_dotenv


from workflow.state import MVPWorkflowState, QualityCheckResult

load_dotenv()

# ë¡œê¹… ì„¤ì •
logger = logging.getLogger(__name__)


class AnswerGradeResult(BaseModel):
    """ë‹µë³€ í‰ê°€ ê²°ê³¼"""
    overall_score: float = Field(description="Overall quality score (0.0-1.0)")
    completeness_score: float = Field(description="How completely the answer addresses the query (0.0-1.0)")
    relevance_score: float = Field(description="How relevant the answer is to the query (0.0-1.0)")
    clarity_score: float = Field(description="How clear and well-structured the answer is (0.0-1.0)")
    usefulness_score: float = Field(description="How useful/actionable the answer is (0.0-1.0)")
    missing_aspects: List[str] = Field(description="List of query aspects not addressed in the answer")
    improvement_suggestions: List[str] = Field(description="Specific suggestions to improve the answer")
    strengths: List[str] = Field(description="Strong points of the answer")
    reasoning: str = Field(description="Detailed reasoning for the assessment")


class AnswerGraderNode:
    """ë‹µë³€ í’ˆì§ˆ í‰ê°€ ë…¸ë“œ - CRAG íŒ¨í„´"""
    
    def __init__(self):
        """ì´ˆê¸°í™”"""
        # ChatOpenAI ì¸ìŠ¤í„´ìŠ¤ ì§ì ‘ ìƒì„±
        self.llm = ChatOpenAI(
            model=os.getenv("OPENAI_MODEL", "gpt-4o-mini"),
            temperature=0,  # ì¼ê´€ëœ í‰ê°€ë¥¼ ìœ„í•´ temperature 0
            openai_api_key=os.getenv("OPENAI_API_KEY")
        )
        
        # ë‹µë³€ í‰ê°€ ì„ê³„ê°’ (.envì—ì„œ ì½ê¸°)
        self.threshold = float(os.getenv("CRAG_ANSWER_GRADE_THRESHOLD", "0.6"))
        
        # ë‹µë³€ í‰ê°€ í”„ë¡¬í”„íŠ¸
        self.grading_prompt = ChatPromptTemplate.from_messages([
            ("system", """You are a quality evaluator for a RAG system specializing in automobile manufacturing documentation.
Your task is to evaluate the overall quality of generated answers.

Evaluation Criteria:

1. COMPLETENESS (0.0-1.0):
   - 1.0: Answer addresses ALL aspects of the query comprehensively
   - 0.7-0.9: Most aspects covered with good detail
   - 0.4-0.6: Some aspects covered but missing important details
   - 0.0-0.3: Major aspects missing or incomplete

2. RELEVANCE (0.0-1.0):
   - 1.0: Answer directly and precisely addresses the query
   - 0.7-0.9: Mostly relevant with minor digressions
   - 0.4-0.6: Partially relevant but includes unnecessary information
   - 0.0-0.3: Answer misses the point or addresses wrong topic

3. CLARITY (0.0-1.0):
   - 1.0: Crystal clear, well-structured, easy to follow
   - 0.7-0.9: Clear with minor structural issues
   - 0.4-0.6: Somewhat clear but could be better organized
   - 0.0-0.3: Confusing, poorly structured, hard to understand

4. USEFULNESS (0.0-1.0):
   - 1.0: Highly actionable with specific steps/information
   - 0.7-0.9: Useful with most necessary information
   - 0.4-0.6: Somewhat useful but lacks specifics
   - 0.0-0.3: Not useful, too vague or generic

Overall Score Calculation:
- Completeness: 35% weight
- Relevance: 30% weight
- Clarity: 20% weight
- Usefulness: 15% weight

Be specific in identifying:
- What aspects of the query were not addressed
- Concrete suggestions for improvement
- Strong points that should be maintained

For vehicle manual queries, pay special attention to:
- Safety warnings and cautions
- Specific procedures and steps
- Technical specifications
- Maintenance schedules
- Part numbers or references
- Proper citations with reference numbers [1], [2], etc.
- Completeness of References section at the end"""),
            ("human", """Original Query: {query}

Generated Answer:
{answer}

Source Documents Available:
{documents_summary}

Evaluate the answer quality comprehensively.
Identify strengths, weaknesses, and specific improvements needed.""")
        ])
        
    def _summarize_documents(self, documents: List[Document]) -> str:
        """
        ë¬¸ì„œ ìš”ì•½ ìƒì„± (í‰ê°€ ì°¸ê³ ìš©)
        
        Args:
            documents: ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
            
        Returns:
            ë¬¸ì„œ ìš”ì•½ í…ìŠ¤íŠ¸
        """
        if not documents:
            return "No source documents available"
        
        summary = f"Total documents: {len(documents)}\n"
        
        # ì¹´í…Œê³ ë¦¬ë³„ ë¬¸ì„œ ìˆ˜ ì§‘ê³„
        categories = {}
        sources = set()
        pages = set()
        
        for doc in documents:
            category = doc.metadata.get("category", "unknown")
            categories[category] = categories.get(category, 0) + 1
            sources.add(doc.metadata.get("source", "unknown"))
            pages.add(doc.metadata.get("page", 0))
        
        summary += f"Sources: {', '.join(sources)}\n"
        summary += f"Pages covered: {len(pages)} pages\n"
        summary += "Document categories:\n"
        for category, count in categories.items():
            summary += f"  - {category}: {count} documents\n"
        
        # ìƒìœ„ 3ê°œ ë¬¸ì„œì˜ ì£¼ìš” ë‚´ìš© í¬í•¨
        summary += "\nKey content from top documents:\n"
        for idx, doc in enumerate(documents[:3], 1):
            content_preview = doc.page_content[:200] + "..." if len(doc.page_content) > 200 else doc.page_content
            summary += f"{idx}. [{doc.metadata.get('category', 'unknown')}] {content_preview}\n"
        
        return summary
    
    def _calculate_overall_score(self, grade_result: AnswerGradeResult) -> float:
        """
        ê°€ì¤‘ì¹˜ë¥¼ ì ìš©í•œ ì „ì²´ ì ìˆ˜ ê³„ì‚°
        
        Args:
            grade_result: í‰ê°€ ê²°ê³¼
            
        Returns:
            ì „ì²´ ì ìˆ˜ (0.0-1.0)
        """
        weights = {
            "completeness": 0.35,
            "relevance": 0.30,
            "clarity": 0.20,
            "usefulness": 0.15
        }
        
        overall = (
            grade_result.completeness_score * weights["completeness"] +
            grade_result.relevance_score * weights["relevance"] +
            grade_result.clarity_score * weights["clarity"] +
            grade_result.usefulness_score * weights["usefulness"]
        )
        
        return min(overall, 1.0)
    
    
    def __call__(self, state: MVPWorkflowState) -> Dict[str, Any]:
        """
        ë…¸ë“œ ì‹¤í–‰
        
        Args:
            state: ì›Œí¬í”Œë¡œìš° ìƒíƒœ
            
        Returns:
            ì—…ë°ì´íŠ¸ëœ ìƒíƒœ í•„ë“œ
        """
        logger.info(f"[ANSWER_GRADER] Node started")
        try:
            # í‰ê°€í•  ë‹µë³€ ê°€ì ¸ì˜¤ê¸°
            answer_to_grade = state.get("intermediate_answer") or state.get("final_answer")
            
            if not answer_to_grade:
                return {
                    "answer_grade": {
                        "is_valid": False,
                        "score": 0.0,
                        "reason": "No answer to grade",
                        "suggestions": ["Generate answer first"],
                        "needs_retry": False
                    },
                    "warnings": ["No answer available for grading"]
                }
            
            # ì›ë³¸ ì¿¼ë¦¬
            query = state["query"]
            
            # ì‚¬ìš©ëœ ë¬¸ì„œë“¤
            documents = state.get("documents", [])
            
            # CRITICAL: documentsê°€ Noneì´ë©´ ì¦‰ì‹œ ì‹¤íŒ¨
            if documents is None:
                raise ValueError(
                    "CRITICAL ERROR: documents is None in answer grader. "
                    "Previous nodes must provide documents list, not None."
                )
            
            # CRITICAL: documentsê°€ ë¹ˆ ë¦¬ìŠ¤íŠ¸ë©´ ì¦‰ì‹œ ì‹¤íŒ¨
            if not documents:
                raise ValueError(
                    "CRITICAL ERROR: Empty documents list in answer grader. "
                    "Retrieval must provide at least one document. "
                    "Cannot grade answer without source documents."
                )
            
            # ë¬¸ì„œ ìš”ì•½ ìƒì„±
            documents_summary = self._summarize_documents(documents)
            
            # LLMì„ ì‚¬ìš©í•œ ë‹µë³€ í‰ê°€
            structured_llm = self.llm.with_structured_output(
                AnswerGradeResult
            )
            
            grade_result = structured_llm.invoke(
                self.grading_prompt.format_messages(
                    query=query,
                    answer=answer_to_grade,
                    documents_summary=documents_summary
                )
            )
            
            # ì „ì²´ ì ìˆ˜ ê³„ì‚° (ê°€ì¤‘ í‰ê· )
            overall_score = self._calculate_overall_score(grade_result)
            
            # 4ì°¨ì› í’ˆì§ˆ ì ìˆ˜ ìƒì„¸ ë¡œê¹…
            logger.info(f"[ANSWER_GRADER] Overall: {overall_score:.3f} | Completeness: {grade_result.completeness_score:.3f} | Relevance: {grade_result.relevance_score:.3f} | Clarity: {grade_result.clarity_score:.3f}")
            logger.info(f"[ANSWER_GRADER] Usefulness: {grade_result.usefulness_score:.3f}")
            
            if grade_result.missing_aspects:
                missing_count = len(grade_result.missing_aspects)
                missing_preview = ' | '.join([aspect[:25] + '...' if len(aspect) > 25 else aspect for aspect in grade_result.missing_aspects[:2]])
                logger.info(f"[ANSWER_GRADER] {missing_count} missing aspects: {missing_preview}")
            
            if grade_result.improvement_suggestions:
                suggestions_count = len(grade_result.improvement_suggestions)
                logger.info(f"[ANSWER_GRADER] {suggestions_count} improvement suggestions available")
            
            # ì¬ì‹œë„ í•„ìš” ì—¬ë¶€ ê²°ì •
            needs_retry = overall_score < self.threshold
            
            # ê°œì„ ì´ í•„ìš”í•œ ì£¼ìš” ì˜ì—­ ì‹ë³„
            low_scores = []
            if grade_result.completeness_score < 0.6:
                low_scores.append(f"Completeness: {grade_result.completeness_score:.2f}")
            if grade_result.relevance_score < 0.6:
                low_scores.append(f"Relevance: {grade_result.relevance_score:.2f}")
            if grade_result.clarity_score < 0.6:
                low_scores.append(f"Clarity: {grade_result.clarity_score:.2f}")
            if grade_result.usefulness_score < 0.6:
                low_scores.append(f"Usefulness: {grade_result.usefulness_score:.2f}")
            
            # QualityCheckResult í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            quality_check: QualityCheckResult = {
                "is_valid": overall_score >= self.threshold,
                "score": overall_score,
                "reason": grade_result.reasoning,
                "suggestions": grade_result.improvement_suggestions,
                "needs_retry": needs_retry
            }
            
            # ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸
            metadata = state.get("metadata", {})
            metadata["answer_grade"] = {
                "overall_score": overall_score,
                "completeness": grade_result.completeness_score,
                "relevance": grade_result.relevance_score,
                "clarity": grade_result.clarity_score,
                "usefulness": grade_result.usefulness_score,
                "missing_aspects": grade_result.missing_aspects,
                "strengths": grade_result.strengths,
                "low_scores": low_scores,
                "threshold": self.threshold
            }
            
            # ê²½ê³  ì¶”ê°€ (í•„ìš”ì‹œ)
            warnings = []
            if overall_score < 0.5:
                warnings.append(f"Low answer quality: {overall_score:.2f}")
            if grade_result.missing_aspects:
                warnings.append(f"Missing aspects: {', '.join(grade_result.missing_aspects[:3])}")
            
            # ë©”ì‹œì§€ ìƒì„± - í’ˆì§ˆ í‰ê°€ ê²°ê³¼ ë° ìµœì¢… ë‹µë³€
            messages = []
            
            # 1. í‰ê°€ ì‹œì‘ ë©”ì‹œì§€
            messages.append(
                AIMessage(content="ğŸ” ë‹µë³€ í’ˆì§ˆ í‰ê°€ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")
            )
            
            # 2. í‰ê°€ ì ìˆ˜ ìƒì„¸
            messages.append(
                AIMessage(content=f"""ğŸ“Š í’ˆì§ˆ í‰ê°€ ê²°ê³¼:
  â€¢ ì™„ì „ì„±: {grade_result.completeness_score:.0%}
  â€¢ ê´€ë ¨ì„±: {grade_result.relevance_score:.0%}
  â€¢ ëª…í™•ì„±: {grade_result.clarity_score:.0%}
  â€¢ ìœ ìš©ì„±: {grade_result.usefulness_score:.0%}""")
            )
            
            # 3. ì¢…í•© í‰ê°€ ë° ìµœì¢… ë‹µë³€
            if overall_score >= self.threshold:
                # í’ˆì§ˆ ê¸°ì¤€ í†µê³¼
                messages.append(
                    AIMessage(content=f"âœ… ë‹µë³€ í’ˆì§ˆ ê²€ì¦ í†µê³¼ (ì¢…í•© ì ìˆ˜: {overall_score:.0%})")
                )
                # ìµœì¢… ë‹µë³€ ì¶”ê°€
                if state.get("final_answer"):
                    messages.append(
                        AIMessage(content=state["final_answer"])
                    )
            else:
                # í’ˆì§ˆ ê¸°ì¤€ ë¯¸ë‹¬
                retry_count = state.get("retry_count", 0)
                max_retries = int(os.getenv("CRAG_MAX_RETRIES", "3"))
                
                if needs_retry and retry_count < max_retries:
                    # ì¬ì‹œë„ ê°€ëŠ¥
                    messages.append(
                        AIMessage(content=f"âš ï¸ ë‹µë³€ í’ˆì§ˆ ë¯¸ë‹¬ (ì¢…í•© ì ìˆ˜: {overall_score:.0%}) - ì¬ì‹œë„ í•„ìš” ({retry_count+1}/{max_retries})")
                    )
                else:
                    # ì¬ì‹œë„ ë¶ˆê°€ëŠ¥ (ìµœëŒ€ íšŸìˆ˜ ì´ˆê³¼) - ë¶€ë¶„ ë‹µë³€ ì œê³µ
                    messages.append(
                        AIMessage(content=f"âš ï¸ ë‹µë³€ í’ˆì§ˆ ê¸°ì¤€ ë¯¸ë‹¬ (ì¢…í•© ì ìˆ˜: {overall_score:.0%}) - ìµœì„ ì˜ ë‹µë³€ì„ ì œê³µí•©ë‹ˆë‹¤")
                    )
                    # ë¶€ë¶„ ë‹µë³€ ì œê³µ
                    if state.get("final_answer"):
                        messages.append(
                            AIMessage(content=f"ğŸ“ ë¶€ë¶„ ë‹µë³€:\n{state['final_answer']}\n\nâš ï¸ ì°¸ê³ : ì´ ë‹µë³€ì€ í’ˆì§ˆ ê¸°ì¤€ì„ ì™„ì „íˆ ì¶©ì¡±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ë‹¤ìŒ ì´ìœ ë¡œ ì œí•œì ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤:\n- ì™„ì „ì„±: {grade_result.completeness_score:.0%} (ë¶€ì¡±í•œ ë¶€ë¶„: {', '.join(grade_result.missing_aspects[:3]) if grade_result.missing_aspects else 'ì—†ìŒ'})\n- ê´€ë ¨ì„±: {grade_result.relevance_score:.0%}\n- ëª…í™•ì„±: {grade_result.clarity_score:.0%}")
                        )
            
            return {
                "messages": messages,  # ë©”ì‹œì§€ ì¶”ê°€
                "answer_grade": quality_check,
                "should_retry": needs_retry,
                "metadata": metadata,
                "warnings": warnings  # Always return list, never None
            }
            
        except Exception as e:
            logger.error(f"[ANSWER_GRADER] Node failed: {str(e)}")
            return {
                "error": f"Answer grading failed: {str(e)}",
                "workflow_status": "failed",
                "warnings": [f"Grading error: {str(e)}"]
            }
    
    def invoke(self, state: MVPWorkflowState) -> Dict[str, Any]:
        """ë™ê¸° ì‹¤í–‰ (LangGraph í˜¸í™˜ì„±)"""
        logger.debug(f"[ANSWER_GRADER] Invoke called (sync wrapper)")
        return self.__call__(state)