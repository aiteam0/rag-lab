"""
Synthesis Node
ê²€ìƒ‰ëœ ë¬¸ì„œë“¤ì„ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ë…¸ë“œ
"""

import os
import logging
from typing import Dict, Any, List, Optional
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.documents import Document
from langchain_core.messages import AIMessage
from pydantic import BaseModel, Field
from dotenv import load_dotenv


from workflow.state import MVPWorkflowState

load_dotenv()

# ë¡œê¹… ì„¤ì •
logger = logging.getLogger(__name__)


class SynthesisResult(BaseModel):
    """ë‹µë³€ ìƒì„± ê²°ê³¼"""
    answer: str = Field(description="Generated answer with inline citations [1], [2], etc. MUST end with References table")
    confidence: float = Field(description="Confidence score (0.0-1.0)")
    sources_used: List[str] = Field(description="List of source references used in format: '[1]', '[2]', etc.")
    key_points: List[str] = Field(description="Key points extracted from documents")
    references_table: str = Field(description="MANDATORY References table in format: | ì°¸ì¡°ë²ˆí˜¸ | ë¬¸ì„œëª… | í˜ì´ì§€ | ë‚´ìš© ìš”ì•½ |")


class SynthesisNode:
    """ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ë…¸ë“œ"""
    
    def __init__(self):
        """ì´ˆê¸°í™”"""
        # ChatOpenAI ì¸ìŠ¤í„´ìŠ¤ ì§ì ‘ ìƒì„±
        self.llm = ChatOpenAI(
            model=os.getenv("OPENAI_MODEL", "gpt-4o-mini"),
            temperature=0.1,  # ë” ì¼ê´€ëœ ë‹µë³€ì„ ìœ„í•´ ë‚®ì€ temperature
            openai_api_key=os.getenv("OPENAI_API_KEY")
        )
        
        # ë‹µë³€ ìƒì„± í”„ë¡¬í”„íŠ¸
        self.synthesis_prompt = ChatPromptTemplate.from_messages([
            ("system", """You are an expert assistant for an automobile manufacturing RAG system.
Your task is to generate comprehensive and accurate answers based on the retrieved documents.

CRITICAL - Information Priority Hierarchy:
1. **HIGHEST PRIORITY - Human Verified Content**: If a document has "Human Verified" information, this is the ground truth and should be used as the primary source
2. **HIGH PRIORITY - Structured Entity Data**: 
   - **PPT Embedded Documents (ë˜‘ë”±ì´)**: Special document type with structured metadata from PPT presentations
   - **Tables**: Structured tabular data with titles, details, and keywords
   - **Figures**: Visual information with descriptions and contextual data
3. **STANDARD PRIORITY - Document Content**: Regular document text is the baseline information source

Guidelines:
1. Base your answer ONLY on the provided documents
2. When human feedback exists, prioritize it over other sources
3. When entity information exists (tables/figures/ë˜‘ë”±ì´), use the structured data to provide precise details
4. **CRITICAL - ë˜‘ë”±ì´ Entity Mention**: 
   - When a document has entity type "ë˜‘ë”±ì´", ALWAYS mention it's a "PPT ì‚½ì… ë¬¸ì„œ" or "PPT Embedded Document"
   - Include the document title from entity metadata when available
   - Mention that this is a specially structured document from PPT presentations
   - Example: "ì´ ì •ë³´ëŠ” 'PPT ì‚½ì… ë¬¸ì„œ(ë˜‘ë”±ì´)'ì¸ [ì œëª©]ì—ì„œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤"
5. If information is not in the documents, say so clearly
6. Cite sources using reference numbers [1], [2], etc. in the main text
7. Structure your answer clearly with proper formatting
8. **CRITICAL: For Korean documents, preserve the EXACT original terms and expressions**
   - Use original Korean terms exactly as written
   - Maintain parenthetical expressions as-is
   - Do NOT paraphrase or reword key terms from the source documents
9. Include specific details like numbers, procedures, and specifications
10. If there are conflicting information, human feedback takes precedence
11. Use the same reference number for the same document throughout the answer
12. Place reference numbers immediately after the relevant statement
13. When quoting or referencing policy terms, use the exact wording from the source

Page Image Display Guidelines:
14. **IMPORTANT**: If retrieved documents have page images (page_image_path in metadata):
    - Collect all unique page images from cited documents
    - Display them at the END of your answer in a dedicated section
    - Group by source document and order by page number
    - Format: ![Source Page X](path) for markdown rendering
14. Page Image Section Format:
    ```
    ## ì°¸ì¡° í˜ì´ì§€ ì´ë¯¸ì§€ (Referenced Page Images)
    
    ### {{source_name}}
    ![Page 1](data/images/filename-page-1.png)
    ![Page 3](data/images/filename-page-3.png)
    
    ### {{another_source}}
    ![Page 2](data/images/another-page-2.png)
    ```

Answer Structure:
- Start with a direct answer to the question
- Provide supporting details from documents with inline citations [1], [2]
- Include relevant warnings or cautions if mentioned
- **NEW**: Add "ì°¸ì¡° í˜ì´ì§€ ì´ë¯¸ì§€ (Referenced Page Images)" section if page images exist
- End with a "References" section listing all cited documents

References Format:
Create a numbered list at the end of your answer with this format:
References:
[1] filename.pdf, Page X - Brief description of content
[2] filename.pdf, Page Y - Brief description of content

Important:
- DO NOT make up information not in the documents
- DO NOT add personal opinions or assumptions
- DO maintain consistent reference numbering throughout
- DO include ALL cited documents in the References section
- DO use the document numbers provided in square brackets [1], [2], etc.

CRITICAL REQUIREMENT - STRUCTURED OUTPUT:
You are generating a structured output with these fields:
1. answer: Main answer with inline citations [1], [2], etc.
2. confidence: How confident you are (0.0-1.0)
3. sources_used: List of references like ['[1]', '[2]', '[3]']
4. key_points: Key points from the documents
5. references_table: MANDATORY table with format below

The references_table field MUST contain a markdown table:
| ì°¸ì¡°ë²ˆí˜¸ | ë¬¸ì„œëª… | í˜ì´ì§€ | ë‚´ìš© ìš”ì•½ |
|---------|--------|--------|-----------|
| [1] | actual_filename.pdf | p.X | What this document contains |
| [2] | actual_filename.pdf | p.Y | What this document contains |

IMPORTANT: The references_table is a SEPARATE field from answer.
DO NOT put the References table in the answer field.
Put it in the references_table field."""),
            ("human", """Query: {query}

Retrieved Documents:
{documents}

Generate a comprehensive response with the following structure:
1. answer: Comprehensive answer with inline citations [1], [2], etc.
2. confidence: Your confidence score (0.0-1.0)
3. sources_used: List like ['[1]', '[2]', '[3]'] for all cited documents
4. key_points: Main points extracted from documents
5. references_table: MANDATORY markdown table with source information

Extract source filename and page from each document's metadata for proper references.""")
        ])
        
        # ë¬¸ì„œ í¬ë§·íŒ… í…œí”Œë¦¿
        self.document_formatter_prompt = """
[{idx}] Document Reference:
- Source: {source}
- Page: {page}
- Category: {category}
- Content: {content}
{caption}
{entity_info}
{human_feedback}
{page_image_note}
---
Note: Use [{idx}] when citing this document in your answer.
"""
    
    
    def _format_entity_info(self, metadata: dict) -> str:
        """
        Entity ì •ë³´ë¥¼ ì ì ˆí•œ í˜•ì‹ìœ¼ë¡œ í¬ë§·íŒ… (íƒ€ì… ì•ˆì „ì„± ë³´ì¥)
        
        Args:
            metadata: ë¬¸ì„œ ë©”íƒ€ë°ì´í„°
            
        Returns:
            í¬ë§·íŒ…ëœ entity ì •ë³´
        """
        entity = metadata.get("entity")
        if not entity:
            return ""
        
        # entityê°€ dictionaryê°€ ì•„ë‹Œ ê²½ìš° ì•ˆì „í•˜ê²Œ ì²˜ë¦¬
        if not isinstance(entity, dict):
            # entityê°€ stringì´ê±°ë‚˜ ë‹¤ë¥¸ íƒ€ì…ì¸ ê²½ìš° ê¸°ë³¸ ì •ë³´ë§Œ í‘œì‹œ
            return f"- Entity Info: {str(entity)}\n"
        
        category = metadata.get("category", "")
        entity_type = entity.get("type", "")
        
        # 'ë˜‘ë”±ì´' íƒ€ì…ì¸ ê²½ìš°: PPT ì‚½ì… ë¬¸ì„œ ì •ë³´ ì œê³µ
        if entity_type == "ë˜‘ë”±ì´":
            entity_text = "- PPT Embedded Document (ë˜‘ë”±ì´):\n"
            title = entity.get("title")
            if title and isinstance(title, str):
                entity_text += f"  Title: {title}\n"
            details = entity.get("details")
            if details and isinstance(details, str):
                entity_text += f"  Details: {details}\n"
            keywords = entity.get("keywords")
            if keywords and isinstance(keywords, list):
                entity_text += f"  Keywords: {', '.join(str(k) for k in keywords)}\n"
            hypothetical_questions = entity.get("hypothetical_questions")
            if hypothetical_questions and isinstance(hypothetical_questions, list):
                # ìµœëŒ€ 3ê°œì˜ ì§ˆë¬¸ë§Œ í‘œì‹œ
                questions_to_show = hypothetical_questions[:3]
                entity_text += f"  Can Answer: {'; '.join(str(q) for q in questions_to_show)}\n"
            return entity_text.rstrip()
        
        # í…Œì´ë¸”ì¸ ê²½ìš°: êµ¬ì¡°í™”ëœ ì •ë³´ ì œê³µ
        elif category == "table" and entity:
            entity_text = "- Table: "
            title = entity.get("title")
            if title and isinstance(title, str):
                entity_text += f"{title}\n"
            details = entity.get("details")
            if details and isinstance(details, str):
                entity_text += f"  Details: {details}\n"
            keywords = entity.get("keywords")
            if keywords and isinstance(keywords, list):
                entity_text += f"  Keywords: {', '.join(str(k) for k in keywords)}\n"
            return entity_text.rstrip()
        
        # ê·¸ë¦¼ì¸ ê²½ìš°: ì„¤ëª… í¬í•¨
        elif category == "figure" and entity:
            entity_text = "- Figure: "
            title = entity.get("title")
            if title and isinstance(title, str):
                entity_text += f"{title}\n"
            details = entity.get("details")
            if details and isinstance(details, str):
                entity_text += f"  Description: {details}\n"
            return entity_text.rstrip()
        
        return ""
    
    def _collect_page_images(self, documents: List[Document]) -> List[Dict[str, Any]]:
        """
        ë¬¸ì„œë“¤ì—ì„œ ìœ ë‹ˆí¬í•œ í˜ì´ì§€ ì´ë¯¸ì§€ ìˆ˜ì§‘
        
        Args:
            documents: ê²€ìƒ‰ëœ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
            
        Returns:
            í˜ì´ì§€ ì´ë¯¸ì§€ ì •ë³´ë¥¼ ë‹´ì€ ë”•ì…”ë„ˆë¦¬ ë¦¬ìŠ¤íŠ¸
        """
        page_images = []
        seen_paths = set()
        
        for doc in documents:
            if not isinstance(doc, Document):
                continue
                
            metadata = doc.metadata or {}
            page_image_path = metadata.get("page_image_path", "")
            
            # ìœ íš¨í•œ ê²½ë¡œì´ê³  ì¤‘ë³µë˜ì§€ ì•Šì€ ê²½ìš°ë§Œ ì¶”ê°€
            if page_image_path and page_image_path not in seen_paths:
                seen_paths.add(page_image_path)
                
                # source íŒŒì¼ëª… ì¶”ì¶œ (í‘œì‹œìš©)
                source = metadata.get("source", "")
                source_name = os.path.basename(source) if source else "Unknown"
                
                page_images.append({
                    "path": page_image_path,
                    "page": metadata.get("page", 0),
                    "source": source_name,
                    "category": metadata.get("category", "")
                })
        
        # í˜ì´ì§€ ë²ˆí˜¸ìˆœìœ¼ë¡œ ì •ë ¬
        page_images.sort(key=lambda x: (x["source"], x["page"]))
        
        return page_images
    
    def _format_documents(self, documents: List[Document], truncate: bool = False) -> str:
        """
        ë¬¸ì„œë“¤ì„ í”„ë¡¬í”„íŠ¸ìš© í…ìŠ¤íŠ¸ë¡œ í¬ë§·íŒ…
        
        Args:
            documents: ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
            truncate: Trueì¼ ê²½ìš° ë¬¸ì„œ ë‚´ìš© ì¶•ì•½ (fallbackìš©)
            
        Returns:
            í¬ë§·íŒ…ëœ ë¬¸ì„œ í…ìŠ¤íŠ¸
        """
        if not documents:
            return "No documents available"
        
        formatted_docs = []
        for idx, doc in enumerate(documents, 1):
            # Document ê°ì²´ íƒ€ì… ê²€ì¦ ë° ë³µì›
            if isinstance(doc, str):
                # LangGraphê°€ Documentë¥¼ stringìœ¼ë¡œ ì§ë ¬í™”í•œ ê²½ìš°
                try:
                    import json
                    doc_dict = json.loads(doc)
                    from langchain_core.documents import Document
                    doc = Document(
                        page_content=doc_dict.get("page_content", ""),
                        metadata=doc_dict.get("metadata", {})
                    )
                except (json.JSONDecodeError, TypeError):
                    logger.warning(f"[SYNTHESIS] Failed to parse document string at index {idx}")
                    continue
            elif isinstance(doc, dict):
                # LangGraph ì§ë ¬í™”ë¡œ dictê°€ ëœ ê²½ìš°
                from langchain_core.documents import Document
                doc = Document(
                    page_content=doc.get("page_content", ""),
                    metadata=doc.get("metadata", {})
                )
            elif not hasattr(doc, 'metadata') or not hasattr(doc, 'page_content'):
                # ì˜ëª»ëœ í˜•ì‹ì˜ ê°ì²´ì¸ ê²½ìš°
                logger.warning(f"[SYNTHESIS] Invalid document format at index {idx}: {type(doc)}")
                continue
            
            metadata = doc.metadata
            
            # ìº¡ì…˜ì´ ìˆìœ¼ë©´ ì¶”ê°€
            caption_text = ""
            if metadata.get("caption"):
                caption_text = f"- Caption: {metadata['caption']}"
            
            # Entity ì •ë³´ í¬ë§·íŒ…
            entity_info_text = self._format_entity_info(metadata)
            
            # Human feedbackì´ ìˆìœ¼ë©´ ì¶”ê°€ (íƒ€ì… ì•ˆì „ì„± ë³´ì¥)
            human_feedback_text = ""
            human_feedback = metadata.get("human_feedback")
            if human_feedback and isinstance(human_feedback, str) and human_feedback.strip():
                human_feedback_text = f"- Human Verified: {human_feedback}"
            
            # í˜ì´ì§€ ì´ë¯¸ì§€ ê²½ë¡œê°€ ìˆìœ¼ë©´ ë…¸íŠ¸ ì¶”ê°€
            page_image_path = metadata.get("page_image_path", "")
            page_image_note = ""
            if page_image_path and isinstance(page_image_path, str) and page_image_path.strip():
                page_image_note = f"- Page Image Available: {page_image_path}"
            
            # ë¬¸ì„œ ë‚´ìš© (truncateê°€ Trueì¼ ë•Œë§Œ ì¶•ì•½)
            content = doc.page_content[:500] if truncate else doc.page_content
            
            formatted_doc = self.document_formatter_prompt.format(
                idx=idx,
                source=metadata.get("source", "Unknown"),
                page=metadata.get("page", "N/A"),
                category=metadata.get("category", "Unknown"),
                content=content,
                caption=caption_text,
                entity_info=entity_info_text,
                human_feedback=human_feedback_text,
                page_image_note=page_image_note
            )
            
            # "ë˜‘ë”±ì´" entity typeì„ ë” ëª…í™•í•˜ê²Œ ê°•ì¡°
            if entity_info_text and "ë˜‘ë”±ì´" in entity_info_text:
                # PPT Embedded Documentë¥¼ ì‹œê°ì ìœ¼ë¡œ ê°•ì¡°
                formatted_doc = formatted_doc.replace(
                    "- PPT Embedded Document (ë˜‘ë”±ì´):",
                    "- **[SPECIAL] PPT Embedded Document (ë˜‘ë”±ì´)**:"
                )
                # ë¬¸ì„œ ì‹œì‘ ë¶€ë¶„ì— íŠ¹ë³„ í‘œì‹œ ì¶”ê°€
                formatted_doc = f"[ğŸ“Œ PPT ì‚½ì… ë¬¸ì„œ]\n{formatted_doc}"
            
            formatted_docs.append(formatted_doc)
        
        return "\n".join(formatted_docs)
    
    def _generate_answer_with_fallback(
        self, 
        query: str, 
        documents: List[Document]
    ) -> SynthesisResult:
        """
        ë‹µë³€ ìƒì„± (max token ì—ëŸ¬ ì‹œ fallback í¬í•¨)
        
        Args:
            query: ì§ˆë¬¸
            documents: ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
            
        Returns:
            ìƒì„±ëœ ë‹µë³€ ê²°ê³¼
        """
        # í˜ì´ì§€ ì´ë¯¸ì§€ ìˆ˜ì§‘
        page_images = self._collect_page_images(documents)
        
        # ë¨¼ì € ì „ì²´ ë¬¸ì„œë¡œ ì‹œë„
        try:
            formatted_docs = self._format_documents(documents, truncate=False)
            
            # í˜ì´ì§€ ì´ë¯¸ì§€ ì„¹ì…˜ ì¶”ê°€
            if page_images:
                image_section = "\n\n## Page Images Available for Reference:\n"
                current_source = None
                
                for img in page_images:
                    # ì†ŒìŠ¤ë³„ë¡œ ê·¸ë£¹í™”
                    if img['source'] != current_source:
                        current_source = img['source']
                        image_section += f"\n### From {current_source}:\n"
                    
                    image_section += f"- Page {img['page']}: {img['path']}\n"
                
                formatted_docs += image_section
            
            structured_llm = self.llm.with_structured_output(
                SynthesisResult
            )
            
            result = structured_llm.invoke(
                self.synthesis_prompt.format_messages(
                    query=query,
                    documents=formatted_docs
                )
            )
            
            # ìƒì„±ëœ ë‹µë³€ ë¡œê¹…
            logger.info(f"[SYNTHESIS] === Generated Answer Summary ===")
            logger.info(f"[SYNTHESIS] Query: {query}")
            logger.info(f"[SYNTHESIS] Answer Length: {len(result.answer)} chars")
            logger.info(f"[SYNTHESIS] Confidence: {result.confidence:.2f}")
            logger.info(f"[SYNTHESIS] Sources Used: {result.sources_used}")
            logger.info(f"[SYNTHESIS] Key Points Count: {len(result.key_points)}")
            if result.key_points:
                logger.info(f"[SYNTHESIS] First Key Point: {result.key_points[0]}")
            logger.info(f"[SYNTHESIS] Full Answer:")
            logger.info(f"[SYNTHESIS] {result.answer}")
            logger.info(f"[SYNTHESIS] === End of Answer ===")
            
            return result
            
        except Exception as e:
            # Token ì œí•œ ì—ëŸ¬ì¸ ê²½ìš° fallback
            if "maximum context length" in str(e).lower() or "token" in str(e).lower():
                print(f"Token limit exceeded, falling back to truncated documents")
                
                # ë¬¸ì„œ ì¶•ì•½í•˜ì—¬ ì¬ì‹œë„
                formatted_docs = self._format_documents(documents, truncate=True)
                
                # í˜ì´ì§€ ì´ë¯¸ì§€ ì„¹ì…˜ ì¶”ê°€ (truncateì—ì„œë„ ë™ì¼í•˜ê²Œ)
                if page_images:
                    image_section = "\n\n## Page Images Available for Reference:\n"
                    current_source = None
                    
                    for img in page_images:
                        # ì†ŒìŠ¤ë³„ë¡œ ê·¸ë£¹í™”
                        if img['source'] != current_source:
                            current_source = img['source']
                            image_section += f"\n### From {current_source}:\n"
                        
                        image_section += f"- Page {img['page']}: {img['path']}\n"
                    
                    formatted_docs += image_section
                
                structured_llm = self.llm.with_structured_output(
                    SynthesisResult
                )
                
                result = structured_llm.invoke(
                    self.synthesis_prompt.format_messages(
                        query=query,
                        documents=formatted_docs
                    )
                )
                
                # Fallback ì‹œì—ë„ ë‹µë³€ ë¡œê¹…
                logger.warning(f"[SYNTHESIS] Generated answer using truncated documents (fallback)")
                logger.info(f"[SYNTHESIS] Answer Length: {len(result.answer)} chars")
                logger.info(f"[SYNTHESIS] Confidence: {result.confidence:.2f}")
                
                return result
            else:
                # ë‹¤ë¥¸ ì—ëŸ¬ëŠ” ê·¸ëŒ€ë¡œ ì „íŒŒ
                raise e
    
    def __call__(self, state: MVPWorkflowState) -> Dict[str, Any]:
        """
        ë…¸ë“œ ì‹¤í–‰
        
        Args:
            state: ì›Œí¬í”Œë¡œìš° ìƒíƒœ
            
        Returns:
            ì—…ë°ì´íŠ¸ëœ ìƒíƒœ í•„ë“œ
        """
        logger.info(f"[SYNTHESIS] Node started")
        
        try:
            # í˜„ì¬ ì²˜ë¦¬ ì¤‘ì¸ ì¿¼ë¦¬ì™€ ë¬¸ì„œ ê²°ì •
            subtasks = state.get("subtasks", [])
            current_idx = state.get("current_subtask_idx", 0)
            logger.debug(f"[SYNTHESIS] Subtasks: {len(subtasks)}, current_idx: {current_idx}")
            
            if subtasks and current_idx < len(subtasks):
                # ì„œë¸ŒíƒœìŠ¤í¬ ì²˜ë¦¬
                current_subtask = subtasks[current_idx]
                query = current_subtask["query"]
                # Retrieval Nodeì—ì„œ ê²€ìƒ‰ëœ ë¬¸ì„œ ê·¸ëŒ€ë¡œ ì‚¬ìš©
                documents = current_subtask.get("documents", [])
                subtask_id = current_subtask.get("id", "no-id")[:8]
                logger.info(f"[SYNTHESIS] Processing subtask [{subtask_id}]: '{query}'")
                logger.debug(f"[SYNTHESIS] Subtask has {len(documents)} documents")
            else:
                # ì „ì²´ ì¿¼ë¦¬ ì²˜ë¦¬
                query = state["query"]
                # ì „ì²´ ê²€ìƒ‰ëœ ë¬¸ì„œ ê·¸ëŒ€ë¡œ ì‚¬ìš©
                documents = state.get("documents", [])
                logger.info(f"[SYNTHESIS] Processing full query: '{query}'")
                logger.debug(f"[SYNTHESIS] State has {len(documents)} documents")
            
            # documentsê°€ Noneì´ë©´ ì¦‰ì‹œ ì‹¤íŒ¨
            if documents is None:
                raise ValueError(
                    "CRITICAL ERROR: documents is None in synthesis. "
                    "Retrieval node must provide documents list (can be empty but not None)."
                )
            
            # CRITICAL: documentsê°€ ë¹ˆ ë¦¬ìŠ¤íŠ¸ë©´ ì¦‰ì‹œ ì‹¤íŒ¨
            if not documents:
                logger.error(f"[SYNTHESIS] CRITICAL: Empty documents list")
                raise ValueError(
                    "CRITICAL ERROR: Empty documents list in synthesis. "
                    "Retrieval must provide at least one document. "
                    "Cannot generate answer without source documents."
                )
            
            # ì¬ì‹œë„ ìƒí™© ê°ì§€ ë° í”¼ë“œë°± í™œìš©
            retry_count = state.get("retry_count", 0)
            hallucination_feedback = state.get("hallucination_check")
            quality_feedback = state.get("answer_grade")
            
            # ì¬ì‹œë„ ì—¬ë¶€ í™•ì¸ (í”¼ë“œë°±ì—ì„œ needs_retry í™•ì¸)
            is_retry_from_hallucination = hallucination_feedback and hallucination_feedback.get("needs_retry", False)
            is_retry_from_quality = quality_feedback and quality_feedback.get("needs_retry", False)
            is_retry = is_retry_from_hallucination or is_retry_from_quality
            
            # ì¬ì‹œë„ì¸ ê²½ìš° retry_count ì¦ê°€
            if is_retry:
                retry_count = retry_count + 1
                logger.info(f"[SYNTHESIS] Retry attempt {retry_count} - analyzing feedback")
                
                # í™˜ê° ì²´í¬ ì‹¤íŒ¨ë¡œ ì¸í•œ ì¬ì‹œë„
                if is_retry_from_hallucination:
                    logger.info(f"[SYNTHESIS] Using corrective generation due to hallucination concerns")
                    logger.debug(f"[SYNTHESIS] Hallucination score: {hallucination_feedback.get('score', 0)}")
                    synthesis_result = self._generate_corrective_answer(
                        query, documents, hallucination_feedback, state.get("metadata", {})
                    )
                # í’ˆì§ˆ ì²´í¬ ì‹¤íŒ¨ë¡œ ì¸í•œ ì¬ì‹œë„
                elif is_retry_from_quality:
                    logger.info(f"[SYNTHESIS] Using improved generation due to quality concerns")
                    logger.debug(f"[SYNTHESIS] Quality score: {quality_feedback.get('score', 0)}")
                    synthesis_result = self._generate_improved_answer(
                        query, documents, quality_feedback, state.get("metadata", {})
                    )
            else:
                # ì²« ë²ˆì§¸ ì‹œë„
                logger.info(f"[SYNTHESIS] Generating answer using {len(documents)} documents...")
                synthesis_result = self._generate_answer_with_fallback(query, documents)
            logger.info(f"[SYNTHESIS] Answer generated with confidence: {synthesis_result.confidence:.3f}")
            
            # ì‚¬ìš©ëœ ì†ŒìŠ¤ì™€ í‚¤í¬ì¸íŠ¸ ìƒì„¸ ì •ë³´ ë¡œê¹…
            sources_count = len(synthesis_result.sources_used)
            sources_preview = ', '.join(synthesis_result.sources_used[:3]) + ('...' if sources_count > 3 else '')
            logger.info(f"[SYNTHESIS] Used {sources_count} sources: [{sources_preview}]")
            
            if synthesis_result.key_points:
                points_count = len(synthesis_result.key_points)
                points_preview = ' | '.join([kp[:35] + '...' if len(kp) > 35 else kp for kp in synthesis_result.key_points[:2]])
                logger.info(f"[SYNTHESIS] {points_count} key points: {points_preview}")
            
            # ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸
            metadata = state.get("metadata", {})
            metadata["synthesis"] = {
                "documents_used": len(documents),
                "sources": synthesis_result.sources_used,
                "key_points": synthesis_result.key_points,
                "confidence": synthesis_result.confidence
            }
            logger.debug(f"[SYNTHESIS] Updated metadata with synthesis results")
            
            # References í…Œì´ë¸”ì„ ë‹µë³€ì— ì¶”ê°€
            final_answer = synthesis_result.answer
            if hasattr(synthesis_result, 'references_table') and synthesis_result.references_table:
                # References í…Œì´ë¸”ì´ ë³„ë„ í•„ë“œë¡œ ì œê³µëœ ê²½ìš° ë‹µë³€ ëì— ì¶”ê°€
                if "References:" not in final_answer:
                    final_answer = f"{final_answer}\n\n## References:\n{synthesis_result.references_table}"
            
            # ë©”ì‹œì§€ ìƒì„± - í†µí•© ë° ê°„ì†Œí™”
            messages = []
            
            # ì¬ì‹œë„ ì •ë³´ê°€ ìˆëŠ” ê²½ìš°ì—ë§Œ í‘œì‹œ
            max_retries = int(os.getenv("CRAG_MAX_RETRIES", "3"))
            if retry_count > 0:
                retry_reason = ""
                if is_retry_from_hallucination:
                    retry_reason = "í™˜ê° ê²€ì¦ ì‹¤íŒ¨"
                elif is_retry_from_quality:
                    retry_reason = "í’ˆì§ˆ ê¸°ì¤€ ë¯¸ë‹¬"
                messages.append(
                    AIMessage(content=f"ğŸ”„ ë‹µë³€ ì¬ìƒì„± ì¤‘... (ì‹œë„ {retry_count+1}/{max_retries}, ì‚¬ìœ : {retry_reason})")
                )
            
            # í†µí•©ëœ ë‹µë³€ ìƒì„± ë©”ì‹œì§€
            messages.append(
                AIMessage(content=f"âœï¸ {len(documents)}ê°œ ë¬¸ì„œì—ì„œ ë‹µë³€ ìƒì„± ì¤‘... (ì‹ ë¢°ë„: {synthesis_result.confidence:.0%})")
            )
            
            # ì„œë¸ŒíƒœìŠ¤í¬ ì—…ë°ì´íŠ¸
            if subtasks and current_idx < len(subtasks):
                subtasks[current_idx]["answer"] = final_answer
                subtasks[current_idx]["status"] = "synthesized"
                subtask_id = subtasks[current_idx].get("id", "no-id")[:8]
                logger.info(f"[SYNTHESIS] Updated subtask [{subtask_id}] status: 'retrieved' -> 'synthesized'")
                
                result = {
                    "messages": messages,  # ë©”ì‹œì§€ ì¶”ê°€
                    "subtasks": subtasks,
                    "intermediate_answer": final_answer,
                    "confidence_score": synthesis_result.confidence,
                    "metadata": metadata,
                    "retry_count": retry_count  # ì¬ì‹œë„ íšŸìˆ˜ í¬í•¨
                }
                logger.info(f"[SYNTHESIS] Node completed - intermediate answer generated")
                return result
            else:
                # ìµœì¢… ë‹µë³€
                result = {
                    "messages": messages,  # ë©”ì‹œì§€ ì¶”ê°€
                    "final_answer": final_answer,
                    "confidence_score": synthesis_result.confidence,
                    "metadata": metadata,
                    "retry_count": retry_count  # ì¬ì‹œë„ íšŸìˆ˜ í¬í•¨
                }
                logger.info(f"[SYNTHESIS] Node completed - final answer generated")
                return result
            
        except Exception as e:
            logger.error(f"[SYNTHESIS] Node failed: {str(e)}")
            return {
                "error": f"Synthesis failed: {str(e)}",
                "workflow_status": "failed",
                "warnings": [f"Synthesis error: {str(e)}"]
            }
    
    def invoke(self, state: MVPWorkflowState) -> Dict[str, Any]:
        """ë™ê¸° ì‹¤í–‰ (LangGraph í˜¸í™˜ì„±)"""
        logger.debug(f"[SYNTHESIS] Invoke called (sync wrapper)")
        return self.__call__(state)
    
    def _generate_corrective_answer(self, query: str, documents: List[Document], 
                                         hallucination_feedback: Dict[str, Any], 
                                         metadata: Dict[str, Any]) -> SynthesisResult:
        """
        í™˜ê° ì²´í¬ ì‹¤íŒ¨ ì‹œ ë³´ìˆ˜ì ì¸ ë‹µë³€ ìƒì„±
        
        Args:
            query: ì‚¬ìš©ì ì¿¼ë¦¬
            documents: ê²€ìƒ‰ëœ ë¬¸ì„œë“¤
            hallucination_feedback: í™˜ê° ì²´í¬ í”¼ë“œë°±
            metadata: ì¶”ê°€ ë©”íƒ€ë°ì´í„°
            
        Returns:
            ë³´ìˆ˜ì ìœ¼ë¡œ ìƒì„±ëœ ë‹µë³€ ê²°ê³¼
        """
        logger.info(f"[SYNTHESIS] Generating corrective answer to avoid hallucination")
        
        # ë¬¸ì œê°€ ëœ ì£¼ì¥ë“¤ê³¼ ì œì•ˆì‚¬í•­ ì¶”ì¶œ
        hallucination_meta = metadata.get("hallucination_check", {})
        problematic_claims = hallucination_meta.get("problematic_claims", [])
        supported_claims = hallucination_meta.get("supported_claims", [])
        suggestions = hallucination_feedback.get("suggestions", [])
        
        logger.debug(f"[SYNTHESIS] Problematic claims to avoid: {problematic_claims}")
        logger.debug(f"[SYNTHESIS] Improvement suggestions: {suggestions}")
        
        # ë³´ìˆ˜ì ì¸ í”„ë¡¬í”„íŠ¸ ìƒì„±
        # ë¬¸ì œê°€ ëœ ì£¼ì¥ë“¤ í¬ë§·íŒ…
        problematic_claims_text = "\n".join(f'  âœ— {claim}' for claim in problematic_claims) if problematic_claims else '  None identified'
        supported_claims_text = "\n".join(f'  âœ“ {claim}' for claim in supported_claims) if supported_claims else '  None identified'
        suggestions_text = "\n".join(f'  â†’ {suggestion}' for suggestion in suggestions) if suggestions else '  None provided'
        
        corrective_prompt = ChatPromptTemplate.from_messages([
            ("system", """CRITICAL: This is a RETRY due to hallucination concerns in the previous attempt.

PREVIOUS ISSUES:
- Hallucination score: {hallucination_score:.2f}
- Problematic claims that MUST BE AVOIDED:
{problematic_claims}

- Supported claims that CAN BE KEPT:
{supported_claims}

- Improvement suggestions:
{suggestions}

STRICT CORRECTIVE GUIDELINES:
1. BE EXTREMELY CONSERVATIVE - only state what is EXPLICITLY written in documents
2. DO NOT make any of the problematic claims listed above
3. Include reference numbers [1], [2], etc. for EVERY factual statement
4. IMPORTANT: When information is not available in documents, explicitly state:
   - "ë¬¸ì„œì— í•´ë‹¹ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤" (Korean)
   - "This information is not available in the provided documents" (English)
5. Prioritize accuracy over completeness - it's better to provide less information that is certain
6. Use direct quotes when possible, with clear attribution using reference numbers
7. Clearly distinguish between:
   - What IS explicitly stated in documents (use: "ë¬¸ì„œì— ë”°ë¥´ë©´" or "According to the documents")
   - What is NOT in documents (use: "ë¬¸ì„œì— ëª…ì‹œë˜ì§€ ì•ŠìŒ" or "Not specified in documents")
   - What requires additional information (use: "ì¶”ê°€ ì •ë³´ê°€ í•„ìš”í•©ë‹ˆë‹¤" or "Additional information needed")
8. Include a complete References section at the end with all cited documents

ORIGINAL GUIDELINES (with emphasis on accuracy):
{template}""".format(
                hallucination_score=hallucination_feedback.get('score', 0),
                problematic_claims=problematic_claims_text,
                supported_claims=supported_claims_text,
                suggestions=suggestions_text,
                template=self.synthesis_prompt.messages[0].prompt.template
            )),
            ("human", """Query: {query}

Retrieved Documents:
{documents}

Generate a CORRECTED answer that avoids all hallucination issues.
Be conservative and cite sources explicitly.
Clearly state "ë¬¸ì„œì— ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤" or "Information not available in documents" when relevant details are missing.""")
        ])
        
        # ë” ë‚®ì€ temperature ì‚¬ìš© (ë³´ìˆ˜ì  ìƒì„±)
        conservative_llm = ChatOpenAI(
            model=os.getenv("OPENAI_MODEL", "gpt-4o-mini"),
            temperature=0.1,  # ë§¤ìš° ë‚®ì€ temperature
            openai_api_key=os.getenv("OPENAI_API_KEY")
        )
        
        # êµ¬ì¡°í™”ëœ ì¶œë ¥ìœ¼ë¡œ ë‹µë³€ ìƒì„±
        structured_llm = conservative_llm.with_structured_output(
            SynthesisResult
        )
        
        # í˜ì´ì§€ ì´ë¯¸ì§€ ìˆ˜ì§‘
        page_images = self._collect_page_images(documents)
        
        # ë¬¸ì„œ í¬ë§·íŒ…
        formatted_docs = self._format_documents(documents)
        
        # í˜ì´ì§€ ì´ë¯¸ì§€ ì„¹ì…˜ ì¶”ê°€
        if page_images:
            image_section = "\n\n## Page Images Available for Reference:\n"
            current_source = None
            
            for img in page_images:
                # ì†ŒìŠ¤ë³„ë¡œ ê·¸ë£¹í™”
                if img['source'] != current_source:
                    current_source = img['source']
                    image_section += f"\n### From {current_source}:\n"
                
                image_section += f"- Page {img['page']}: {img['path']}\n"
            
            formatted_docs += image_section
        
        try:
            result = structured_llm.invoke(
                corrective_prompt.format_messages(
                    query=query,
                    documents=formatted_docs
                )
            )
            logger.info(f"[SYNTHESIS] Corrective answer generated successfully")
            return result
            
        except Exception as e:
            logger.error(f"[SYNTHESIS] Corrective generation failed: {str(e)}")
            # Fallback to original method
            return self._generate_answer_with_fallback(query, documents)
    
    def _generate_improved_answer(self, query: str, documents: List[Document],
                                       quality_feedback: Dict[str, Any],
                                       metadata: Dict[str, Any]) -> SynthesisResult:
        """
        í’ˆì§ˆ ì²´í¬ ì‹¤íŒ¨ ì‹œ í–¥ìƒëœ ë‹µë³€ ìƒì„±
        
        Args:
            query: ì‚¬ìš©ì ì¿¼ë¦¬
            documents: ê²€ìƒ‰ëœ ë¬¸ì„œë“¤
            quality_feedback: í’ˆì§ˆ í‰ê°€ í”¼ë“œë°±
            metadata: ì¶”ê°€ ë©”íƒ€ë°ì´í„°
            
        Returns:
            í’ˆì§ˆì´ í–¥ìƒëœ ë‹µë³€ ê²°ê³¼
        """
        logger.info(f"[SYNTHESIS] Generating improved answer based on quality feedback")
        
        # í’ˆì§ˆ í‰ê°€ ìƒì„¸ ì •ë³´ ì¶”ì¶œ
        grade_meta = metadata.get("answer_grade", {})
        missing_aspects = grade_meta.get("missing_aspects", [])
        suggestions = quality_feedback.get("suggestions", [])
        strengths = grade_meta.get("strengths", [])
        
        # ì ìˆ˜ë³„ ê°œì„  ì˜ì—­ íŒŒì•…
        completeness = grade_meta.get("completeness", 0)
        relevance = grade_meta.get("relevance", 0)
        clarity = grade_meta.get("clarity", 0)
        usefulness = grade_meta.get("usefulness", 0)
        
        logger.debug(f"[SYNTHESIS] Quality scores - C:{completeness:.2f} R:{relevance:.2f} Cl:{clarity:.2f} U:{usefulness:.2f}")
        logger.debug(f"[SYNTHESIS] Missing aspects: {missing_aspects}")
        logger.debug(f"[SYNTHESIS] Improvement suggestions: {suggestions}")
        
        # í’ˆì§ˆ ê°œì„  í”„ë¡¬í”„íŠ¸ ìƒì„±
        missing_aspects_text = "\n".join(f'  âœ“ {aspect}' for aspect in missing_aspects) if missing_aspects else '  None identified'
        suggestions_text = "\n".join(f'  â†’ {suggestion}' for suggestion in suggestions) if suggestions else '  None provided'
        strengths_text = "\n".join(f'  â€¢ {strength}' for strength in strengths) if strengths else '  None identified'
        
        improvement_prompt = ChatPromptTemplate.from_messages([
            ("system", """IMPORTANT: This is a RETRY to improve answer quality based on evaluation feedback.

PREVIOUS QUALITY ASSESSMENT:
- Overall score: {score:.2f}
- Completeness: {completeness:.2f} (35% weight)
- Relevance: {relevance:.2f} (30% weight)  
- Clarity: {clarity:.2f} (20% weight)
- Usefulness: {usefulness:.2f} (15% weight)

MISSING ASPECTS TO ADDRESS:
{missing_aspects_text}

IMPROVEMENT SUGGESTIONS TO IMPLEMENT:
{suggestions_text}

STRENGTHS TO MAINTAIN:
{strengths_text}

QUALITY IMPROVEMENT GUIDELINES:
1. COMPLETENESS: Address ALL missing aspects listed above
2. STRUCTURE: Organize answer with clear sections and formatting
   - Use headings for major topics
   - Use bullet points for lists
   - Use numbered steps for procedures
3. SPECIFICITY: Include concrete details
   - Exact specifications and numbers
   - Step-by-step procedures
   - Required tools and materials
   - Time estimates
4. USEFULNESS: Make the answer actionable
   - Clear instructions
   - Practical guidance
   - Safety warnings and cautions
5. CLARITY: Use clear, concise language
   - Define technical terms
   - Avoid ambiguity
   - Logical flow from general to specific

For vehicle manual queries, ensure you include:
- Safety warnings and precautions
- Required tools and materials
- Step-by-step procedures
- Time estimates
- Maintenance schedules if relevant
- Part numbers or specifications
- Troubleshooting tips if applicable
- Proper citations using reference numbers [1], [2], etc.
- Complete References section at the end

ORIGINAL GUIDELINES (with emphasis on completeness):
{template}""".format(
                score=quality_feedback.get('score', 0),
                completeness=completeness,
                relevance=relevance,
                clarity=clarity,
                usefulness=usefulness,
                missing_aspects_text=missing_aspects_text,
                suggestions_text=suggestions_text,
                strengths_text=strengths_text,
                template=self.synthesis_prompt.messages[0].prompt.template
            )),
            ("human", """Query: {query}

Retrieved Documents:
{documents}

Generate an IMPROVED answer that addresses all feedback.
Focus on completeness, structure, and usefulness.""")
        ])
        
        # êµ¬ì¡°í™”ëœ ì¶œë ¥ìœ¼ë¡œ ë‹µë³€ ìƒì„±
        structured_llm = self.llm.with_structured_output(
            SynthesisResult
        )
        
        # ë¬¸ì„œ í¬ë§·íŒ…
        formatted_docs = self._format_documents(documents)
        
        try:
            result = structured_llm.invoke(
                improvement_prompt.format_messages(
                    query=query,
                    documents=formatted_docs
                )
            )
            logger.info(f"[SYNTHESIS] Improved answer generated successfully")
            return result
            
        except Exception as e:
            logger.error(f"[SYNTHESIS] Improved generation failed: {str(e)}")
            # Fallback to original method
            return self._generate_answer_with_fallback(query, documents)