#!/usr/bin/env python3
"""
Answer Grader Node Test
ë‹µë³€ í’ˆì§ˆ í‰ê°€ ë…¸ë“œê°€ ë‹µë³€ì˜ í’ˆì§ˆì„ ì˜¬ë°”ë¥´ê²Œ í‰ê°€í•˜ëŠ”ì§€ í…ŒìŠ¤íŠ¸
"""

import sys
import asyncio
from pathlib import Path
from langchain_core.documents import Document

# í”„ë¡œì íŠ¸ ê²½ë¡œ ì¶”ê°€
sys.path.append(str(Path(__file__).parent.parent.parent))

from workflow.nodes.answer_grader import AnswerGraderNode
from workflow.state import MVPWorkflowState


async def test_answer_grader():
    """Answer Grader ë…¸ë“œ í…ŒìŠ¤íŠ¸"""
    print("=" * 60)
    print("Answer Grader Node Test")
    print("=" * 60)
    
    # ë…¸ë“œ ìƒì„±
    node = AnswerGraderNode()
    print("âœ… Node created successfully\n")
    
    # í…ŒìŠ¤íŠ¸ìš© ë¬¸ì„œ
    test_documents = [
        Document(
            page_content="ì—”ì§„ ì˜¤ì¼ êµì²´ ì ˆì°¨: 1. ì°¨ëŸ‰ì„ í‰í‰í•œ ê³³ì— ì£¼ì°¨ 2. ì—”ì§„ì„ ë„ê³  ì‹íž˜ 3. ë“œë ˆì¸ í”ŒëŸ¬ê·¸ ì œê±° 4. ì˜¤ì¼ ë°°ì¶œ 5. ìƒˆ í•„í„° ìž¥ì°© 6. ìƒˆ ì˜¤ì¼ ì£¼ìž…",
            metadata={"source": "manual.pdf", "page": 45, "category": "list"}
        ),
        Document(
            page_content="ê¶Œìž¥ ì˜¤ì¼: 5W-30, ìš©ëŸ‰: 4.5ë¦¬í„°, êµì²´ ì£¼ê¸°: 10,000km",
            metadata={"source": "manual.pdf", "page": 46, "category": "paragraph"}
        )
    ]
    
    # í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ 1: ê³ í’ˆì§ˆ ë‹µë³€ (í†µê³¼í•´ì•¼ í•¨)
    print("\n" + "="*40)
    print("Test Case 1: High Quality Answer (Should Pass)")
    print("="*40)
    
    high_quality_answer = """
    ì—”ì§„ ì˜¤ì¼ êµì²´ ë°©ë²•:
    
    1. ì¤€ë¹„ ë‹¨ê³„:
       - ì°¨ëŸ‰ì„ í‰í‰í•œ ê³³ì— ì£¼ì°¨í•©ë‹ˆë‹¤
       - ì—”ì§„ì„ ë„ê³  ì¶©ë¶„ížˆ ì‹íž™ë‹ˆë‹¤ (ìµœì†Œ 10ë¶„)
    
    2. ì˜¤ì¼ ë°°ì¶œ:
       - ë“œë ˆì¸ í”ŒëŸ¬ê·¸ë¥¼ ì œê±°í•©ë‹ˆë‹¤
       - ê¸°ì¡´ ì˜¤ì¼ì„ ì™„ì „ížˆ ë°°ì¶œì‹œí‚µë‹ˆë‹¤
    
    3. í•„í„° êµì²´:
       - ê¸°ì¡´ ì˜¤ì¼ í•„í„°ë¥¼ ì œê±°í•©ë‹ˆë‹¤
       - ìƒˆ í•„í„°ë¥¼ ìž¥ì°©í•©ë‹ˆë‹¤
    
    4. ìƒˆ ì˜¤ì¼ ì£¼ìž…:
       - ê¶Œìž¥ ì˜¤ì¼(5W-30)ì„ ì‚¬ìš©í•©ë‹ˆë‹¤
       - ê·œì • ìš©ëŸ‰(4.5ë¦¬í„°)ë§Œí¼ ì£¼ìž…í•©ë‹ˆë‹¤
    
    5. êµì²´ ì£¼ê¸°:
       - 10,000kmë§ˆë‹¤ ë˜ëŠ” 6ê°œì›”ë§ˆë‹¤ êµì²´í•˜ì„¸ìš”
    
    ì£¼ì˜ì‚¬í•­: ë°˜ë“œì‹œ ì—”ì§„ì´ ì‹ì€ í›„ ìž‘ì—…í•˜ì‹œê³ , íì˜¤ì¼ì€ ì ì ˆížˆ ì²˜ë¦¬í•˜ì„¸ìš”.
    """
    
    state_1 = {
        "query": "ì—”ì§„ ì˜¤ì¼ êµì²´ ë°©ë²•ì„ ìžì„¸ížˆ ì„¤ëª…í•´ì£¼ì„¸ìš”",
        "final_answer": high_quality_answer,
        "documents": test_documents,
        "confidence_score": 0.9,
        "metadata": {},
        "workflow_status": "running"
    }
    
    try:
        print("ðŸ“Š Grading high quality answer...")
        result_1 = await node(state_1)
        
        # ê²°ê³¼ ê²€ì¦
        assert "answer_grade" in result_1, "answer_grade field missing"
        
        grade_result = result_1["answer_grade"]
        print(f"\nâœ… Grade Result:")
        print(f"  - Is valid: {grade_result.get('is_valid', False)}")
        print(f"  - Score: {grade_result.get('score', 0.0):.3f}")
        print(f"  - Needs retry: {grade_result.get('needs_retry', False)}")
        
        if grade_result.get('is_valid'):
            print("  âœ… Answer quality approved")
        else:
            print("  âš ï¸  Answer quality insufficient")
        
        # ë©”íƒ€ë°ì´í„° í™•ì¸
        if "metadata" in result_1 and "answer_grade" in result_1["metadata"]:
            grade_meta = result_1["metadata"]["answer_grade"]
            print(f"\nðŸ“ˆ Quality Scores:")
            print(f"  - Overall: {grade_meta.get('overall_score', 0.0):.3f}")
            print(f"  - Completeness: {grade_meta.get('completeness', 0.0):.3f}")
            print(f"  - Relevance: {grade_meta.get('relevance', 0.0):.3f}")
            print(f"  - Clarity: {grade_meta.get('clarity', 0.0):.3f}")
            print(f"  - Usefulness: {grade_meta.get('usefulness', 0.0):.3f}")
            
            if grade_meta.get('strengths'):
                print(f"\n  ðŸ’ª Strengths:")
                for strength in grade_meta['strengths'][:3]:
                    print(f"    â€¢ {strength}")
        
    except Exception as e:
        print(f"âŒ Test Case 1 failed: {e}")
        import traceback
        traceback.print_exc()
    
    # í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ 2: ì €í’ˆì§ˆ ë‹µë³€ (ì‹¤íŒ¨í•´ì•¼ í•¨)
    print("\n" + "="*40)
    print("Test Case 2: Low Quality Answer (Should Fail)")
    print("="*40)
    
    low_quality_answer = """
    ì˜¤ì¼ì„ êµì²´í•˜ì„¸ìš”.
    """
    
    state_2 = {
        "query": "ì—”ì§„ ì˜¤ì¼ êµì²´ ë°©ë²•ì„ ìžì„¸ížˆ ì„¤ëª…í•´ì£¼ì„¸ìš”",
        "final_answer": low_quality_answer,
        "documents": test_documents,
        "confidence_score": 0.3,
        "metadata": {},
        "workflow_status": "running"
    }
    
    try:
        print("ðŸ“Š Grading low quality answer...")
        result_2 = await node(state_2)
        
        grade_result = result_2["answer_grade"]
        print(f"\nâœ… Grade Result:")
        print(f"  - Is valid: {grade_result.get('is_valid', False)}")
        print(f"  - Score: {grade_result.get('score', 0.0):.3f}")
        print(f"  - Needs retry: {grade_result.get('needs_retry', False)}")
        
        if not grade_result.get('is_valid'):
            print("  âœ… Low quality correctly detected")
            print(f"  - Reason: {grade_result.get('reason', 'N/A')[:200]}...")
            
            if grade_result.get('suggestions'):
                print(f"\n  ðŸ“ Improvement suggestions:")
                for suggestion in grade_result['suggestions'][:3]:
                    print(f"    â€¢ {suggestion}")
        
        # ë©”íƒ€ë°ì´í„°ì—ì„œ missing aspects í™•ì¸
        if "metadata" in result_2 and "answer_grade" in result_2["metadata"]:
            grade_meta = result_2["metadata"]["answer_grade"]
            if grade_meta.get('missing_aspects'):
                print(f"\n  âŒ Missing aspects:")
                for aspect in grade_meta['missing_aspects'][:3]:
                    print(f"    â€¢ {aspect}")
        
    except Exception as e:
        print(f"âŒ Test Case 2 failed: {e}")
        import traceback
        traceback.print_exc()
    
    # í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ 3: ì¤‘ê°„ í’ˆì§ˆ ë‹µë³€ (ê²½ê³„ ì¼€ì´ìŠ¤)
    print("\n" + "="*40)
    print("Test Case 3: Medium Quality Answer (Edge Case)")
    print("="*40)
    
    medium_quality_answer = """
    ì—”ì§„ ì˜¤ì¼ì„ êµì²´í•˜ë ¤ë©´:
    1. ë“œë ˆì¸ í”ŒëŸ¬ê·¸ë¥¼ ì—´ì–´ ì˜¤ì¼ì„ ë°°ì¶œí•©ë‹ˆë‹¤
    2. ìƒˆ ì˜¤ì¼ì„ 4.5ë¦¬í„° ë„£ìŠµë‹ˆë‹¤
    3. 10,000kmë§ˆë‹¤ êµì²´í•˜ì„¸ìš”
    
    ê¶Œìž¥ ì˜¤ì¼ì€ 5W-30ìž…ë‹ˆë‹¤.
    """
    
    state_3 = {
        "query": "ì—”ì§„ ì˜¤ì¼ êµì²´ ë°©ë²•ì„ ìžì„¸ížˆ ì„¤ëª…í•´ì£¼ì„¸ìš”",
        "final_answer": medium_quality_answer,
        "documents": test_documents,
        "confidence_score": 0.6,
        "metadata": {},
        "workflow_status": "running"
    }
    
    try:
        print("ðŸ“Š Grading medium quality answer...")
        result_3 = await node(state_3)
        
        grade_result = result_3["answer_grade"]
        print(f"\nâœ… Grade Result:")
        print(f"  - Is valid: {grade_result.get('is_valid', False)}")
        print(f"  - Score: {grade_result.get('score', 0.0):.3f}")
        print(f"  - Analysis: Acceptable but could be improved")
        
        # Threshold í™•ì¸
        if "metadata" in result_3 and "answer_grade" in result_3["metadata"]:
            threshold = result_3["metadata"]["answer_grade"].get('threshold', 0.6)
            score = grade_result.get('score', 0.0)
            print(f"\n  ðŸ“ Threshold comparison:")
            print(f"    Score: {score:.3f} {'â‰¥' if score >= threshold else '<'} Threshold: {threshold}")
        
    except Exception as e:
        print(f"âŒ Test Case 3 failed: {e}")
        import traceback
        traceback.print_exc()
    
    # í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ 4: ë‹µë³€ ì—†ìŒ (ì—ëŸ¬ ì¼€ì´ìŠ¤)
    print("\n" + "="*40)
    print("Test Case 4: No Answer (Error Case)")
    print("="*40)
    
    state_4 = {
        "query": "ì—”ì§„ ì˜¤ì¼ êµì²´ ë°©ë²•",
        # final_answerì™€ intermediate_answer ëª¨ë‘ ì—†ìŒ
        "documents": test_documents,
        "metadata": {},
        "workflow_status": "running"
    }
    
    try:
        print("âš ï¸  Testing with no answer...")
        result_4 = await node(state_4)
        
        grade_result = result_4.get("answer_grade", {})
        if not grade_result.get('is_valid'):
            print(f"âœ… Correctly handled missing answer")
            print(f"  - Reason: {grade_result.get('reason', 'N/A')}")
        
    except Exception as e:
        print(f"âŒ Unexpected error: {e}")
    
    print("\n" + "="*60)
    print("Answer Grader Test Completed")
    print("="*60)


if __name__ == "__main__":
    asyncio.run(test_answer_grader())