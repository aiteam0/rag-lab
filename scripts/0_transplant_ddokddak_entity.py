#!/usr/bin/env python3
"""
DDokDDak Entity Transplanter
DDokDDak(ë˜‘ë”±ì´) JSON íŒŒì¼ì˜ ë©”íƒ€ë°ì´í„°ë¥¼ DDU documentsì— ì´ì‹í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸

Usage:
    # ê¸°ë³¸ ì‹¤í–‰ (í™˜ê²½ë³€ìˆ˜ ì‚¬ìš©)
    uv run python scripts/transplant_ddokddak_entity.py
    
    # ì»¤ìŠ¤í…€ íŒŒì¼ ì§€ì •
    uv run python scripts/transplant_ddokddak_entity.py --ddokddak-json <path> --ddu-pickle <path>
    
Example:
    # ê¸°ë³¸ ì‹¤í–‰
    uv run python scripts/transplant_ddokddak_entity.py
    
    # íŠ¹ì • íŒŒì¼ ì§€ì •
    uv run python scripts/transplant_ddokddak_entity.py \
        --ddokddak-json data/ddokddak_custom.json \
        --ddu-pickle data/custom_ddu.pkl
"""

# ============================================================================
# í™˜ê²½ ë³€ìˆ˜ ì„¤ì • (DEFAULT VALUES)
# ============================================================================
DEFAULT_DDOKDDAK_JSON = "data/ddokddak_ë””ì§€í„¸ì •ë¶€í˜ì‹ _ì¶”ì§„ê³„íš_TEST3P_20250902_150632.json"
DEFAULT_DDU_PICKLE = "data/merged_ddu_documents.pkl"
DEFAULT_OUTPUT_PICKLE = "data/transplanted_ddu_documents.pkl"
DEFAULT_OUTPUT_JSON = "data/transplanted_ddu_documents.json"  # JSON ì¶œë ¥ ê²½ë¡œ

# ì´ì‹ ëŒ€ìƒ ì¹´í…Œê³ ë¦¬ ì„¤ì •
ALLOWED_CATEGORIES = ['paragraph', 'heading1', 'heading2', 'heading3']

# ì‹¤í–‰ ëª¨ë“œ ì„¤ì •
DEFAULT_DRY_RUN = False  # False: ì‹¤ì œ ì €ì¥, True: ê²€ì¦ë§Œ
DEFAULT_VERBOSE = True   # ìƒì„¸ ì¶œë ¥ ì—¬ë¶€
DEFAULT_SAVE_JSON = True  # JSONë„ í•¨ê»˜ ì €ì¥

# ============================================================================

import json
import pickle
import sys
import os
import shutil
import argparse
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Any, Optional
from collections import defaultdict
from dotenv import load_dotenv

# .env íŒŒì¼ ë¡œë“œ
load_dotenv()

# í™˜ê²½ë³€ìˆ˜ì—ì„œ ê°’ ì½ê¸° (override defaults if set)
DDOKDDAK_JSON = os.getenv("TRANSPLANT_DDOKDDAK_JSON", DEFAULT_DDOKDDAK_JSON)
DDU_PICKLE = os.getenv("TRANSPLANT_DDU_PICKLE", DEFAULT_DDU_PICKLE)
OUTPUT_PICKLE = os.getenv("TRANSPLANT_OUTPUT_PICKLE", DEFAULT_OUTPUT_PICKLE)
OUTPUT_JSON = os.getenv("TRANSPLANT_OUTPUT_JSON", DEFAULT_OUTPUT_JSON)
DRY_RUN = os.getenv("TRANSPLANT_DRY_RUN", str(DEFAULT_DRY_RUN)).lower() == "true"
VERBOSE = os.getenv("TRANSPLANT_VERBOSE", str(DEFAULT_VERBOSE)).lower() == "true"
SAVE_JSON = os.getenv("TRANSPLANT_SAVE_JSON", str(DEFAULT_SAVE_JSON)).lower() == "true"

# Langchain Document import
try:
    from langchain.schema import Document
except ImportError:
    from langchain_core.documents import Document


class DdokddakEntityTransplanter:
    """DDokDDak ë©”íƒ€ë°ì´í„°ë¥¼ DDU documentsì— ì´ì‹í•˜ëŠ” í´ë˜ìŠ¤"""
    
    # ì´ì‹ ëŒ€ìƒ ì¹´í…Œê³ ë¦¬ (ì „ì—­ ì„¤ì • ì‚¬ìš©)
    ALLOWED_CATEGORIES = ALLOWED_CATEGORIES
    
    def __init__(self, ddokddak_json_path: str, ddu_pickle_path: str):
        """
        ì´ˆê¸°í™” ë° ë°ì´í„° ê²€ì¦
        
        Args:
            ddokddak_json_path: DDokDDak JSON íŒŒì¼ ê²½ë¡œ
            ddu_pickle_path: DDU documents pickle íŒŒì¼ ê²½ë¡œ
        
        Raises:
            FileNotFoundError: íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•Šì„ ë•Œ
            ValueError: ë°ì´í„° ê²€ì¦ ì‹¤íŒ¨ì‹œ
        """
        print(f"\n{'='*60}")
        print("DDokDDak Entity Transplanter ì´ˆê¸°í™”")
        print(f"{'='*60}")
        
        # íŒŒì¼ ì¡´ì¬ í™•ì¸
        if not os.path.exists(ddokddak_json_path):
            raise FileNotFoundError(f"DDokDDak JSON file not found: {ddokddak_json_path}")
        if not os.path.exists(ddu_pickle_path):
            raise FileNotFoundError(f"DDU pickle file not found: {ddu_pickle_path}")
        
        # ë°ì´í„° ë¡œë“œ ë° ê²€ì¦
        print(f"ğŸ“„ Loading DDokDDak: {Path(ddokddak_json_path).name}")
        self.ddokddak_data = self._load_and_validate_ddokddak(ddokddak_json_path)
        
        print(f"ğŸ“¦ Loading DDU documents: {Path(ddu_pickle_path).name}")
        self.ddu_documents = self._load_ddu_pickle(ddu_pickle_path)
        print(f"   Total DDU documents: {len(self.ddu_documents)}")
        
        # í†µê³„ ì´ˆê¸°í™”
        self.transplant_stats = {
            'success': 0,
            'skipped': 0,
            'matched_docs': 0,
            'by_category': defaultdict(int)
        }
        
        # ë©”íƒ€ë°ì´í„° ì •ë³´ ì¶œë ¥
        self._print_metadata_info()
    
    def _load_and_validate_ddokddak(self, json_path: str) -> Dict[str, Any]:
        """
        DDokDDak ë°ì´í„° ë¡œë“œ ë° ê²€ì¦
        
        Args:
            json_path: JSON íŒŒì¼ ê²½ë¡œ
            
        Returns:
            ê²€ì¦ëœ DDokDDak ë°ì´í„°
            
        Raises:
            ValueError: ê²€ì¦ ì‹¤íŒ¨ì‹œ
        """
        with open(json_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        # ê¸°ë³¸ êµ¬ì¡° í™•ì¸
        if 'metadata' not in data:
            raise ValueError(f"Missing 'metadata' in DDokDDak JSON")
        if 'result' not in data:
            raise ValueError(f"Missing 'result' in DDokDDak JSON")
        
        # source_page None ì²´í¬
        source_page = data['metadata'].get('source_page')
        if source_page is None:
            raise ValueError(
                f"source_page cannot be None in DDokDDak JSON\n"
                f"  File: {json_path}"
            )
        
        # í•„ìˆ˜ í•„ë“œ ì²´í¬
        required_fields = ['title', 'keywords', 'hypothetical_questions']
        missing_fields = []
        for field in required_fields:
            if field not in data['result'] or not data['result'][field]:
                missing_fields.append(field)
        
        if missing_fields:
            raise ValueError(
                f"Missing or empty required fields in DDokDDak JSON:\n"
                f"  Fields: {', '.join(missing_fields)}\n"
                f"  File: {json_path}"
            )
        
        print(f"âœ… DDokDDak validation passed")
        return data
    
    def _load_ddu_pickle(self, pickle_path: str) -> List[Document]:
        """
        DDU pickle íŒŒì¼ ë¡œë“œ
        
        Args:
            pickle_path: Pickle íŒŒì¼ ê²½ë¡œ
            
        Returns:
            DDU documents ë¦¬ìŠ¤íŠ¸
        """
        with open(pickle_path, 'rb') as f:
            documents = pickle.load(f)
        
        # íƒ€ì… í™•ì¸
        if not isinstance(documents, list):
            raise ValueError(f"DDU pickle must contain a list, got {type(documents)}")
        
        if documents and not hasattr(documents[0], 'metadata'):
            raise ValueError(f"DDU documents must have metadata attribute")
        
        return documents
    
    def _print_metadata_info(self):
        """DDokDDak ë©”íƒ€ë°ì´í„° ì •ë³´ ì¶œë ¥"""
        print(f"\nğŸ“‹ DDokDDak Metadata:")
        print(f"   Source file: {self.ddokddak_data['metadata']['source_file']}")
        print(f"   Source page: {self.ddokddak_data['metadata']['source_page']}")
        print(f"   Title: {self.ddokddak_data['result']['title']}")
        print(f"   Keywords: {len(self.ddokddak_data['result']['keywords'])} items")
        print(f"   Questions: {len(self.ddokddak_data['result']['hypothetical_questions'])} items")
        
        # ì²˜ìŒ 3ê°œ í‚¤ì›Œë“œ ì¶œë ¥
        keywords = self.ddokddak_data['result']['keywords'][:3]
        print(f"   Sample keywords: {', '.join(keywords)}...")
    
    def find_matching_documents(self) -> List[Document]:
        """
        ì •í™•í•œ íŒŒì¼ëª…ê³¼ í˜ì´ì§€ë¡œ ë§¤ì¹­ë˜ëŠ” DDU ë¬¸ì„œ ì°¾ê¸°
        
        Returns:
            ë§¤ì¹­ëœ DDU documents ë¦¬ìŠ¤íŠ¸
            
        Raises:
            ValueError: ë§¤ì¹­ë˜ëŠ” ë¬¸ì„œê°€ ì—†ì„ ë•Œ
        """
        source_file = self.ddokddak_data['metadata']['source_file']
        source_page = self.ddokddak_data['metadata']['source_page']
        
        print(f"\nğŸ” Finding matching documents...")
        print(f"   Target file: {source_file}")
        print(f"   Target page: {source_page}")
        
        matches = []
        
        # ê° DDU ë¬¸ì„œ í™•ì¸
        for ddu in self.ddu_documents:
            ddu_source = ddu.metadata.get('source', '')
            ddu_page = ddu.metadata.get('page')
            
            # ì •í™•í•œ ë§¤ì¹­ (íŒŒì¼ëª…ì— data/ prefixê°€ ìˆì„ ìˆ˜ ìˆìŒ)
            source_match = (
                ddu_source == source_file or 
                ddu_source == f"data/{source_file}" or
                ddu_source.endswith(f"/{source_file}")
            )
            
            if source_match and ddu_page == source_page:
                matches.append(ddu)
        
        # ë§¤ì¹­ ì‹¤íŒ¨ì‹œ ì—ëŸ¬
        if not matches:
            # ë””ë²„ê¹…ì„ ìœ„í•´ ì²« 5ê°œ ë¬¸ì„œì˜ source ì¶œë ¥
            print("\nâŒ No matching documents found!")
            print("\nFirst 5 DDU document sources for debugging:")
            for i, ddu in enumerate(self.ddu_documents[:5]):
                print(f"   {i+1}. source: {ddu.metadata.get('source')}, page: {ddu.metadata.get('page')}")
            
            raise ValueError(
                f"No matching DDU documents found for:\n"
                f"  File: {source_file}\n"
                f"  Page: {source_page}\n"
                f"  Please check if the file name and page number are correct."
            )
        
        print(f"âœ… Found {len(matches)} matching documents")
        
        # ì¹´í…Œê³ ë¦¬ë³„ ë¶„ë¥˜
        category_counts = defaultdict(int)
        for doc in matches:
            category = doc.metadata.get('category', 'unknown')
            category_counts[category] += 1
        
        print(f"\nğŸ“Š Matching documents by category:")
        for cat, count in sorted(category_counts.items()):
            status = "âœ…" if cat in self.ALLOWED_CATEGORIES else "â­ï¸"
            print(f"   {status} {cat}: {count}")
        
        self.transplant_stats['matched_docs'] = len(matches)
        return matches
    
    def transform_entity(self, ddokddak_result: Dict[str, Any]) -> Dict[str, Any]:
        """
        DDokDDak ë°ì´í„°ë¥¼ DDU entity í˜•ì‹ìœ¼ë¡œ ë³€í™˜ (ê¸¸ì´ ì œí•œ ì—†ìŒ)
        
        Args:
            ddokddak_result: DDokDDak result ë°ì´í„°
            
        Returns:
            DDU entity í˜•ì‹ì˜ ë”•ì…”ë„ˆë¦¬
        """
        return {
            "type": "ë˜‘ë”±ì´",  # ê³ ì •ê°’
            "title": ddokddak_result.get("title", ""),
            "details": ddokddak_result.get("details", ""),  # ê¸¸ì´ ì œí•œ ì—†ìŒ
            "keywords": ddokddak_result.get("keywords", []),
            "hypothetical_questions": ddokddak_result.get("hypothetical_questions", []),
            "raw_output": ddokddak_result.get("raw_output", "")  # ê¸¸ì´ ì œí•œ ì—†ìŒ
        }
    
    def transplant_entities(self) -> Dict[str, Any]:
        """
        ë©”ì¸ ì´ì‹ ì‹¤í–‰
        
        Returns:
            ì´ì‹ í†µê³„
        """
        print(f"\n{'='*60}")
        print("ğŸ”„ Starting Entity Transplant")
        print(f"{'='*60}")
        
        try:
            # 1. ë§¤ì¹­ ë¬¸ì„œ ì°¾ê¸° (ì‹¤íŒ¨ì‹œ ì—ëŸ¬)
            matched_documents = self.find_matching_documents()
            
            # 2. Entity ë³€í™˜
            new_entity = self.transform_entity(self.ddokddak_data['result'])
            print(f"\nğŸ“ Entity prepared for transplant")
            print(f"   Type: {new_entity['type']}")
            print(f"   Title: {new_entity['title'][:50]}...")
            
            # 3. ê° ë§¤ì¹­ëœ ë¬¸ì„œ ì²˜ë¦¬
            print(f"\nğŸš€ Processing {len(matched_documents)} documents...")
            
            for i, ddu_doc in enumerate(matched_documents, 1):
                category = ddu_doc.metadata.get('category', 'unknown')
                doc_id = ddu_doc.metadata.get('id', f'doc_{i}')
                
                # í—ˆìš©ëœ ì¹´í…Œê³ ë¦¬ë§Œ ì²˜ë¦¬
                if category in self.ALLOWED_CATEGORIES:
                    # ê¸°ì¡´ entity ë°±ì—… (ìˆëŠ” ê²½ìš°)
                    if 'entity' in ddu_doc.metadata and ddu_doc.metadata['entity']:
                        ddu_doc.metadata['original_entity'] = ddu_doc.metadata['entity']
                    
                    # Entity ì´ì‹
                    ddu_doc.metadata['entity'] = new_entity
                    
                    # í†µê³„ ì—…ë°ì´íŠ¸
                    self.transplant_stats['success'] += 1
                    self.transplant_stats['by_category'][category] += 1
                    
                    print(f"   âœ… [{i}/{len(matched_documents)}] Transplanted to {category} (ID: {doc_id})")
                else:
                    self.transplant_stats['skipped'] += 1
                    print(f"   â­ï¸  [{i}/{len(matched_documents)}] Skipped {category} (not allowed)")
            
            print(f"\nâœ… Transplant completed successfully!")
            
        except Exception as e:
            print(f"\nâŒ Transplant failed: {e}")
            raise
        
        return self.transplant_stats
    
    def save_results(self, output_path: str, save_json: bool = False, json_path: str = None):
        """
        ê²°ê³¼ ì €ì¥ (pickle ë° ì„ íƒì ìœ¼ë¡œ JSON)
        
        Args:
            output_path: ì¶œë ¥ pickle íŒŒì¼ ê²½ë¡œ
            save_json: JSONë„ ì €ì¥í• ì§€ ì—¬ë¶€
            json_path: JSON íŒŒì¼ ê²½ë¡œ (Noneì´ë©´ output_path ê¸°ë°˜ìœ¼ë¡œ ìƒì„±)
        """
        print(f"\nğŸ’¾ Saving results...")
        
        # Pickle ì €ì¥
        # ë°±ì—… ìƒì„±
        if os.path.exists(output_path):
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            backup_path = f"{output_path}.backup_{timestamp}"
            shutil.copy(output_path, backup_path)
            print(f"   ğŸ“ Backup created: {Path(backup_path).name}")
        
        # ìˆ˜ì •ëœ ë°ì´í„° ì €ì¥
        with open(output_path, 'wb') as f:
            pickle.dump(self.ddu_documents, f)
        
        print(f"   âœ… Saved pickle to: {Path(output_path).name}")
        
        # íŒŒì¼ í¬ê¸° ì •ë³´
        file_size = os.path.getsize(output_path) / (1024 * 1024)  # MB
        print(f"   ğŸ“Š Pickle size: {file_size:.2f} MB")
        
        # JSON ì €ì¥ (ì„ íƒì )
        if save_json:
            if json_path is None:
                # pickle ê²½ë¡œë¥¼ ê¸°ë°˜ìœ¼ë¡œ JSON ê²½ë¡œ ìƒì„±
                json_path = str(Path(output_path).with_suffix('.json'))
            
            print(f"\n   ğŸ“ Converting to JSON...")
            
            # Document ê°ì²´ë¥¼ dictë¡œ ë³€í™˜
            json_documents = []
            for doc in self.ddu_documents:
                doc_dict = {
                    "page_content": doc.page_content,
                    "metadata": doc.metadata
                }
                json_documents.append(doc_dict)
            
            # JSON íŒŒì¼ë¡œ ì €ì¥
            with open(json_path, 'w', encoding='utf-8') as f:
                json.dump(json_documents, f, ensure_ascii=False, indent=2)
            
            print(f"   âœ… Saved JSON to: {Path(json_path).name}")
            
            # JSON íŒŒì¼ í¬ê¸°
            json_size = os.path.getsize(json_path) / (1024 * 1024)  # MB
            print(f"   ğŸ“Š JSON size: {json_size:.2f} MB")
    
    def generate_report(self):
        """ì´ì‹ ê²°ê³¼ ë³´ê³ ì„œ ì¶œë ¥"""
        print(f"\n{'='*60}")
        print("ğŸ“Š TRANSPLANT REPORT")
        print(f"{'='*60}")
        
        print(f"\nğŸ“ˆ Summary:")
        print(f"   Matched documents: {self.transplant_stats['matched_docs']}")
        print(f"   Successfully transplanted: {self.transplant_stats['success']}")
        print(f"   Skipped (wrong category): {self.transplant_stats['skipped']}")
        
        if self.transplant_stats['success'] > 0:
            success_rate = (self.transplant_stats['success'] / self.transplant_stats['matched_docs']) * 100
            print(f"   Success rate: {success_rate:.1f}%")
        
        if self.transplant_stats['by_category']:
            print(f"\nğŸ“‹ By Category:")
            for cat, count in sorted(self.transplant_stats['by_category'].items()):
                print(f"   â€¢ {cat}: {count}")
        
        print(f"\n{'='*60}")


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(
        description='DDokDDak Entity Transplanter - ë˜‘ë”±ì´ ë©”íƒ€ë°ì´í„°ë¥¼ DDU documentsì— ì´ì‹',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
í™˜ê²½ ë³€ìˆ˜ ì„¤ì • (.env íŒŒì¼ ë˜ëŠ” í™˜ê²½ë³€ìˆ˜):
  TRANSPLANT_DDOKDDAK_JSON  - DDokDDak JSON íŒŒì¼ ê²½ë¡œ
  TRANSPLANT_DDU_PICKLE     - DDU pickle íŒŒì¼ ê²½ë¡œ
  TRANSPLANT_OUTPUT_PICKLE  - ì¶œë ¥ pickle íŒŒì¼ ê²½ë¡œ
  TRANSPLANT_OUTPUT_JSON    - ì¶œë ¥ JSON íŒŒì¼ ê²½ë¡œ
  TRANSPLANT_DRY_RUN       - ê²€ì¦ë§Œ ìˆ˜í–‰ (true/false)
  TRANSPLANT_VERBOSE       - ìƒì„¸ ì¶œë ¥ (true/false)
  TRANSPLANT_SAVE_JSON     - JSONë„ í•¨ê»˜ ì €ì¥ (true/false)

Examples:
  # ê¸°ë³¸ ì‹¤í–‰ (í™˜ê²½ë³€ìˆ˜ ì‚¬ìš©)
  uv run python scripts/transplant_ddokddak_entity.py
  
  # ì»¤ìŠ¤í…€ íŒŒì¼ ì§€ì •
  uv run python scripts/transplant_ddokddak_entity.py \\
    --ddokddak-json data/custom.json \\
    --ddu-pickle data/custom.pkl
  
  # Dry run ëª¨ë“œ
  uv run python scripts/transplant_ddokddak_entity.py --dry-run
  
  # Batch processing
  for json in data/ddokddak_*.json; do
    uv run python scripts/transplant_ddokddak_entity.py --ddokddak-json "$json"
  done
        """
    )
    
    # ëª¨ë“  ì¸ìë¥¼ ì„ íƒì ìœ¼ë¡œ ë§Œë“¤ê³  ê¸°ë³¸ê°’ ì œê³µ
    parser.add_argument(
        '--ddokddak-json',
        default=DDOKDDAK_JSON,
        help=f'DDokDDak JSON íŒŒì¼ ê²½ë¡œ (default: {DDOKDDAK_JSON})'
    )
    parser.add_argument(
        '--ddu-pickle',
        default=DDU_PICKLE,
        help=f'DDU documents pickle íŒŒì¼ ê²½ë¡œ (default: {DDU_PICKLE})'
    )
    parser.add_argument(
        '--output-pickle',
        default=OUTPUT_PICKLE,
        help=f'ì¶œë ¥ pickle íŒŒì¼ ê²½ë¡œ (default: {OUTPUT_PICKLE})'
    )
    parser.add_argument(
        '--output-json',
        default=OUTPUT_JSON,
        help=f'ì¶œë ¥ JSON íŒŒì¼ ê²½ë¡œ (default: {OUTPUT_JSON})'
    )
    parser.add_argument(
        '--save-json',
        action='store_true',
        default=SAVE_JSON,
        help=f'JSON íŒŒì¼ë„ í•¨ê»˜ ì €ì¥ (default: {SAVE_JSON})'
    )
    parser.add_argument(
        '--dry-run',
        action='store_true',
        default=DRY_RUN,
        help=f'ê²€ì¦ë§Œ ìˆ˜í–‰, ê²°ê³¼ ì €ì¥í•˜ì§€ ì•ŠìŒ (default: {DRY_RUN})'
    )
    parser.add_argument(
        '--verbose',
        action='store_true',
        default=VERBOSE,
        help=f'ìƒì„¸ ì¶œë ¥ ëª¨ë“œ (default: {VERBOSE})'
    )
    
    args = parser.parse_args()
    
    print("\n" + "="*60)
    print("ğŸ§¬ DDokDDak Entity Transplanter")
    print("="*60)
    print(f"Version: 1.1.0")
    print(f"Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    # í˜„ì¬ ì„¤ì • ì¶œë ¥
    if args.verbose:
        print("\nğŸ“‹ Current Configuration:")
        print(f"   DDokDDak JSON: {args.ddokddak_json}")
        print(f"   DDU Pickle: {args.ddu_pickle}")
        print(f"   Output Pickle: {args.output_pickle}")
        print(f"   Output JSON: {args.output_json}")
        print(f"   Save JSON: {args.save_json}")
        print(f"   Dry Run: {args.dry_run}")
        print(f"   Verbose: {args.verbose}")
        print(f"   Allowed Categories: {', '.join(ALLOWED_CATEGORIES)}")
    
    try:
        # 1. Transplanter ì´ˆê¸°í™” (ê²€ì¦ í¬í•¨)
        transplanter = DdokddakEntityTransplanter(
            args.ddokddak_json,
            args.ddu_pickle
        )
        
        # 2. Entity ì´ì‹ ì‹¤í–‰
        stats = transplanter.transplant_entities()
        
        # 3. ê²°ê³¼ ì €ì¥ (dry-runì´ ì•„ë‹Œ ê²½ìš°)
        if not args.dry_run:
            transplanter.save_results(
                args.output_pickle,
                save_json=args.save_json,
                json_path=args.output_json
            )
        else:
            print("\nâš ï¸  Dry-run mode: Results not saved")
        
        # 4. ë³´ê³ ì„œ ì¶œë ¥
        transplanter.generate_report()
        
        print("\nâœ¨ All done!")
        return 0
        
    except FileNotFoundError as e:
        print(f"\nâŒ File Error: {e}")
        return 1
        
    except ValueError as e:
        print(f"\nâŒ Validation Error: {e}")
        return 1
        
    except Exception as e:
        print(f"\nâŒ Unexpected Error: {e}")
        import traceback
        traceback.print_exc()
        return 1


if __name__ == "__main__":
    sys.exit(main())