#!/usr/bin/env python3
"""
Retry ë©”ì»¤ë‹ˆì¦˜ í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸
í”¼ë“œë°± ê¸°ë°˜ ì¬ì‹œë„ê°€ ì œëŒ€ë¡œ ì‘ë™í•˜ëŠ”ì§€ ê²€ì¦
"""

import asyncio
import os
import sys
from pathlib import Path
from datetime import datetime
from rich.console import Console
from rich.table import Table
from rich.panel import Panel
from rich import print as rprint
from typing import Dict, Any

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
project_root = Path(__file__).resolve().parent.parent
sys.path.insert(0, str(project_root))

from workflow.nodes.synthesis import SynthesisNode
from workflow.state import MVPWorkflowState
from langchain_core.documents import Document

console = Console()

class RetryMechanismTester:
    """Retry ë©”ì»¤ë‹ˆì¦˜ í…ŒìŠ¤í„°"""
    
    def __init__(self):
        self.synthesis_node = SynthesisNode()
        
    async def test_hallucination_retry(self):
        """í™˜ê° ì²´í¬ ì‹¤íŒ¨ ì¬ì‹œë„ í…ŒìŠ¤íŠ¸"""
        console.print("\n[bold cyan]ğŸ”„ Testing Hallucination Retry Mechanism[/bold cyan]")
        
        # í…ŒìŠ¤íŠ¸ ë¬¸ì„œ
        documents = [
            Document(
                page_content="ì—”ì§„ ì˜¤ì¼ì€ ì •ê¸°ì ìœ¼ë¡œ êµì²´í•´ì•¼ í•©ë‹ˆë‹¤. ì—”ì§„ë£¸ì„ ì—´ê³  ì˜¤ì¼ ë ˆë²¨ì„ í™•ì¸í•˜ì„¸ìš”.",
                metadata={"source": "manual.pdf", "page": 45, "category": "maintenance"}
            ),
            Document(
                page_content="ëƒ‰ê°ìˆ˜ ë ˆë²¨ì„ í™•ì¸í•˜ë ¤ë©´ ì—”ì§„ì´ ì‹ì€ í›„ì— í™•ì¸í•´ì•¼ í•©ë‹ˆë‹¤.",
                metadata={"source": "manual.pdf", "page": 50, "category": "safety"}
            )
        ]
        
        # State ì¤€ë¹„ - ì²« ë²ˆì§¸ ì‹œë„
        state_first = {
            "query": "ì—”ì§„ ì˜¤ì¼ êµì²´ ë°©ë²•ì„ ì•Œë ¤ì£¼ì„¸ìš”",
            "documents": documents,
            "retry_count": 0,
            "metadata": {}
        }
        
        console.print("\n[yellow]1ï¸âƒ£ First attempt (no retry):[/yellow]")
        result1 = await self.synthesis_node(state_first)
        
        # ì²« ë²ˆì§¸ ë‹µë³€ ì¶œë ¥
        if "intermediate_answer" in result1:
            console.print(Panel(
                result1["intermediate_answer"][:300] + "...",
                title="First Answer (truncated)",
                border_style="yellow"
            ))
        
        # State ì¤€ë¹„ - ì¬ì‹œë„ (í™˜ê° í”¼ë“œë°± í¬í•¨)
        state_retry = {
            "query": "ì—”ì§„ ì˜¤ì¼ êµì²´ ë°©ë²•ì„ ì•Œë ¤ì£¼ì„¸ìš”",
            "documents": documents,
            "retry_count": 1,
            "hallucination_check": {
                "is_valid": False,
                "score": 0.3,
                "reason": "Answer contains unsupported claims",
                "suggestions": [
                    "Only include information explicitly stated in documents",
                    "Add page citations for all claims",
                    "Avoid mentioning specific oil capacity or intervals not in documents"
                ],
                "needs_retry": True
            },
            "metadata": {
                "hallucination_check": {
                    "hallucination_score": 0.7,
                    "problematic_claims": [
                        "ì˜¤ì¼ ìš©ëŸ‰ì´ 4.5Lì…ë‹ˆë‹¤",
                        "10,000kmë§ˆë‹¤ êµì²´í•˜ì„¸ìš”"
                    ],
                    "supported_claims": [
                        "ì—”ì§„ ì˜¤ì¼ì€ ì •ê¸°ì ìœ¼ë¡œ êµì²´í•´ì•¼ í•©ë‹ˆë‹¤"
                    ]
                }
            }
        }
        
        console.print("\n[green]2ï¸âƒ£ Retry with hallucination feedback:[/green]")
        result2 = await self.synthesis_node(state_retry)
        
        # ì¬ì‹œë„ ë‹µë³€ ì¶œë ¥
        if "intermediate_answer" in result2:
            console.print(Panel(
                result2["intermediate_answer"][:300] + "...",
                title="Corrected Answer (truncated)",
                border_style="green"
            ))
        
        # ê²°ê³¼ ë¹„êµ
        self._compare_results(result1, result2, "Hallucination")
        
    async def test_quality_retry(self):
        """í’ˆì§ˆ ì²´í¬ ì‹¤íŒ¨ ì¬ì‹œë„ í…ŒìŠ¤íŠ¸"""
        console.print("\n[bold cyan]ğŸ”„ Testing Quality Improvement Retry Mechanism[/bold cyan]")
        
        # í…ŒìŠ¤íŠ¸ ë¬¸ì„œ
        documents = [
            Document(
                page_content="íƒ€ì´ì–´ ê³µê¸°ì•• ì ê²€: 1. íƒ€ì´ì–´ê°€ ì°¨ê°€ìš´ ìƒíƒœì—ì„œ ì ê²€ 2. ê¶Œì¥ ê³µê¸°ì••ì€ ìš´ì „ì„ ë„ì–´ ìŠ¤í‹°ì»¤ í™•ì¸ 3. ì••ë ¥ê³„ë¡œ ì¸¡ì • 4. ë¶€ì¡±ì‹œ ë³´ì¶©",
                metadata={"source": "manual.pdf", "page": 120, "category": "maintenance"}
            ),
            Document(
                page_content="íƒ€ì´ì–´ ê³µê¸°ì•• ì ê²€ ì£¼ê¸°: ë§¤ì›” 1íšŒ ê¶Œì¥. ì¥ê±°ë¦¬ ìš´í–‰ ì „ í•„ìˆ˜ ì ê²€.",
                metadata={"source": "manual.pdf", "page": 121, "category": "maintenance"}
            )
        ]
        
        # State ì¤€ë¹„ - ì¬ì‹œë„ (í’ˆì§ˆ í”¼ë“œë°± í¬í•¨)
        state_retry = {
            "query": "íƒ€ì´ì–´ ê³µê¸°ì•• ì ê²€ ë°©ë²•",
            "documents": documents,
            "retry_count": 1,
            "answer_grade": {
                "is_valid": False,
                "score": 0.45,
                "reason": "Answer lacks completeness and specific details",
                "suggestions": [
                    "Add step-by-step procedure",
                    "Include safety warnings",
                    "Specify required tools",
                    "Add time estimates"
                ],
                "needs_retry": True
            },
            "metadata": {
                "answer_grade": {
                    "overall_score": 0.45,
                    "completeness": 0.3,
                    "relevance": 0.7,
                    "clarity": 0.6,
                    "usefulness": 0.2,
                    "missing_aspects": [
                        "Required tools (ì••ë ¥ê³„)",
                        "Safety precautions",
                        "Time required for the task"
                    ],
                    "strengths": [
                        "Clear language",
                        "Mentions basic steps"
                    ]
                }
            }
        }
        
        console.print("\n[green]ğŸ“ˆ Generating improved answer with quality feedback:[/green]")
        result = await self.synthesis_node(state_retry)
        
        # ê°œì„ ëœ ë‹µë³€ ì¶œë ¥
        if "intermediate_answer" in result:
            console.print(Panel(
                result["intermediate_answer"],
                title="Quality-Improved Answer",
                border_style="green"
            ))
            
        # ë©”íƒ€ë°ì´í„° ì¶œë ¥
        if "metadata" in result and "synthesis" in result["metadata"]:
            meta = result["metadata"]["synthesis"]
            console.print(f"\n[dim]Confidence: {meta.get('confidence', 0):.2f}[/dim]")
            console.print(f"[dim]Sources used: {meta.get('sources', [])}[/dim]")
    
    def _compare_results(self, result1: Dict, result2: Dict, test_type: str):
        """ê²°ê³¼ ë¹„êµ í…Œì´ë¸” ìƒì„±"""
        table = Table(title=f"{test_type} Retry Comparison")
        table.add_column("Metric", style="cyan")
        table.add_column("First Attempt", style="yellow")
        table.add_column("After Retry", style="green")
        
        # ë‹µë³€ ê¸¸ì´ ë¹„êµ
        len1 = len(result1.get("intermediate_answer", ""))
        len2 = len(result2.get("intermediate_answer", ""))
        table.add_row("Answer Length", str(len1), str(len2))
        
        # ì‹ ë¢°ë„ ë¹„êµ
        conf1 = result1.get("confidence_score", 0)
        conf2 = result2.get("confidence_score", 0)
        table.add_row("Confidence", f"{conf1:.2f}", f"{conf2:.2f}")
        
        # ì†ŒìŠ¤ ìˆ˜ ë¹„êµ
        sources1 = len(result1.get("metadata", {}).get("synthesis", {}).get("sources", []))
        sources2 = len(result2.get("metadata", {}).get("synthesis", {}).get("sources", []))
        table.add_row("Sources Used", str(sources1), str(sources2))
        
        console.print(table)
        
    async def run_all_tests(self):
        """ëª¨ë“  í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        console.print(Panel(
            "[bold]ğŸ§ª Retry Mechanism Test Suite[/bold]\n"
            "Testing feedback-based retry generation",
            border_style="cyan"
        ))
        
        try:
            # í™˜ê° ì¬ì‹œë„ í…ŒìŠ¤íŠ¸
            await self.test_hallucination_retry()
            
            # í’ˆì§ˆ ì¬ì‹œë„ í…ŒìŠ¤íŠ¸
            await self.test_quality_retry()
            
            console.print("\n[bold green]âœ… All retry mechanism tests completed![/bold green]")
            
        except Exception as e:
            console.print(f"\n[bold red]âŒ Test failed: {str(e)}[/bold red]")
            import traceback
            console.print(traceback.format_exc())

async def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    tester = RetryMechanismTester()
    await tester.run_all_tests()

if __name__ == "__main__":
    asyncio.run(main())