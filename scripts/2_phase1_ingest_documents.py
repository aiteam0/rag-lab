"""
Document Ingestion Script for MVP RAG System
Pickle íŒŒì¼ì—ì„œ ë¬¸ì„œë¥¼ ë¡œë“œí•˜ê³  ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥
"""

import asyncio
import os
import sys
import json
from pathlib import Path
from dotenv import load_dotenv
from tqdm.asyncio import tqdm
import time

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€
sys.path.append(str(Path(__file__).parent.parent))

from ingest.database import DatabaseManager
from ingest.loader import DDUPickleLoader
from ingest.embeddings import DualLanguageEmbeddings
from ingest.models import DDUDocument

# .env íŒŒì¼ ë¡œë“œ
load_dotenv()


async def ingest_documents(pickle_path: str, batch_size: int = 10):
    """
    DDU ë¬¸ì„œ ì¸ì œìŠ¤íŠ¸
    
    Args:
        pickle_path: Pickle íŒŒì¼ ê²½ë¡œ
        batch_size: ë°°ì¹˜ í¬ê¸°
    """
    print("=" * 60)
    print("MVP RAG System - Document Ingestion")
    print("=" * 60)
    
    # Pickle íŒŒì¼ ê²€ì¦
    print(f"\nğŸ“ Loading pickle file: {pickle_path}")
    if not DDUPickleLoader.validate_pickle_file(pickle_path):
        print("âŒ Invalid pickle file format")
        return
    
    # ë¡œë” ì´ˆê¸°í™”
    loader = DDUPickleLoader(pickle_path)
    
    # í†µê³„ ì¶œë ¥
    print("\nğŸ“Š Document Statistics:")
    stats = loader.get_statistics()
    print(f"  - Total Documents: {stats['total_documents']}")
    print(f"  - Categories: {stats['categories']}")
    print(f"  - Sources: {list(stats['sources'].keys())[:3]}...")
    print(f"  - Page Range: {stats['page_range']['min']} - {stats['page_range']['max']}")
    print(f"  - Has Translation: {stats['has_translation']} docs")
    print(f"  - Has Contextualize: {stats['has_contextualize']} docs")
    
    # ì‚¬ìš©ì í™•ì¸
    response = input(f"\nProceed with ingestion of {stats['total_documents']} documents? (y/N): ")
    if response.lower() != 'y':
        print("Ingestion cancelled")
        return
    
    # ë¬¸ì„œ ë¡œë“œ
    print("\nğŸ“Œ Loading documents...")
    documents = loader.load_documents()
    print(f"âœ… Loaded {len(documents)} documents")
    
    # ë°ì´í„°ë² ì´ìŠ¤ ë§¤ë‹ˆì € ì´ˆê¸°í™”
    db_manager = DatabaseManager()
    await db_manager.initialize()
    
    # ì„ë² ë”© ìƒì„±ê¸° ì´ˆê¸°í™”
    embeddings = DualLanguageEmbeddings()
    
    # ë°°ì¹˜ ì²˜ë¦¬
    print(f"\nğŸ“Œ Starting ingestion (batch size: {batch_size})...")
    start_time = time.time()
    
    success_count = 0
    error_count = 0
    
    # tqdmìœ¼ë¡œ ì§„í–‰ ìƒí™© í‘œì‹œ
    for i in tqdm(range(0, len(documents), batch_size), desc="Ingesting"):
        batch = documents[i:i+batch_size]
        
        for doc in batch:
            try:
                # ë¬¸ì„œë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜
                doc_dict = doc.to_db_dict()
                
                # ì„ë² ë”© ìƒì„±
                korean_emb, english_emb = await embeddings.embed_document(doc_dict)
                
                # entity í•„ë“œë¥¼ JSON ë¬¸ìì—´ë¡œ ë³€í™˜
                entity_json = None
                if doc_dict.get("entity") is not None:
                    entity_json = json.dumps(doc_dict.get("entity"), ensure_ascii=False)
                
                # ë²¡í„°ë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜ (pgvector í˜•ì‹)
                korean_emb_str = None
                if korean_emb:
                    korean_emb_str = f"[{','.join(map(str, korean_emb))}]"
                
                english_emb_str = None  
                if english_emb:
                    english_emb_str = f"[{','.join(map(str, english_emb))}]"
                
                # DB ì €ì¥
                async with db_manager.pool.acquire() as conn:
                    await conn.execute("""
                        INSERT INTO mvp_ddu_documents (
                            source, page, category, page_content,
                            translation_text, contextualize_text, caption,
                            entity, image_path, human_feedback,
                            embedding_korean, embedding_english,
                            search_vector_korean, search_vector_english
                        ) VALUES ($1, $2, $3, $4, $5, $6, $7, $8::jsonb, $9, $10,
                                 $11::vector, $12::vector,
                                 to_tsvector('simple', COALESCE($13, '')),
                                 to_tsvector('english', COALESCE($14, '')))
                    """,
                        doc_dict.get("source"),
                        doc_dict.get("page"),
                        doc_dict.get("category"),
                        doc_dict.get("page_content"),
                        doc_dict.get("translation_text"),
                        doc_dict.get("contextualize_text"),
                        doc_dict.get("caption"),
                        entity_json,  # JSON ë¬¸ìì—´ë¡œ ë³€í™˜ëœ entity
                        doc_dict.get("image_path"),
                        doc_dict.get("human_feedback", ""),
                        korean_emb_str,  # ë¬¸ìì—´ë¡œ ë³€í™˜ëœ ë²¡í„°
                        english_emb_str,  # ë¬¸ìì—´ë¡œ ë³€í™˜ëœ ë²¡í„°
                        # í•œêµ­ì–´ ê²€ìƒ‰ í…ìŠ¤íŠ¸
                        (doc_dict.get("contextualize_text", "") + " " + 
                         doc_dict.get("page_content", "") + " " +
                         doc_dict.get("caption", "")),
                        # ì˜ì–´ ê²€ìƒ‰ í…ìŠ¤íŠ¸
                        doc_dict.get("translation_text", "")
                    )
                
                success_count += 1
                
            except Exception as e:
                print(f"\nâš ï¸  Error ingesting document {doc.source} (page {doc.page}): {e}")
                error_count += 1
    
    # ì²˜ë¦¬ ì‹œê°„ ê³„ì‚°
    elapsed_time = time.time() - start_time
    
    # ìµœì¢… í†µê³„ ì¶œë ¥
    print("\n" + "=" * 60)
    print("Ingestion Summary")
    print("=" * 60)
    print(f"âœ… Successfully ingested: {success_count} documents")
    if error_count > 0:
        print(f"âŒ Failed: {error_count} documents")
    print(f"â±ï¸  Total time: {elapsed_time:.2f} seconds")
    print(f"ğŸ“ˆ Average: {elapsed_time/len(documents):.3f} seconds per document")
    
    # DB í†µê³„ í™•ì¸
    print("\nğŸ“Š Final Database Statistics:")
    stats = await db_manager.get_table_stats()
    print(f"  - Total Documents: {stats['total_documents']}")
    for category, count in list(stats['categories'].items())[:5]:
        print(f"  - {category}: {count} docs")
    
    # ì—°ê²° ì¢…ë£Œ
    await db_manager.close()
    print("\nâœ… Ingestion completed successfully!")


async def test_ingestion(pickle_path: str, limit: int = 5):
    """
    ì†Œìˆ˜ì˜ ë¬¸ì„œë¡œ ì¸ì œìŠ¤íŠ¸ í…ŒìŠ¤íŠ¸
    
    Args:
        pickle_path: Pickle íŒŒì¼ ê²½ë¡œ
        limit: í…ŒìŠ¤íŠ¸í•  ë¬¸ì„œ ìˆ˜
    """
    print(f"\nğŸ§ª Test Mode: Ingesting first {limit} documents")
    
    # ë¡œë” ì´ˆê¸°í™”
    loader = DDUPickleLoader(pickle_path)
    documents = loader.load_documents()[:limit]
    
    # ë°ì´í„°ë² ì´ìŠ¤ ë§¤ë‹ˆì € ì´ˆê¸°í™”
    db_manager = DatabaseManager()
    await db_manager.initialize()
    
    # ì„ë² ë”© ìƒì„±ê¸° ì´ˆê¸°í™”
    embeddings = DualLanguageEmbeddings()
    
    for i, doc in enumerate(documents, 1):
        print(f"\nğŸ“„ Document {i}/{limit}")
        print(f"  - Source: {doc.source}")
        print(f"  - Page: {doc.page}")
        print(f"  - Category: {doc.category}")
        print(f"  - Content Length: {len(doc.page_content or '')} chars")
        
        # ì„ë² ë”© ìƒì„±
        doc_dict = doc.to_db_dict()
        korean_emb, english_emb = await embeddings.embed_document(doc_dict)
        
        print(f"  - Korean Embedding: {'âœ…' if korean_emb else 'âŒ'}")
        print(f"  - English Embedding: {'âœ…' if english_emb else 'âŒ'}")
    
    await db_manager.close()
    print("\nâœ… Test completed!")


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    
    print("\nMVP RAG System - Document Ingestion Utility")
    print("=" * 60)
    
    # ê¸°ë³¸ pickle íŒŒì¼ ê²½ë¡œ
    default_pickle = "/mnt/e/MyProject2/multimodal-rag-wsl-v2/data/gv80_owners_manual_TEST6P_documents.pkl"
    
    # Pickle íŒŒì¼ ì„ íƒ
    if os.path.exists(default_pickle):
        print(f"\nDefault pickle file found: {default_pickle}")
        use_default = input("Use default file? (Y/n): ")
        if use_default.lower() != 'n':
            pickle_path = default_pickle
        else:
            pickle_path = input("Enter pickle file path: ")
    else:
        pickle_path = input("Enter pickle file path: ")
    
    # íŒŒì¼ ì¡´ì¬ í™•ì¸
    if not os.path.exists(pickle_path):
        print(f"âŒ File not found: {pickle_path}")
        return
    
    print("\nOptions:")
    print("1. Full ingestion")
    print("2. Test ingestion (first 5 documents)")
    print("3. Exit")
    
    choice = input("\nSelect option (1-3): ")
    
    if choice == "1":
        batch_size = input("Batch size (default 10): ")
        batch_size = int(batch_size) if batch_size else 10
        asyncio.run(ingest_documents(pickle_path, batch_size))
    elif choice == "2":
        asyncio.run(test_ingestion(pickle_path))
    elif choice == "3":
        print("Exiting...")
    else:
        print("Invalid option")


if __name__ == "__main__":
    # í™˜ê²½ ë³€ìˆ˜ ì²´í¬
    required_env = ["DB_HOST", "DB_PORT", "DB_NAME", "DB_USER", "DB_PASSWORD", 
                    "OPENAI_API_KEY", "OPENAI_EMBEDDING_MODEL"]
    missing = [var for var in required_env if not os.getenv(var)]
    
    if missing:
        print(f"âŒ Missing environment variables: {', '.join(missing)}")
        print("Please create a .env file with the required variables")
        sys.exit(1)
    
    main()