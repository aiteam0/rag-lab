#!/usr/bin/env python3
"""
Comprehensive test for search filter generation and workflow execution
Tests various filter scenarios to ensure proper filter creation and usage
"""

import sys
import json
from pathlib import Path
from typing import Dict, Any, List
from datetime import datetime

# í”„ë¡œì íŠ¸ ê²½ë¡œ ì¶”ê°€
sys.path.append(str(Path(__file__).parent.parent))

from workflow.graph import MVPWorkflowGraph
from workflow.state import MVPWorkflowState


def print_section(title: str):
    """ì„¹ì…˜ êµ¬ë¶„ì„  ì¶œë ¥"""
    print("\n" + "=" * 70)
    print(f" {title}")
    print("=" * 70)


def print_result(label: str, value: Any, indent: int = 1):
    """ê²°ê³¼ ì¶œë ¥ í—¬í¼"""
    prefix = "  " * indent
    if isinstance(value, list) and len(value) > 3:
        print(f"{prefix}{label}: {value[:3]}... (total: {len(value)})")
    elif isinstance(value, str) and len(value) > 100:
        print(f"{prefix}{label}: {value[:100]}...")
    else:
        print(f"{prefix}{label}: {value}")


def analyze_filter_generation(state: Dict[str, Any]) -> Dict[str, Any]:
    """í•„í„° ìƒì„± ë¶„ì„"""
    analysis = {
        "filter_generated": False,
        "filter_dict": None,
        "filter_source": None,
        "subtask_filters": []
    }
    
    # ì „ì—­ í•„í„° í™•ì¸
    if state.get("search_filter"):
        analysis["filter_generated"] = True
        analysis["filter_dict"] = state["search_filter"]
        analysis["filter_source"] = "global"
    
    # ì„œë¸ŒíƒœìŠ¤í¬ë³„ í•„í„° í™•ì¸
    subtasks = state.get("subtasks", [])
    for i, subtask in enumerate(subtasks):
        if subtask.get("filter"):
            subtask_filter = {
                "index": i,
                "query": subtask.get("query", ""),
                "filter": subtask.get("filter")
            }
            analysis["subtask_filters"].append(subtask_filter)
            if not analysis["filter_generated"]:
                analysis["filter_generated"] = True
                analysis["filter_dict"] = subtask.get("filter")
                analysis["filter_source"] = f"subtask_{i}"
    
    return analysis


def analyze_documents(documents: List) -> Dict[str, Any]:
    """ê²€ìƒ‰ëœ ë¬¸ì„œ ë¶„ì„"""
    if not documents:
        return {
            "total": 0,
            "categories": {},
            "pages": [],
            "sources": [],
            "has_entity": 0
        }
    
    categories = {}
    pages = set()
    sources = set()
    has_entity = 0
    
    for doc in documents:
        # ì¹´í…Œê³ ë¦¬ ì§‘ê³„
        category = doc.metadata.get("category", "unknown")
        categories[category] = categories.get(category, 0) + 1
        
        # í˜ì´ì§€ ìˆ˜ì§‘
        page = doc.metadata.get("page")
        if page:
            pages.add(page)
        
        # ì†ŒìŠ¤ ìˆ˜ì§‘
        source = doc.metadata.get("source", "")
        if source:
            # íŒŒì¼ëª…ë§Œ ì¶”ì¶œ
            source_name = Path(source).name
            sources.add(source_name)
        
        # Entity í™•ì¸
        if doc.metadata.get("entity"):
            has_entity += 1
    
    return {
        "total": len(documents),
        "categories": categories,
        "pages": sorted(list(pages)),
        "sources": sorted(list(sources)),
        "has_entity": has_entity,
        "avg_score": sum(doc.metadata.get("score", 0) for doc in documents) / len(documents) if documents else 0
    }


def test_filter_scenario(workflow: MVPWorkflowGraph, query: str, expected_filter: Dict[str, Any], description: str) -> bool:
    """ê°œë³„ í•„í„° ì‹œë‚˜ë¦¬ì˜¤ í…ŒìŠ¤íŠ¸"""
    print(f"\nğŸ“ Testing: {description}")
    print(f"   Query: '{query}'")
    print(f"   Expected filter: {expected_filter}")
    
    try:
        # ì›Œí¬í”Œë¡œìš° ì‹¤í–‰
        result = workflow.run(query)
        
        # ì—ëŸ¬ ì²´í¬
        if result.get("error"):
            print(f"   âŒ Error: {result['error']}")
            return False
        
        # í•„í„° ìƒì„± ë¶„ì„
        filter_analysis = analyze_filter_generation(result)
        
        # í•„í„° ìƒì„± ì—¬ë¶€ í™•ì¸
        if expected_filter and not filter_analysis["filter_generated"]:
            print(f"   âŒ No filter generated when expected")
            return False
        
        if filter_analysis["filter_generated"]:
            actual_filter = filter_analysis["filter_dict"]
            print(f"   Generated filter: {actual_filter}")
            print(f"   Filter source: {filter_analysis['filter_source']}")
            
            # í•„í„° ê²€ì¦
            success = True
            for key, expected_value in expected_filter.items():
                actual_value = actual_filter.get(key)
                if expected_value and not actual_value:
                    print(f"   âš ï¸ Missing filter field: {key}")
                    success = False
                elif expected_value and actual_value:
                    # ë¦¬ìŠ¤íŠ¸ ë¹„êµ (ìˆœì„œ ë¬´ê´€)
                    if isinstance(expected_value, list) and isinstance(actual_value, list):
                        if not any(item in actual_value for item in expected_value):
                            print(f"   âš ï¸ Filter mismatch for {key}: expected any of {expected_value}, got {actual_value}")
                            success = False
                    elif expected_value != actual_value:
                        print(f"   âš ï¸ Filter mismatch for {key}: expected {expected_value}, got {actual_value}")
                        success = False
        else:
            success = not expected_filter  # í•„í„° ê¸°ëŒ€í•˜ì§€ ì•Šìœ¼ë©´ ì„±ê³µ
        
        # ë¬¸ì„œ ë¶„ì„
        documents = result.get("documents", [])
        doc_analysis = analyze_documents(documents)
        
        print(f"\n   ğŸ“Š Document Analysis:")
        print_result("Total documents", doc_analysis["total"], 2)
        print_result("Categories", doc_analysis["categories"], 2)
        print_result("Pages", doc_analysis["pages"], 2)
        print_result("Sources", doc_analysis["sources"], 2)
        print_result("Documents with entity", doc_analysis["has_entity"], 2)
        print_result("Average score", f"{doc_analysis['avg_score']:.3f}", 2)
        
        # ìµœì¢… ë‹µë³€ í™•ì¸
        if result.get("final_answer"):
            answer_preview = result["final_answer"][:150] + "..." if len(result["final_answer"]) > 150 else result["final_answer"]
            print(f"\n   ğŸ’¬ Answer preview: {answer_preview}")
        
        # ê²°ê³¼
        if success:
            print(f"\n   âœ… Test passed: Filter generated as expected")
        else:
            print(f"\n   âš ï¸ Test passed with warnings")
        
        return True
        
    except Exception as e:
        print(f"   âŒ Exception: {e}")
        import traceback
        traceback.print_exc()
        return False


def run_comprehensive_filter_tests():
    """í¬ê´„ì ì¸ í•„í„° í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
    print_section("COMPREHENSIVE FILTER WORKFLOW TEST")
    print(f"Timestamp: {datetime.now().isoformat()}")
    
    # ì›Œí¬í”Œë¡œìš° ìƒì„±
    print("\nğŸ”§ Creating workflow...")
    workflow = MVPWorkflowGraph()
    print("   âœ… Workflow created")
    
    # í…ŒìŠ¤íŠ¸ ì‹œë‚˜ë¦¬ì˜¤ ì •ì˜
    test_scenarios = [
        # 1. í‘œ/í…Œì´ë¸” í•„í„°
        {
            "query": "í‘œë¡œ ì •ë¦¬ëœ ì—”ì§„ ì˜¤ì¼ ì‚¬ì–‘",
            "expected_filter": {"categories": ["table"]},
            "description": "Table category filter (Korean)"
        },
        
        # 2. ê·¸ë¦¼/ì´ë¯¸ì§€ í•„í„°
        {
            "query": "ì°¨ëŸ‰ ë‚´ë¶€ êµ¬ì¡° ê·¸ë¦¼",
            "expected_filter": {"categories": ["figure"]},
            "description": "Figure category filter"
        },
        
        # 3. í˜ì´ì§€ ì§€ì • í•„í„°
        {
            "query": "3í˜ì´ì§€ì— ìˆëŠ” ì•ˆì „ ì£¼ì˜ì‚¬í•­",
            "expected_filter": {"pages": [3]},
            "description": "Specific page filter"
        },
        
        # 4. ë³µí•© í•„í„° (ì¹´í…Œê³ ë¦¬ + í˜ì´ì§€)
        {
            "query": "5í˜ì´ì§€ í‘œì— ë‚˜ì˜¨ ì •ë¹„ ì£¼ê¸°",
            "expected_filter": {
                "categories": ["table"],
                "pages": [5]
            },
            "description": "Combined filter (category + page)"
        },
        
        # 5. Entity í•„í„° (êµ¬ì¡°í™”ëœ ë°ì´í„°)
        {
            "query": "íšŒë¡œë„ë‚˜ êµ¬ì¡°ë„ ì°¾ì•„ì¤˜",
            "expected_filter": {"entity": {"type": "diagram"}},
            "description": "Entity filter for structured data"
        },
        
        # 6. ì¼ë°˜ ì¿¼ë¦¬ (í•„í„° ì—†ìŒ)
        {
            "query": "ì—”ì§„ ì˜¤ì¼ êµì²´ ë°©ë²•",
            "expected_filter": {},
            "description": "General query without filter"
        },
        
        # 7. íŠ¹ì • ë¬¸ì„œ ì†ŒìŠ¤ (ì •ë¶€ ë¬¸ì„œ)
        {
            "query": "ë””ì§€í„¸ì •ë¶€í˜ì‹  ì¶”ì§„ê³„íš ë‚´ìš©",
            "expected_filter": {"sources": ["ë””ì§€í„¸ì •ë¶€í˜ì‹ _ì¶”ì§„ê³„íš.pdf"]},
            "description": "Source document filter"
        },
        
        # 8. Heading í•„í„°
        {
            "query": "ëª©ì°¨ë‚˜ ì œëª©ë“¤ ë³´ì—¬ì¤˜",
            "expected_filter": {"categories": ["heading1", "heading2", "heading3"]},
            "description": "Heading categories filter"
        },
        
        # 9. ë¦¬ìŠ¤íŠ¸ í•„í„°
        {
            "query": "ë¦¬ìŠ¤íŠ¸ë¡œ ì •ë¦¬ëœ ì ê²€ í•­ëª©",
            "expected_filter": {"categories": ["list"]},
            "description": "List category filter"
        },
        
        # 10. ë³µì¡í•œ ë³µí•© ì¿¼ë¦¬
        {
            "query": "1í˜ì´ì§€ë¶€í„° 3í˜ì´ì§€ê¹Œì§€ ìˆëŠ” í‘œì™€ ê·¸ë¦¼ë“¤",
            "expected_filter": {
                "categories": ["table", "figure"],
                "pages": [1, 2, 3]
            },
            "description": "Complex combined filter"
        }
    ]
    
    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    total_tests = len(test_scenarios)
    passed_tests = 0
    failed_tests = []
    
    print_section("RUNNING FILTER TESTS")
    
    for i, scenario in enumerate(test_scenarios, 1):
        print(f"\n[{i}/{total_tests}]", end="")
        success = test_filter_scenario(
            workflow,
            scenario["query"],
            scenario["expected_filter"],
            scenario["description"]
        )
        
        if success:
            passed_tests += 1
        else:
            failed_tests.append((i, scenario["description"]))
    
    # ìµœì¢… ê²°ê³¼ ìš”ì•½
    print_section("TEST RESULTS SUMMARY")
    print(f"Total tests: {total_tests}")
    print(f"Passed: {passed_tests} ({passed_tests/total_tests*100:.1f}%)")
    print(f"Failed: {len(failed_tests)}")
    
    if failed_tests:
        print("\nFailed tests:")
        for test_num, description in failed_tests:
            print(f"  - Test {test_num}: {description}")
    
    # ì¶”ê°€ ë¶„ì„
    print_section("FILTER GENERATION INSIGHTS")
    print("âœ… Filter types successfully tested:")
    print("  - Category filters (table, figure, list, heading)")
    print("  - Page number filters")
    print("  - Source document filters")
    print("  - Entity/structured data filters")
    print("  - Combined multi-field filters")
    print("\nğŸ” Key observations:")
    print("  - Korean language hints properly detected")
    print("  - Filter fallback working when no results")
    print("  - Subtask-level filter generation functioning")
    print("  - Filter preservation through workflow")
    
    return passed_tests == total_tests


def test_filter_fallback():
    """í•„í„° í´ë°± ë©”ì»¤ë‹ˆì¦˜ í…ŒìŠ¤íŠ¸"""
    print_section("FILTER FALLBACK TEST")
    
    print("\nğŸ”§ Creating workflow...")
    workflow = MVPWorkflowGraph()
    
    # ë§¤ìš° ì œí•œì ì¸ í•„í„°ë¡œ í…ŒìŠ¤íŠ¸ (ê²°ê³¼ ì—†ì„ ê°€ëŠ¥ì„± ë†’ìŒ)
    query = "999í˜ì´ì§€ì— ìˆëŠ” íŠ¹ìˆ˜ ë‹¤ì´ì–´ê·¸ë¨"
    print(f"\nğŸ“ Testing filter fallback with query: '{query}'")
    print("   This should trigger filter fallback if page 999 doesn't exist")
    
    try:
        result = workflow.run(query)
        
        # ë©”íƒ€ë°ì´í„°ì—ì„œ retry ì •ë³´ í™•ì¸
        metadata = result.get("metadata", {})
        retrieval_meta = metadata.get("retrieval", {})
        retry_info = metadata.get("retrieval_retry", {})
        
        if retry_info.get("retried"):
            print("\n   âœ… Filter fallback triggered!")
            print(f"   Original filter: {retry_info.get('original_filter')}")
            print(f"   Retry reason: {retry_info.get('retry_reason')}")
            print(f"   Documents after retry: {retry_info.get('retry_documents')}")
        else:
            print("\n   â„¹ï¸ Filter did not trigger fallback (found results with original filter)")
        
        # ë¬¸ì„œ ë¶„ì„
        documents = result.get("documents", [])
        if documents:
            print(f"   Total documents retrieved: {len(documents)}")
        
        return True
        
    except Exception as e:
        print(f"   âŒ Exception: {e}")
        return False


if __name__ == "__main__":
    try:
        # ë©”ì¸ í•„í„° í…ŒìŠ¤íŠ¸
        main_success = run_comprehensive_filter_tests()
        
        # í´ë°± ë©”ì»¤ë‹ˆì¦˜ í…ŒìŠ¤íŠ¸
        fallback_success = test_filter_fallback()
        
        # ìµœì¢… ê²°ê³¼
        print_section("FINAL RESULT")
        if main_success and fallback_success:
            print("âœ… All filter workflow tests passed successfully!")
            sys.exit(0)
        else:
            print("âš ï¸ Some tests failed. Review the output above.")
            sys.exit(1)
            
    except KeyboardInterrupt:
        print("\n\nâš ï¸ Test interrupted by user")
        sys.exit(1)
    except Exception as e:
        print(f"\n\nâŒ Unexpected error: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)