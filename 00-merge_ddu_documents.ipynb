{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DDU Documents Merger\n",
    "\n",
    "ì´ ë…¸íŠ¸ë¶ì€ ë‘ ê°œì˜ DDU (Document Decomposition Unit) pickle íŒŒì¼ì„ í•˜ë‚˜ë¡œ ë³‘í•©í•©ë‹ˆë‹¤.\n",
    "\n",
    "## ë³‘í•©í•  íŒŒì¼:\n",
    "1. `gv80_owners_manual_TEST6P_documents.pkl` - GV80 ì°¨ëŸ‰ ë§¤ë‰´ì–¼\n",
    "2. `ë””ì§€í„¸ì •ë¶€í˜ì‹ _ì¶”ì§„ê³„íš_documents.pkl` - ë””ì§€í„¸ ì •ë¶€ í˜ì‹  ê³„íš ë¬¸ì„œ\n",
    "\n",
    "## DDU Document Schema\n",
    "ê° ë¬¸ì„œëŠ” Langchain Document ê°ì²´ë¡œ ë‹¤ìŒ êµ¬ì¡°ë¥¼ ê°€ì§‘ë‹ˆë‹¤:\n",
    "- `page_content`: ì‹¤ì œ í…ìŠ¤íŠ¸ ë‚´ìš©\n",
    "- `metadata`: 14ê°œ ì¹´í…Œê³ ë¦¬ì™€ ë‹¤ì–‘í•œ ë©”íƒ€ë°ì´í„°"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì‘ì—… ë””ë ‰í† ë¦¬: /mnt/e/MyProject2/multimodal-rag-wsl-v2\n",
      "ì‹¤í–‰ ì‹œê°„: 2025-08-26 16:10:23\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Langchain Document íƒ€ì… ì„í¬íŠ¸\n",
    "try:\n",
    "    from langchain.schema import Document\n",
    "except ImportError:\n",
    "    from langchain_core.documents import Document\n",
    "\n",
    "print(f\"ì‘ì—… ë””ë ‰í† ë¦¬: {os.getcwd()}\")\n",
    "print(f\"ì‹¤í–‰ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. ë°ì´í„° íŒŒì¼ ê²½ë¡œ ì„¤ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GV80 íŒŒì¼ ì¡´ì¬: True\n",
      "ë””ì§€í„¸ì •ë¶€ íŒŒì¼ ì¡´ì¬: True\n"
     ]
    }
   ],
   "source": [
    "# ë°ì´í„° ë””ë ‰í† ë¦¬ ê²½ë¡œ\n",
    "DATA_DIR = Path(\"data\")\n",
    "\n",
    "# ì…ë ¥ íŒŒì¼ ê²½ë¡œ\n",
    "GV80_FILE = DATA_DIR / \"gv80_owners_manual_TEST6P_documents.pkl\"\n",
    "DIGITAL_GOV_FILE = DATA_DIR / \"ë””ì§€í„¸ì •ë¶€í˜ì‹ _ì¶”ì§„ê³„íš_documents.pkl\"\n",
    "\n",
    "# ì¶œë ¥ íŒŒì¼ ê²½ë¡œ\n",
    "MERGED_FILE = DATA_DIR / \"merged_ddu_documents.pkl\"\n",
    "\n",
    "# íŒŒì¼ ì¡´ì¬ í™•ì¸\n",
    "print(f\"GV80 íŒŒì¼ ì¡´ì¬: {GV80_FILE.exists()}\")\n",
    "print(f\"ë””ì§€í„¸ì •ë¶€ íŒŒì¼ ì¡´ì¬: {DIGITAL_GOV_FILE.exists()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Pickle íŒŒì¼ ë¡œë“œ ë° êµ¬ì¡° í™•ì¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ë¡œë”© ì¤‘: gv80_owners_manual_TEST6P_documents.pkl\n",
      "- ë¬¸ì„œ ê°œìˆ˜: 122\n",
      "- ì¹´í…Œê³ ë¦¬ ë¶„í¬:\n",
      "  â€¢ caption: 4\n",
      "  â€¢ figure: 10\n",
      "  â€¢ header: 3\n",
      "  â€¢ heading1: 23\n",
      "  â€¢ list: 1\n",
      "  â€¢ paragraph: 78\n",
      "  â€¢ table: 3\n",
      "\n",
      "- ì²« ë²ˆì§¸ ë¬¸ì„œ ë©”íƒ€ë°ì´í„° í‚¤: ['source', 'page', 'category', 'id', 'raw_output', 'translation_text', 'translation_markdown', 'translation_html', 'contextualize_text', 'caption', 'entity', 'image_path', 'coordinates', 'processing_type', 'processing_status', 'source_parser', 'element_type', 'human_feedback']\n",
      "- ì†ŒìŠ¤: data/gv80_owners_manual_TEST6P.pdf\n",
      "\n",
      "ë¡œë”© ì¤‘: ë””ì§€í„¸ì •ë¶€í˜ì‹ _ì¶”ì§„ê³„íš_documents.pkl\n",
      "- ë¬¸ì„œ ê°œìˆ˜: 158\n",
      "- ì¹´í…Œê³ ë¦¬ ë¶„í¬:\n",
      "  â€¢ figure: 3\n",
      "  â€¢ header: 2\n",
      "  â€¢ heading1: 34\n",
      "  â€¢ list: 4\n",
      "  â€¢ paragraph: 104\n",
      "  â€¢ table: 11\n",
      "\n",
      "- ì²« ë²ˆì§¸ ë¬¸ì„œ ë©”íƒ€ë°ì´í„° í‚¤: ['source', 'page', 'category', 'id', 'raw_output', 'translation_text', 'translation_markdown', 'translation_html', 'contextualize_text', 'caption', 'entity', 'image_path', 'coordinates', 'processing_type', 'processing_status', 'source_parser', 'element_type', 'human_feedback']\n",
      "- ì†ŒìŠ¤: data/ë””ì§€í„¸ì •ë¶€í˜ì‹ _ì¶”ì§„ê³„íš.pdf\n"
     ]
    }
   ],
   "source": [
    "def load_pickle_file(file_path: Path) -> List[Document]:\n",
    "    \"\"\"Pickle íŒŒì¼ì„ ë¡œë“œí•˜ê³  ê¸°ë³¸ ì •ë³´ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤.\"\"\"\n",
    "    print(f\"\\në¡œë”© ì¤‘: {file_path.name}\")\n",
    "    \n",
    "    with open(file_path, 'rb') as f:\n",
    "        documents = pickle.load(f)\n",
    "    \n",
    "    print(f\"- ë¬¸ì„œ ê°œìˆ˜: {len(documents)}\")\n",
    "    \n",
    "    # ì¹´í…Œê³ ë¦¬ ë¶„í¬ í™•ì¸\n",
    "    categories = {}\n",
    "    for doc in documents:\n",
    "        category = doc.metadata.get('category', 'unknown')\n",
    "        categories[category] = categories.get(category, 0) + 1\n",
    "    \n",
    "    print(\"- ì¹´í…Œê³ ë¦¬ ë¶„í¬:\")\n",
    "    for cat, count in sorted(categories.items()):\n",
    "        print(f\"  â€¢ {cat}: {count}\")\n",
    "    \n",
    "    # ì²« ë²ˆì§¸ ë¬¸ì„œ ìƒ˜í”Œ í™•ì¸\n",
    "    if documents:\n",
    "        first_doc = documents[0]\n",
    "        print(f\"\\n- ì²« ë²ˆì§¸ ë¬¸ì„œ ë©”íƒ€ë°ì´í„° í‚¤: {list(first_doc.metadata.keys())}\")\n",
    "        print(f\"- ì†ŒìŠ¤: {first_doc.metadata.get('source', 'N/A')}\")\n",
    "    \n",
    "    return documents\n",
    "\n",
    "# ë‘ íŒŒì¼ ë¡œë“œ\n",
    "gv80_docs = load_pickle_file(GV80_FILE)\n",
    "digital_gov_docs = load_pickle_file(DIGITAL_GOV_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. ë¬¸ì„œ ë³‘í•© ì „ ê²€ì¦\n",
    "\n",
    "ë³‘í•© ì „ì— í™•ì¸í•´ì•¼ í•  ì‚¬í•­:\n",
    "- ID ì¤‘ë³µ ì—¬ë¶€\n",
    "- ë©”íƒ€ë°ì´í„° êµ¬ì¡° ì¼ê´€ì„±\n",
    "- ì†ŒìŠ¤ ê²½ë¡œ ì¶©ëŒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== ë³‘í•© ì „ ê²€ì¦ ê²°ê³¼ ===\n",
      "ID ì¶©ëŒ ê°œìˆ˜: 116\n",
      "  ì¶©ëŒ ID: [1, 2, 3, 4, 5]...\n",
      "\n",
      "ë©”íƒ€ë°ì´í„° í‚¤ ì°¨ì´: ì—†ìŒ\n",
      "\n",
      "ê³ ìœ  ì†ŒìŠ¤ íŒŒì¼: 2ê°œ\n",
      "  - data/gv80_owners_manual_TEST6P.pdf\n",
      "  - data/ë””ì§€í„¸ì •ë¶€í˜ì‹ _ì¶”ì§„ê³„íš.pdf\n",
      "\n",
      "ì „ì²´ ì¹´í…Œê³ ë¦¬ ì¢…ë¥˜: 7ê°œ\n",
      "  caption, figure, header, heading1, list, paragraph, table\n"
     ]
    }
   ],
   "source": [
    "def validate_documents(docs1: List[Document], docs2: List[Document]) -> Dict[str, Any]:\n",
    "    \"\"\"ë‘ ë¬¸ì„œ ì„¸íŠ¸ì˜ í˜¸í™˜ì„±ì„ ê²€ì¦í•©ë‹ˆë‹¤.\"\"\"\n",
    "    validation_result = {\n",
    "        'id_conflicts': [],\n",
    "        'metadata_keys_diff': set(),\n",
    "        'source_files': set(),\n",
    "        'total_pages': set(),\n",
    "        'categories': set()\n",
    "    }\n",
    "    \n",
    "    # ID ìˆ˜ì§‘\n",
    "    ids1 = {doc.metadata.get('id') for doc in docs1 if doc.metadata.get('id')}\n",
    "    ids2 = {doc.metadata.get('id') for doc in docs2 if doc.metadata.get('id')}\n",
    "    \n",
    "    # ID ì¶©ëŒ í™•ì¸\n",
    "    validation_result['id_conflicts'] = list(ids1.intersection(ids2))\n",
    "    \n",
    "    # ë©”íƒ€ë°ì´í„° í‚¤ ë¹„êµ\n",
    "    keys1 = set(docs1[0].metadata.keys()) if docs1 else set()\n",
    "    keys2 = set(docs2[0].metadata.keys()) if docs2 else set()\n",
    "    validation_result['metadata_keys_diff'] = keys1.symmetric_difference(keys2)\n",
    "    \n",
    "    # ì†ŒìŠ¤ íŒŒì¼ ìˆ˜ì§‘\n",
    "    for doc in docs1 + docs2:\n",
    "        if 'source' in doc.metadata:\n",
    "            validation_result['source_files'].add(doc.metadata['source'])\n",
    "        if 'page' in doc.metadata:\n",
    "            validation_result['total_pages'].add(doc.metadata['page'])\n",
    "        if 'category' in doc.metadata:\n",
    "            validation_result['categories'].add(doc.metadata['category'])\n",
    "    \n",
    "    return validation_result\n",
    "\n",
    "# ê²€ì¦ ì‹¤í–‰\n",
    "validation = validate_documents(gv80_docs, digital_gov_docs)\n",
    "\n",
    "print(\"\\n=== ë³‘í•© ì „ ê²€ì¦ ê²°ê³¼ ===\")\n",
    "print(f\"ID ì¶©ëŒ ê°œìˆ˜: {len(validation['id_conflicts'])}\")\n",
    "if validation['id_conflicts']:\n",
    "    print(f\"  ì¶©ëŒ ID: {validation['id_conflicts'][:5]}...\")\n",
    "\n",
    "print(f\"\\në©”íƒ€ë°ì´í„° í‚¤ ì°¨ì´: {validation['metadata_keys_diff'] if validation['metadata_keys_diff'] else 'ì—†ìŒ'}\")\n",
    "print(f\"\\nê³ ìœ  ì†ŒìŠ¤ íŒŒì¼: {len(validation['source_files'])}ê°œ\")\n",
    "for source in validation['source_files']:\n",
    "    print(f\"  - {source}\")\n",
    "\n",
    "print(f\"\\nì „ì²´ ì¹´í…Œê³ ë¦¬ ì¢…ë¥˜: {len(validation['categories'])}ê°œ\")\n",
    "print(f\"  {', '.join(sorted(validation['categories']))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. ë¬¸ì„œ ë³‘í•© ë¡œì§\n",
    "\n",
    "ë³‘í•© ì‹œ ê³ ë ¤ì‚¬í•­:\n",
    "- ID ì¶©ëŒ ì‹œ ì†ŒìŠ¤ë³„ prefix ì¶”ê°€\n",
    "- ë©”íƒ€ë°ì´í„° ë¬´ê²°ì„± ìœ ì§€\n",
    "- ì›ë³¸ ìˆœì„œ ë³´ì¡´"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì˜µì…˜ 1: ì¹´í…Œê³ ë¦¬ë¥¼ í¬í•¨í•œ ID ìƒì„±\n",
      "ğŸ“ ID ì¬êµ¬ì„± ì¤‘...\n",
      "\n",
      "âœ… ID ì¬êµ¬ì„± ì™„ë£Œ!\n",
      "  - ì´ 163ê°œ ID ë§¤í•‘ ìƒì„±\n",
      "\n",
      "ğŸ“Š Prefixë³„ ë¬¸ì„œ ìˆ˜:\n",
      "  - gv80_owners_manual_test6p: 122ê°œ\n",
      "  - ë””ì§€í„¸ì •ë¶€í˜ì‹ _ì¶”ì§„ê³„íš: 158ê°œ\n",
      "\n",
      "âœ… ëª¨ë“  IDê°€ ê³ ìœ í•©ë‹ˆë‹¤.\n",
      "\n",
      "==================================================\n",
      "\n",
      "ì˜µì…˜ 2: ë‹¨ìˆœ ìˆœì°¨ ID ìƒì„±\n",
      "ğŸ“ ID ì¬êµ¬ì„± ì¤‘...\n",
      "\n",
      "âœ… ID ì¬êµ¬ì„± ì™„ë£Œ!\n",
      "  - ì´ 163ê°œ ID ë§¤í•‘ ìƒì„±\n",
      "\n",
      "ğŸ“Š Prefixë³„ ë¬¸ì„œ ìˆ˜:\n",
      "  - gv80_owners_manual_test6p: 122ê°œ\n",
      "  - ë””ì§€í„¸ì •ë¶€í˜ì‹ _ì¶”ì§„ê³„íš: 158ê°œ\n",
      "\n",
      "âœ… ëª¨ë“  IDê°€ ê³ ìœ í•©ë‹ˆë‹¤.\n",
      "\n",
      "ë³‘í•© ì™„ë£Œ!\n",
      "ì´ ë¬¸ì„œ ìˆ˜: 280\n",
      "ì˜ˆìƒ ë¬¸ì„œ ìˆ˜: 280\n",
      "ì¼ì¹˜ ì—¬ë¶€: True\n",
      "\n",
      "ğŸ“‹ ìƒˆ ID ìƒ˜í”Œ (ì²˜ìŒ 5ê°œ):\n",
      "  [gv80_owners_manual_test6p] 0 â†’ gv80_owners_manual_test6p_0001 (ì¹´í…Œê³ ë¦¬: heading1)\n",
      "  [gv80_owners_manual_test6p] 1 â†’ gv80_owners_manual_test6p_0002 (ì¹´í…Œê³ ë¦¬: heading1)\n",
      "  [gv80_owners_manual_test6p] 2 â†’ gv80_owners_manual_test6p_0003 (ì¹´í…Œê³ ë¦¬: figure)\n",
      "  [gv80_owners_manual_test6p] 3 â†’ gv80_owners_manual_test6p_0004 (ì¹´í…Œê³ ë¦¬: table)\n",
      "  [gv80_owners_manual_test6p] 4 â†’ gv80_owners_manual_test6p_0005 (ì¹´í…Œê³ ë¦¬: caption)\n"
     ]
    }
   ],
   "source": [
    "def merge_documents(\n",
    "    docs1: List[Document], \n",
    "    docs2: List[Document],\n",
    "    reorganize_ids: bool = True,\n",
    "    include_category_in_id: bool = False\n",
    ") -> tuple[List[Document], Dict[str, str]]:\n",
    "    \"\"\"ë‘ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ë¥¼ ë³‘í•©í•˜ê³  ìƒˆë¡œìš´ ID ì²´ê³„ë¥¼ ì ìš©í•©ë‹ˆë‹¤.\n",
    "    \n",
    "    Args:\n",
    "        docs1: ì²« ë²ˆì§¸ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸\n",
    "        docs2: ë‘ ë²ˆì§¸ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸\n",
    "        reorganize_ids: ëª¨ë“  ë¬¸ì„œì— ìƒˆ ID ë¶€ì—¬ ì—¬ë¶€\n",
    "        include_category_in_id: IDì— ì¹´í…Œê³ ë¦¬ í¬í•¨ ì—¬ë¶€\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (ë³‘í•©ëœ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸, ID ë§¤í•‘ ë”•ì…”ë„ˆë¦¬)\n",
    "    \"\"\"\n",
    "    import copy\n",
    "    import re\n",
    "    from collections import defaultdict\n",
    "    \n",
    "    merged_docs = []\n",
    "    id_mapping = {}  # old_id -> new_id ë§¤í•‘\n",
    "    \n",
    "    def extract_source_prefix(source_path: str) -> str:\n",
    "        \"\"\"ì†ŒìŠ¤ íŒŒì¼ëª…ì—ì„œ ì˜ë¯¸ìˆëŠ” prefixë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.\n",
    "        íŒŒì¼ëª…ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ë˜, í™•ì¥ìì™€ íŠ¹ìˆ˜ë¬¸ìëŠ” ì œê±°í•©ë‹ˆë‹¤.\n",
    "        \"\"\"\n",
    "        import os\n",
    "        \n",
    "        # íŒŒì¼ëª…ë§Œ ì¶”ì¶œ (ê²½ë¡œ ì œê±°)\n",
    "        filename = os.path.basename(source_path)\n",
    "        \n",
    "        # í™•ì¥ì ì œê±°\n",
    "        name_without_ext = os.path.splitext(filename)[0]\n",
    "        \n",
    "        # íŠ¹ìˆ˜ë¬¸ìë¥¼ ì–¸ë”ìŠ¤ì½”ì–´ë¡œ ë³€í™˜í•˜ê³  ì†Œë¬¸ìë¡œ ë³€í™˜\n",
    "        # í•œê¸€ì€ ìœ ì§€í•˜ë˜, ê³µë°±ê³¼ íŠ¹ìˆ˜ë¬¸ìëŠ” ì–¸ë”ìŠ¤ì½”ì–´ë¡œ ë³€í™˜\n",
    "        clean_name = re.sub(r'[^\\wê°€-í£]+', '_', name_without_ext)\n",
    "        clean_name = clean_name.strip('_')  # ì•ë’¤ ì–¸ë”ìŠ¤ì½”ì–´ ì œê±°\n",
    "        \n",
    "        # ë„ˆë¬´ ê¸¸ë©´ ì ì ˆíˆ ìë¦„ (ìµœëŒ€ 20ì)\n",
    "        # if len(clean_name) > 20:\n",
    "        #     clean_name = clean_name[:20]\n",
    "        \n",
    "        return clean_name.lower()\n",
    "    \n",
    "    def get_category_abbr(category: str) -> str:\n",
    "        \"\"\"ì¹´í…Œê³ ë¦¬ë¥¼ ì§§ì€ ì•½ì–´ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.\"\"\"\n",
    "        category_map = {\n",
    "            'heading1': 'h1',\n",
    "            'heading2': 'h2', \n",
    "            'heading3': 'h3',\n",
    "            'paragraph': 'para',\n",
    "            'list': 'list',\n",
    "            'table': 'tbl',\n",
    "            'figure': 'fig',\n",
    "            'chart': 'chrt',\n",
    "            'equation': 'eq',\n",
    "            'caption': 'cap',\n",
    "            'footnote': 'fn',\n",
    "            'header': 'hdr',\n",
    "            'footer': 'ftr',\n",
    "            'reference': 'ref'\n",
    "        }\n",
    "        return category_map.get(category, 'unk')\n",
    "    \n",
    "    def process_documents(docs: List[Document], default_prefix: str) -> List[Document]:\n",
    "        \"\"\"ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ë¥¼ ì²˜ë¦¬í•˜ê³  ìƒˆ IDë¥¼ ë¶€ì—¬í•©ë‹ˆë‹¤.\"\"\"\n",
    "        processed_docs = []\n",
    "        \n",
    "        # sourceë³„ë¡œ ë¬¸ì„œ ê·¸ë£¹í™”\n",
    "        docs_by_source = defaultdict(list)\n",
    "        for doc in docs:\n",
    "            source = doc.metadata.get('source', 'unknown')\n",
    "            docs_by_source[source].append(doc)\n",
    "        \n",
    "        # ê° sourceë³„ë¡œ ì²˜ë¦¬\n",
    "        for source, source_docs in docs_by_source.items():\n",
    "            prefix = extract_source_prefix(source) if source != 'unknown' else default_prefix\n",
    "            \n",
    "            # ì¹´í…Œê³ ë¦¬ë³„ë¡œ ì¹´ìš´í„° ê´€ë¦¬ (ì˜µì…˜)\n",
    "            if include_category_in_id:\n",
    "                category_counters = defaultdict(int)\n",
    "                \n",
    "                for doc in source_docs:\n",
    "                    new_doc = copy.deepcopy(doc)\n",
    "                    category = doc.metadata.get('category', 'unknown')\n",
    "                    cat_abbr = get_category_abbr(category)\n",
    "                    \n",
    "                    category_counters[cat_abbr] += 1\n",
    "                    new_id = f\"{prefix}_{cat_abbr}_{category_counters[cat_abbr]:03d}\"\n",
    "                    \n",
    "                    # ì›ë³¸ ID ë³´ì¡´\n",
    "                    if 'id' in new_doc.metadata:\n",
    "                        old_id = new_doc.metadata['id']\n",
    "                        new_doc.metadata['original_id'] = old_id\n",
    "                        id_mapping[old_id] = new_id\n",
    "                    \n",
    "                    new_doc.metadata['id'] = new_id\n",
    "                    new_doc.metadata['id_source_prefix'] = prefix\n",
    "                    processed_docs.append(new_doc)\n",
    "            else:\n",
    "                # ë‹¨ìˆœ ìˆœì°¨ ë²ˆí˜¸\n",
    "                for idx, doc in enumerate(source_docs, 1):\n",
    "                    new_doc = copy.deepcopy(doc)\n",
    "                    new_id = f\"{prefix}_{idx:04d}\"\n",
    "                    \n",
    "                    # ì›ë³¸ ID ë³´ì¡´\n",
    "                    if 'id' in new_doc.metadata:\n",
    "                        old_id = new_doc.metadata['id']\n",
    "                        new_doc.metadata['original_id'] = old_id\n",
    "                        id_mapping[old_id] = new_id\n",
    "                    \n",
    "                    new_doc.metadata['id'] = new_id\n",
    "                    new_doc.metadata['id_source_prefix'] = prefix\n",
    "                    processed_docs.append(new_doc)\n",
    "        \n",
    "        return processed_docs\n",
    "    \n",
    "    # ë‘ ë¬¸ì„œ ì„¸íŠ¸ ì²˜ë¦¬\n",
    "    print(\"ğŸ“ ID ì¬êµ¬ì„± ì¤‘...\")\n",
    "    processed_docs1 = process_documents(docs1, 'doc1')\n",
    "    processed_docs2 = process_documents(docs2, 'doc2')\n",
    "    \n",
    "    merged_docs = processed_docs1 + processed_docs2\n",
    "    \n",
    "    # ID ì¬êµ¬ì„± í†µê³„ ì¶œë ¥\n",
    "    print(f\"\\nâœ… ID ì¬êµ¬ì„± ì™„ë£Œ!\")\n",
    "    print(f\"  - ì´ {len(id_mapping)}ê°œ ID ë§¤í•‘ ìƒì„±\")\n",
    "    \n",
    "    # prefixë³„ í†µê³„\n",
    "    prefix_stats = defaultdict(int)\n",
    "    for doc in merged_docs:\n",
    "        prefix = doc.metadata.get('id_source_prefix', 'unknown')\n",
    "        prefix_stats[prefix] += 1\n",
    "    \n",
    "    print(\"\\nğŸ“Š Prefixë³„ ë¬¸ì„œ ìˆ˜:\")\n",
    "    for prefix, count in sorted(prefix_stats.items()):\n",
    "        print(f\"  - {prefix}: {count}ê°œ\")\n",
    "    \n",
    "    # ì¤‘ë³µ ID ì²´í¬\n",
    "    all_new_ids = [doc.metadata.get('id') for doc in merged_docs if 'id' in doc.metadata]\n",
    "    unique_ids = set(all_new_ids)\n",
    "    if len(all_new_ids) != len(unique_ids):\n",
    "        print(f\"\\nâš ï¸  ê²½ê³ : {len(all_new_ids) - len(unique_ids)}ê°œì˜ ì¤‘ë³µ ID ë°œê²¬!\")\n",
    "    else:\n",
    "        print(f\"\\nâœ… ëª¨ë“  IDê°€ ê³ ìœ í•©ë‹ˆë‹¤.\")\n",
    "    \n",
    "    return merged_docs, id_mapping\n",
    "\n",
    "# ë³‘í•© ì‹¤í–‰ (ì¹´í…Œê³ ë¦¬ í¬í•¨ ì˜µì…˜)\n",
    "print(\"ì˜µì…˜ 1: ì¹´í…Œê³ ë¦¬ë¥¼ í¬í•¨í•œ ID ìƒì„±\")\n",
    "merged_documents_with_cat, id_mapping_with_cat = merge_documents(\n",
    "    gv80_docs, \n",
    "    digital_gov_docs, \n",
    "    include_category_in_id=True\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"\\nì˜µì…˜ 2: ë‹¨ìˆœ ìˆœì°¨ ID ìƒì„±\")\n",
    "merged_documents, id_mapping = merge_documents(\n",
    "    gv80_docs, \n",
    "    digital_gov_docs, \n",
    "    include_category_in_id=False\n",
    ")\n",
    "\n",
    "print(f\"\\në³‘í•© ì™„ë£Œ!\")\n",
    "print(f\"ì´ ë¬¸ì„œ ìˆ˜: {len(merged_documents)}\")\n",
    "print(f\"ì˜ˆìƒ ë¬¸ì„œ ìˆ˜: {len(gv80_docs) + len(digital_gov_docs)}\")\n",
    "print(f\"ì¼ì¹˜ ì—¬ë¶€: {len(merged_documents) == len(gv80_docs) + len(digital_gov_docs)}\")\n",
    "\n",
    "# ìƒ˜í”Œ ID í™•ì¸\n",
    "print(\"\\nğŸ“‹ ìƒˆ ID ìƒ˜í”Œ (ì²˜ìŒ 5ê°œ):\")\n",
    "for doc in merged_documents[:5]:\n",
    "    old_id = doc.metadata.get('original_id', 'N/A')\n",
    "    new_id = doc.metadata.get('id', 'N/A')\n",
    "    category = doc.metadata.get('category', 'N/A')\n",
    "    source_prefix = doc.metadata.get('id_source_prefix', 'N/A')\n",
    "    print(f\"  [{source_prefix}] {old_id} â†’ {new_id} (ì¹´í…Œê³ ë¦¬: {category})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. ë³‘í•© ê²°ê³¼ ê²€ì¦"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== ë³‘í•© ê²°ê³¼ ë¶„ì„ ===\n",
      "\n",
      "ì†ŒìŠ¤ë³„ ë¬¸ì„œ ë¶„í¬:\n",
      "  data/gv80_owners_manual_TEST6P.pdf: 122 ë¬¸ì„œ\n",
      "  data/ë””ì§€í„¸ì •ë¶€í˜ì‹ _ì¶”ì§„ê³„íš.pdf: 158 ë¬¸ì„œ\n",
      "\n",
      "ì†ŒìŠ¤ë³„ ì¹´í…Œê³ ë¦¬ ë¶„í¬:\n",
      "\n",
      "  [data/gv80_owners_manual_TEST6P.pdf]\n",
      "    â€¢ caption: 4\n",
      "    â€¢ figure: 10\n",
      "    â€¢ header: 3\n",
      "    â€¢ heading1: 23\n",
      "    â€¢ list: 1\n",
      "    â€¢ paragraph: 78\n",
      "    â€¢ table: 3\n",
      "\n",
      "  [data/ë””ì§€í„¸ì •ë¶€í˜ì‹ _ì¶”ì§„ê³„íš.pdf]\n",
      "    â€¢ figure: 3\n",
      "    â€¢ header: 2\n",
      "    â€¢ heading1: 34\n",
      "    â€¢ list: 4\n",
      "    â€¢ paragraph: 104\n",
      "    â€¢ table: 11\n",
      "\n",
      "âœ… ëª¨ë“  ë¬¸ì„œì˜ ë©”íƒ€ë°ì´í„°ê°€ ì™„ì „í•©ë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "def analyze_merged_documents(documents: List[Document]) -> None:\n",
    "    \"\"\"ë³‘í•©ëœ ë¬¸ì„œì˜ í†µê³„ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤.\"\"\"\n",
    "    \n",
    "    # ì†ŒìŠ¤ë³„ ë¶„ë¥˜\n",
    "    sources = {}\n",
    "    for doc in documents:\n",
    "        source = doc.metadata.get('source', 'unknown')\n",
    "        if source not in sources:\n",
    "            sources[source] = []\n",
    "        sources[source].append(doc)\n",
    "    \n",
    "    print(\"\\n=== ë³‘í•© ê²°ê³¼ ë¶„ì„ ===\")\n",
    "    print(f\"\\nì†ŒìŠ¤ë³„ ë¬¸ì„œ ë¶„í¬:\")\n",
    "    for source, docs in sources.items():\n",
    "        print(f\"  {source}: {len(docs)} ë¬¸ì„œ\")\n",
    "    \n",
    "    # ì¹´í…Œê³ ë¦¬ë³„ í†µê³„\n",
    "    categories_by_source = {}\n",
    "    for doc in documents:\n",
    "        source = doc.metadata.get('source', 'unknown')\n",
    "        category = doc.metadata.get('category', 'unknown')\n",
    "        \n",
    "        if source not in categories_by_source:\n",
    "            categories_by_source[source] = {}\n",
    "        \n",
    "        if category not in categories_by_source[source]:\n",
    "            categories_by_source[source][category] = 0\n",
    "        \n",
    "        categories_by_source[source][category] += 1\n",
    "    \n",
    "    print(\"\\nì†ŒìŠ¤ë³„ ì¹´í…Œê³ ë¦¬ ë¶„í¬:\")\n",
    "    for source, categories in categories_by_source.items():\n",
    "        print(f\"\\n  [{source}]\")\n",
    "        for cat, count in sorted(categories.items()):\n",
    "            print(f\"    â€¢ {cat}: {count}\")\n",
    "    \n",
    "    # ë©”íƒ€ë°ì´í„° ì™„ì „ì„± ì²´í¬\n",
    "    required_fields = ['source', 'category', 'id', 'page']\n",
    "    incomplete_docs = []\n",
    "    \n",
    "    for i, doc in enumerate(documents):\n",
    "        missing = [field for field in required_fields if field not in doc.metadata]\n",
    "        if missing:\n",
    "            incomplete_docs.append((i, missing))\n",
    "    \n",
    "    if incomplete_docs:\n",
    "        print(f\"\\nâš ï¸  ë©”íƒ€ë°ì´í„° ë¶ˆì™„ì „ ë¬¸ì„œ: {len(incomplete_docs)}ê°œ\")\n",
    "        for idx, missing in incomplete_docs[:5]:\n",
    "            print(f\"    ë¬¸ì„œ {idx}: {missing} ëˆ„ë½\")\n",
    "    else:\n",
    "        print(f\"\\nâœ… ëª¨ë“  ë¬¸ì„œì˜ ë©”íƒ€ë°ì´í„°ê°€ ì™„ì „í•©ë‹ˆë‹¤.\")\n",
    "\n",
    "# ë¶„ì„ ì‹¤í–‰\n",
    "analyze_merged_documents(merged_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. ë³‘í•©ëœ ë¬¸ì„œ ì €ì¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… ì €ì¥ ì™„ë£Œ!\n",
      "  íŒŒì¼: data/merged_ddu_documents.pkl\n",
      "  í¬ê¸°: 0.84 MB\n",
      "  ë¬¸ì„œ ìˆ˜: 280\n",
      "  ë©”íƒ€ë°ì´í„°: data/merged_ddu_documents.metadata.json\n"
     ]
    }
   ],
   "source": [
    "def save_merged_documents(\n",
    "    documents: List[Document], \n",
    "    output_path: Path,\n",
    "    create_backup: bool = True\n",
    ") -> None:\n",
    "    \"\"\"ë³‘í•©ëœ ë¬¸ì„œë¥¼ pickle íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.\"\"\"\n",
    "    \n",
    "    # ë°±ì—… ìƒì„± (ê¸°ì¡´ íŒŒì¼ì´ ìˆëŠ” ê²½ìš°)\n",
    "    if create_backup and output_path.exists():\n",
    "        backup_path = output_path.with_suffix(f'.backup_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.pkl')\n",
    "        print(f\"ê¸°ì¡´ íŒŒì¼ ë°±ì—…: {backup_path}\")\n",
    "        output_path.rename(backup_path)\n",
    "    \n",
    "    # ì €ì¥\n",
    "    with open(output_path, 'wb') as f:\n",
    "        pickle.dump(documents, f)\n",
    "    \n",
    "    file_size = output_path.stat().st_size / (1024 * 1024)  # MBë¡œ ë³€í™˜\n",
    "    print(f\"\\nâœ… ì €ì¥ ì™„ë£Œ!\")\n",
    "    print(f\"  íŒŒì¼: {output_path}\")\n",
    "    print(f\"  í¬ê¸°: {file_size:.2f} MB\")\n",
    "    print(f\"  ë¬¸ì„œ ìˆ˜: {len(documents)}\")\n",
    "    \n",
    "    # ë©”íƒ€ë°ì´í„° íŒŒì¼ë„ ìƒì„± (JSON)\n",
    "    metadata_path = output_path.with_suffix('.metadata.json')\n",
    "    metadata = {\n",
    "        'created_at': datetime.now().isoformat(),\n",
    "        'total_documents': len(documents),\n",
    "        'sources': list({doc.metadata.get('source') for doc in documents if 'source' in doc.metadata}),\n",
    "        'categories': list({doc.metadata.get('category') for doc in documents if 'category' in doc.metadata}),\n",
    "        'merged_from': [\n",
    "            str(GV80_FILE),\n",
    "            str(DIGITAL_GOV_FILE)\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    with open(metadata_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(metadata, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    print(f\"  ë©”íƒ€ë°ì´í„°: {metadata_path}\")\n",
    "\n",
    "# ì €ì¥ ì‹¤í–‰\n",
    "save_merged_documents(merged_documents, MERGED_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ ID ë§¤í•‘ ì €ì¥: data/id_mapping.json\n",
      "   - ì´ 163ê°œ ë§¤í•‘\n",
      "   - ë””ì§€í„¸ì •ë¶€í˜ì‹ : 158ê°œ\n",
      "   - gv80: 5ê°œ\n",
      "\n",
      "======================================================================\n",
      "ğŸ” ID íŒ¨í„´ ìƒì„¸ ë¶„ì„\n",
      "======================================================================\n",
      "\n",
      "ğŸ“Œ ID ì¤‘ë³µ ì²´í¬:\n",
      "  âœ… ëª¨ë“  IDê°€ ê³ ìœ í•©ë‹ˆë‹¤.\n",
      "\n",
      "ğŸ“Š ì¹´í…Œê³ ë¦¬ë³„ ID ë¶„í¬:\n",
      "  ì¹´í…Œê³ ë¦¬            | ë¬¸ì„œìˆ˜      | ì‚¬ìš© Prefix                      | ìƒ˜í”Œ ID\n",
      "  ------------------------------------------------------------------------------------------\n",
      "  paragraph       | 182      | ë””ì§€í„¸ì •ë¶€í˜ì‹ _ì¶”ì§„ê³„íš, gv80_owners_manual_test6p | gv80_owners_manual_test6p_0006\n",
      "  heading1        | 57       | ë””ì§€í„¸ì •ë¶€í˜ì‹ _ì¶”ì§„ê³„íš, gv80_owners_manual_test6p | gv80_owners_manual_test6p_0001\n",
      "  table           | 14       | ë””ì§€í„¸ì •ë¶€í˜ì‹ _ì¶”ì§„ê³„íš, gv80_owners_manual_test6p | gv80_owners_manual_test6p_0004\n",
      "  figure          | 13       | ë””ì§€í„¸ì •ë¶€í˜ì‹ _ì¶”ì§„ê³„íš, gv80_owners_manual_test6p | gv80_owners_manual_test6p_0003\n",
      "  header          | 5        | ë””ì§€í„¸ì •ë¶€í˜ì‹ _ì¶”ì§„ê³„íš, gv80_owners_manual_test6p | gv80_owners_manual_test6p_0044\n",
      "  list            | 5        | ë””ì§€í„¸ì •ë¶€í˜ì‹ _ì¶”ì§„ê³„íš, gv80_owners_manual_test6p | gv80_owners_manual_test6p_0095\n",
      "  caption         | 4        | gv80_owners_manual_test6p      | gv80_owners_manual_test6p_0005\n",
      "\n",
      "ğŸ“„ í˜ì´ì§€ë³„ ID ìˆœì„œ ë¶„ì„ (ìƒ˜í”Œ):\n",
      "  data/gv80_owners_manual_TEST6P.pdf:p5: 36ê°œ ë¬¸ì„œ (ìˆœì°¨ì )\n",
      "    ìƒ˜í”Œ: gv80_owners_manual_test6p_0056, gv80_owners_manual_test6p_0057, gv80_owners_manual_test6p_0058\n",
      "  data/gv80_owners_manual_TEST6P.pdf:p6: 31ê°œ ë¬¸ì„œ (ìˆœì°¨ì )\n",
      "    ìƒ˜í”Œ: gv80_owners_manual_test6p_0092, gv80_owners_manual_test6p_0093, gv80_owners_manual_test6p_0094\n",
      "  data/gv80_owners_manual_TEST6P.pdf:p1: 30ê°œ ë¬¸ì„œ (ìˆœì°¨ì )\n",
      "    ìƒ˜í”Œ: gv80_owners_manual_test6p_0001, gv80_owners_manual_test6p_0002, gv80_owners_manual_test6p_0003\n",
      "  data/ë””ì§€í„¸ì •ë¶€í˜ì‹ _ì¶”ì§„ê³„íš.pdf:p6: 26ê°œ ë¬¸ì„œ (ìˆœì°¨ì )\n",
      "    ìƒ˜í”Œ: ë””ì§€í„¸ì •ë¶€í˜ì‹ _ì¶”ì§„ê³„íš_0065, ë””ì§€í„¸ì •ë¶€í˜ì‹ _ì¶”ì§„ê³„íš_0066, ë””ì§€í„¸ì •ë¶€í˜ì‹ _ì¶”ì§„ê³„íš_0067\n",
      "  data/ë””ì§€í„¸ì •ë¶€í˜ì‹ _ì¶”ì§„ê³„íš.pdf:p10: 22ê°œ ë¬¸ì„œ (ìˆœì°¨ì )\n",
      "    ìƒ˜í”Œ: ë””ì§€í„¸ì •ë¶€í˜ì‹ _ì¶”ì§„ê³„íš_0137, ë””ì§€í„¸ì •ë¶€í˜ì‹ _ì¶”ì§„ê³„íš_0138, ë””ì§€í„¸ì •ë¶€í˜ì‹ _ì¶”ì§„ê³„íš_0139\n",
      "\n",
      "ğŸ·ï¸  Prefix íŒ¨í„´ ë¶„ì„:\n",
      "\n",
      "  [ë””ì§€í„¸ì •ë¶€í˜ì‹ _ì¶”ì§„ê³„íš] - 158ê°œ ë¬¸ì„œ\n",
      "    ì¹´í…Œê³ ë¦¬: table, header, figure, list, heading1\n",
      "    ID ìƒ˜í”Œ: ë””ì§€í„¸ì •ë¶€í˜ì‹ _ì¶”ì§„ê³„íš_0005, ë””ì§€í„¸ì •ë¶€í˜ì‹ _ì¶”ì§„ê³„íš_0004, ë””ì§€í„¸ì •ë¶€í˜ì‹ _ì¶”ì§„ê³„íš_0002\n",
      "\n",
      "  [gv80_owners_manual_test6p] - 122ê°œ ë¬¸ì„œ\n",
      "    ì¹´í…Œê³ ë¦¬: table, header, figure, list, caption\n",
      "    ID ìƒ˜í”Œ: gv80_owners_manual_test6p_0002, gv80_owners_manual_test6p_0001, gv80_owners_manual_test6p_0003\n",
      "\n",
      "ğŸ“ ID ê¸¸ì´ í†µê³„:\n",
      "  ìµœì†Œ: 17ì\n",
      "  ìµœëŒ€: 30ì\n",
      "  í‰ê· : 22.7ì\n",
      "  ê°€ì¥ í”í•œ ê¸¸ì´: 17ì\n",
      "\n",
      "ğŸ”„ ID ë³€í™˜ ì˜ˆì‹œ (10ê°œ):\n",
      "  Original ID                                   â†’ New ID                   \n",
      "  ---------------------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'int' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 228\u001b[39m\n\u001b[32m    226\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m  \u001b[39m\u001b[33m\"\u001b[39m + \u001b[33m\"\u001b[39m\u001b[33m-\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m75\u001b[39m)\n\u001b[32m    227\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, (old_id, new_id) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mlist\u001b[39m(id_mapping.items())[:\u001b[32m10\u001b[39m]):\n\u001b[32m--> \u001b[39m\u001b[32m228\u001b[39m     truncated_old = old_id[:\u001b[32m45\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mold_id\u001b[49m\u001b[43m)\u001b[49m > \u001b[32m45\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m old_id\n\u001b[32m    229\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtruncated_old\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m<45\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m â†’ \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnew_id\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m<25\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    231\u001b[39m \u001b[38;5;66;03m# ë¶„ì„ ê²°ê³¼ë¥¼ JSONìœ¼ë¡œë„ ì €ì¥\u001b[39;00m\n",
      "\u001b[31mTypeError\u001b[39m: object of type 'int' has no len()"
     ]
    }
   ],
   "source": [
    "def save_id_mapping(id_mapping: Dict[str, str], output_path: Path = None) -> None:\n",
    "    \"\"\"ID ë§¤í•‘ì„ JSON íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.\"\"\"\n",
    "    if output_path is None:\n",
    "        output_path = DATA_DIR / \"id_mapping.json\"\n",
    "    \n",
    "    # ID ë§¤í•‘ ì •ë³´ í™•ì¥\n",
    "    mapping_info = {\n",
    "        'created_at': datetime.now().isoformat(),\n",
    "        'total_mappings': len(id_mapping),\n",
    "        'mapping': id_mapping,\n",
    "        'statistics': {}\n",
    "    }\n",
    "    \n",
    "    # prefixë³„ í†µê³„ ê³„ì‚°\n",
    "    prefix_counts = {}\n",
    "    for new_id in id_mapping.values():\n",
    "        prefix = new_id.split('_')[0]\n",
    "        prefix_counts[prefix] = prefix_counts.get(prefix, 0) + 1\n",
    "    \n",
    "    mapping_info['statistics']['by_prefix'] = prefix_counts\n",
    "    \n",
    "    # JSONìœ¼ë¡œ ì €ì¥\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(mapping_info, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    print(f\"ğŸ“ ID ë§¤í•‘ ì €ì¥: {output_path}\")\n",
    "    print(f\"   - ì´ {len(id_mapping)}ê°œ ë§¤í•‘\")\n",
    "    for prefix, count in prefix_counts.items():\n",
    "        print(f\"   - {prefix}: {count}ê°œ\")\n",
    "\n",
    "# ID ë§¤í•‘ ì €ì¥\n",
    "save_id_mapping(id_mapping)\n",
    "\n",
    "# ========================================================================\n",
    "# ID íŒ¨í„´ ë¶„ì„ ë¡œì§ - ì™„ì „ êµ¬í˜„\n",
    "# ========================================================================\n",
    "\n",
    "def analyze_id_patterns(documents: List[Document], id_mapping: Dict[str, str]) -> Dict[str, Any]:\n",
    "    \"\"\"ID íŒ¨í„´ì„ ìƒì„¸í•˜ê²Œ ë¶„ì„í•©ë‹ˆë‹¤.\"\"\"\n",
    "    analysis_result = {\n",
    "        'total_documents': len(documents),\n",
    "        'id_duplicates': [],\n",
    "        'category_distribution': {},\n",
    "        'page_distribution': {},\n",
    "        'prefix_patterns': {},\n",
    "        'consistency_check': {},\n",
    "        'id_length_stats': {}\n",
    "    }\n",
    "    \n",
    "    # 1. ID ì¤‘ë³µ ì²´í¬\n",
    "    all_ids = []\n",
    "    id_counter = {}\n",
    "    for doc in documents:\n",
    "        doc_id = doc.metadata.get('id')\n",
    "        if doc_id:\n",
    "            all_ids.append(doc_id)\n",
    "            id_counter[doc_id] = id_counter.get(doc_id, 0) + 1\n",
    "    \n",
    "    # ì¤‘ë³µ ID ì°¾ê¸°\n",
    "    duplicates = {id: count for id, count in id_counter.items() if count > 1}\n",
    "    analysis_result['id_duplicates'] = duplicates\n",
    "    \n",
    "    # 2. ì¹´í…Œê³ ë¦¬ë³„ ID ë¶„í¬\n",
    "    category_ids = {}\n",
    "    for doc in documents:\n",
    "        category = doc.metadata.get('category', 'unknown')\n",
    "        doc_id = doc.metadata.get('id', 'no_id')\n",
    "        prefix = doc.metadata.get('id_source_prefix', 'unknown')\n",
    "        \n",
    "        if category not in category_ids:\n",
    "            category_ids[category] = {'count': 0, 'prefixes': set(), 'sample_ids': []}\n",
    "        \n",
    "        category_ids[category]['count'] += 1\n",
    "        category_ids[category]['prefixes'].add(prefix)\n",
    "        if len(category_ids[category]['sample_ids']) < 3:\n",
    "            category_ids[category]['sample_ids'].append(doc_id)\n",
    "    \n",
    "    # Setì„ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜\n",
    "    for cat in category_ids:\n",
    "        category_ids[cat]['prefixes'] = list(category_ids[cat]['prefixes'])\n",
    "    \n",
    "    analysis_result['category_distribution'] = category_ids\n",
    "    \n",
    "    # 3. í˜ì´ì§€ë³„ ID ìˆœì„œ ë¶„ì„\n",
    "    page_ids = {}\n",
    "    for doc in documents:\n",
    "        page = doc.metadata.get('page', 'unknown')\n",
    "        doc_id = doc.metadata.get('id', 'no_id')\n",
    "        source = doc.metadata.get('source', 'unknown')\n",
    "        \n",
    "        page_key = f\"{source}:p{page}\"\n",
    "        if page_key not in page_ids:\n",
    "            page_ids[page_key] = []\n",
    "        page_ids[page_key].append(doc_id)\n",
    "    \n",
    "    # ê° í˜ì´ì§€ì˜ ID ìˆ˜ì™€ ìˆœì„œ ì •ë³´ ì €ì¥\n",
    "    for page_key in page_ids:\n",
    "        page_ids[page_key] = {\n",
    "            'count': len(page_ids[page_key]),\n",
    "            'ids': page_ids[page_key][:5],  # ì²˜ìŒ 5ê°œë§Œ ìƒ˜í”Œë¡œ\n",
    "            'is_sequential': check_sequential(page_ids[page_key])\n",
    "        }\n",
    "    \n",
    "    analysis_result['page_distribution'] = page_ids\n",
    "    \n",
    "    # 4. Prefix íŒ¨í„´ ë¶„ì„\n",
    "    prefix_patterns = {}\n",
    "    for doc in documents:\n",
    "        prefix = doc.metadata.get('id_source_prefix', 'unknown')\n",
    "        doc_id = doc.metadata.get('id', 'no_id')\n",
    "        \n",
    "        if prefix not in prefix_patterns:\n",
    "            prefix_patterns[prefix] = {\n",
    "                'count': 0,\n",
    "                'id_format_samples': set(),\n",
    "                'categories': set()\n",
    "            }\n",
    "        \n",
    "        prefix_patterns[prefix]['count'] += 1\n",
    "        prefix_patterns[prefix]['categories'].add(doc.metadata.get('category', 'unknown'))\n",
    "        \n",
    "        # ID í˜•ì‹ ìƒ˜í”Œ ì €ì¥ (ì²˜ìŒ 5ê°œë§Œ)\n",
    "        if len(prefix_patterns[prefix]['id_format_samples']) < 5:\n",
    "            prefix_patterns[prefix]['id_format_samples'].add(doc_id)\n",
    "    \n",
    "    # Setì„ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜\n",
    "    for prefix in prefix_patterns:\n",
    "        prefix_patterns[prefix]['id_format_samples'] = list(prefix_patterns[prefix]['id_format_samples'])\n",
    "        prefix_patterns[prefix]['categories'] = list(prefix_patterns[prefix]['categories'])\n",
    "    \n",
    "    analysis_result['prefix_patterns'] = prefix_patterns\n",
    "    \n",
    "    # 5. ID ê¸¸ì´ í†µê³„\n",
    "    id_lengths = [len(doc_id) for doc_id in all_ids if doc_id]\n",
    "    if id_lengths:\n",
    "        analysis_result['id_length_stats'] = {\n",
    "            'min': min(id_lengths),\n",
    "            'max': max(id_lengths),\n",
    "            'avg': sum(id_lengths) / len(id_lengths),\n",
    "            'most_common': max(set(id_lengths), key=id_lengths.count)\n",
    "        }\n",
    "    \n",
    "    return analysis_result\n",
    "\n",
    "def check_sequential(ids: List[str]) -> bool:\n",
    "    \"\"\"ID ë¦¬ìŠ¤íŠ¸ê°€ ìˆœì°¨ì ì¸ì§€ í™•ì¸í•©ë‹ˆë‹¤.\"\"\"\n",
    "    try:\n",
    "        # IDì—ì„œ ìˆ«ì ë¶€ë¶„ë§Œ ì¶”ì¶œ\n",
    "        numbers = []\n",
    "        for id in ids:\n",
    "            # ë§ˆì§€ë§‰ ì–¸ë”ìŠ¤ì½”ì–´ ì´í›„ì˜ ìˆ«ì ì¶”ì¶œ\n",
    "            parts = id.split('_')\n",
    "            if parts:\n",
    "                last_part = parts[-1]\n",
    "                if last_part.isdigit():\n",
    "                    numbers.append(int(last_part))\n",
    "        \n",
    "        # ìˆœì°¨ì ì¸ì§€ í™•ì¸\n",
    "        if numbers:\n",
    "            sorted_nums = sorted(numbers)\n",
    "            for i in range(1, len(sorted_nums)):\n",
    "                if sorted_nums[i] - sorted_nums[i-1] != 1:\n",
    "                    return False\n",
    "            return True\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    return False\n",
    "\n",
    "# ID íŒ¨í„´ ë¶„ì„ ì‹¤í–‰\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ğŸ” ID íŒ¨í„´ ìƒì„¸ ë¶„ì„\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "analysis = analyze_id_patterns(merged_documents, id_mapping)\n",
    "\n",
    "# 1. ì¤‘ë³µ ID ì²´í¬\n",
    "print(\"\\nğŸ“Œ ID ì¤‘ë³µ ì²´í¬:\")\n",
    "if analysis['id_duplicates']:\n",
    "    print(f\"  âš ï¸  {len(analysis['id_duplicates'])}ê°œì˜ ì¤‘ë³µ ID ë°œê²¬!\")\n",
    "    for dup_id, count in list(analysis['id_duplicates'].items())[:5]:\n",
    "        print(f\"    - {dup_id}: {count}íšŒ ì¤‘ë³µ\")\n",
    "else:\n",
    "    print(\"  âœ… ëª¨ë“  IDê°€ ê³ ìœ í•©ë‹ˆë‹¤.\")\n",
    "\n",
    "# 2. ì¹´í…Œê³ ë¦¬ë³„ ë¶„í¬\n",
    "print(\"\\nğŸ“Š ì¹´í…Œê³ ë¦¬ë³„ ID ë¶„í¬:\")\n",
    "print(f\"  {'ì¹´í…Œê³ ë¦¬':<15} | {'ë¬¸ì„œìˆ˜':<8} | {'ì‚¬ìš© Prefix':<30} | {'ìƒ˜í”Œ ID'}\")\n",
    "print(\"  \" + \"-\"*90)\n",
    "for category, info in sorted(analysis['category_distribution'].items(), \n",
    "                            key=lambda x: x[1]['count'], reverse=True):\n",
    "    prefixes = ', '.join(info['prefixes'][:3])\n",
    "    sample_id = info['sample_ids'][0] if info['sample_ids'] else 'N/A'\n",
    "    print(f\"  {category:<15} | {info['count']:<8} | {prefixes:<30} | {sample_id}\")\n",
    "\n",
    "# 3. í˜ì´ì§€ë³„ ë¶„ì„ (ìƒìœ„ 5ê°œë§Œ)\n",
    "print(\"\\nğŸ“„ í˜ì´ì§€ë³„ ID ìˆœì„œ ë¶„ì„ (ìƒ˜í”Œ):\")\n",
    "page_items = sorted(analysis['page_distribution'].items(), \n",
    "                   key=lambda x: x[1]['count'], reverse=True)[:5]\n",
    "for page_key, info in page_items:\n",
    "    seq_status = \"ìˆœì°¨ì \" if info['is_sequential'] else \"ë¹„ìˆœì°¨ì \"\n",
    "    print(f\"  {page_key}: {info['count']}ê°œ ë¬¸ì„œ ({seq_status})\")\n",
    "    if info['ids']:\n",
    "        print(f\"    ìƒ˜í”Œ: {', '.join(info['ids'][:3])}\")\n",
    "\n",
    "# 4. Prefix íŒ¨í„´ ë¶„ì„\n",
    "print(\"\\nğŸ·ï¸  Prefix íŒ¨í„´ ë¶„ì„:\")\n",
    "for prefix, pattern in sorted(analysis['prefix_patterns'].items(), \n",
    "                              key=lambda x: x[1]['count'], reverse=True):\n",
    "    print(f\"\\n  [{prefix}] - {pattern['count']}ê°œ ë¬¸ì„œ\")\n",
    "    print(f\"    ì¹´í…Œê³ ë¦¬: {', '.join(pattern['categories'][:5])}\")\n",
    "    print(f\"    ID ìƒ˜í”Œ: {', '.join(pattern['id_format_samples'][:3])}\")\n",
    "\n",
    "# 5. ID ê¸¸ì´ í†µê³„\n",
    "if analysis['id_length_stats']:\n",
    "    stats = analysis['id_length_stats']\n",
    "    print(\"\\nğŸ“ ID ê¸¸ì´ í†µê³„:\")\n",
    "    print(f\"  ìµœì†Œ: {stats['min']}ì\")\n",
    "    print(f\"  ìµœëŒ€: {stats['max']}ì\")\n",
    "    print(f\"  í‰ê· : {stats['avg']:.1f}ì\")\n",
    "    print(f\"  ê°€ì¥ í”í•œ ê¸¸ì´: {stats['most_common']}ì\")\n",
    "\n",
    "# 6. ID ë§¤í•‘ ë³€í™˜ ì˜ˆì‹œ\n",
    "print(\"\\nğŸ”„ ID ë³€í™˜ ì˜ˆì‹œ (10ê°œ):\")\n",
    "print(f\"  {'Original ID':<45} â†’ {'New ID':<25}\")\n",
    "print(\"  \" + \"-\"*75)\n",
    "for i, (old_id, new_id) in enumerate(list(id_mapping.items())[:10]):\n",
    "    truncated_old = old_id[:45] if len(old_id) > 45 else old_id\n",
    "    print(f\"  {truncated_old:<45} â†’ {new_id:<25}\")\n",
    "\n",
    "# ë¶„ì„ ê²°ê³¼ë¥¼ JSONìœ¼ë¡œë„ ì €ì¥\n",
    "analysis_path = DATA_DIR / \"id_analysis_report.json\"\n",
    "with open(analysis_path, 'w', encoding='utf-8') as f:\n",
    "    # numpy/datetime ê°ì²´ë¥¼ JSON ì§ë ¬í™” ê°€ëŠ¥í•˜ë„ë¡ ë³€í™˜\n",
    "    json_safe_analysis = json.loads(json.dumps(analysis, default=str))\n",
    "    json.dump(json_safe_analysis, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"\\nğŸ“Š ë¶„ì„ ë³´ê³ ì„œ ì €ì¥: {analysis_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def save_merged_documents(\n",
    "    documents: List[Document], \n",
    "    output_path: Path,\n",
    "    id_mapping: Dict[str, str] = None,\n",
    "    create_backup: bool = True\n",
    ") -> None:\n",
    "    \"\"\"ë³‘í•©ëœ ë¬¸ì„œë¥¼ pickle íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.\"\"\"\n",
    "    \n",
    "    # ë°±ì—… ìƒì„± (ê¸°ì¡´ íŒŒì¼ì´ ìˆëŠ” ê²½ìš°)\n",
    "    if create_backup and output_path.exists():\n",
    "        backup_path = output_path.with_suffix(f'.backup_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.pkl')\n",
    "        print(f\"ê¸°ì¡´ íŒŒì¼ ë°±ì—…: {backup_path}\")\n",
    "        output_path.rename(backup_path)\n",
    "    \n",
    "    # ì €ì¥\n",
    "    with open(output_path, 'wb') as f:\n",
    "        pickle.dump(documents, f)\n",
    "    \n",
    "    file_size = output_path.stat().st_size / (1024 * 1024)  # MBë¡œ ë³€í™˜\n",
    "    print(f\"\\nâœ… ì €ì¥ ì™„ë£Œ!\")\n",
    "    print(f\"  íŒŒì¼: {output_path}\")\n",
    "    print(f\"  í¬ê¸°: {file_size:.2f} MB\")\n",
    "    print(f\"  ë¬¸ì„œ ìˆ˜: {len(documents)}\")\n",
    "    \n",
    "    # ë©”íƒ€ë°ì´í„° íŒŒì¼ë„ ìƒì„± (JSON)\n",
    "    metadata_path = output_path.with_suffix('.metadata.json')\n",
    "    \n",
    "    # ID ì²´ê³„ ì •ë³´ ìˆ˜ì§‘\n",
    "    id_prefixes = set()\n",
    "    categories_used = set()\n",
    "    sources_used = set()\n",
    "    \n",
    "    for doc in documents:\n",
    "        if 'id_source_prefix' in doc.metadata:\n",
    "            id_prefixes.add(doc.metadata['id_source_prefix'])\n",
    "        if 'category' in doc.metadata:\n",
    "            categories_used.add(doc.metadata['category'])\n",
    "        if 'source' in doc.metadata:\n",
    "            sources_used.add(doc.metadata['source'])\n",
    "    \n",
    "    metadata = {\n",
    "        'created_at': datetime.now().isoformat(),\n",
    "        'total_documents': len(documents),\n",
    "        'sources': list(sources_used),\n",
    "        'categories': list(categories_used),\n",
    "        'id_prefixes': list(id_prefixes),\n",
    "        'id_reorganized': id_mapping is not None,\n",
    "        'total_id_mappings': len(id_mapping) if id_mapping else 0,\n",
    "        'merged_from': [\n",
    "            str(GV80_FILE),\n",
    "            str(DIGITAL_GOV_FILE)\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    with open(metadata_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(metadata, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    print(f\"  ë©”íƒ€ë°ì´í„°: {metadata_path}\")\n",
    "    \n",
    "    # ID ë§¤í•‘ë„ ì €ì¥ (ìˆëŠ” ê²½ìš°)\n",
    "    if id_mapping:\n",
    "        mapping_path = output_path.with_suffix('.id_mapping.json')\n",
    "        with open(mapping_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump({\n",
    "                'created_at': datetime.now().isoformat(),\n",
    "                'total_mappings': len(id_mapping),\n",
    "                'mapping': id_mapping\n",
    "            }, f, ensure_ascii=False, indent=2)\n",
    "        print(f\"  ID ë§¤í•‘: {mapping_path}\")\n",
    "\n",
    "# ì €ì¥ ì‹¤í–‰\n",
    "save_merged_documents(merged_documents, MERGED_FILE, id_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. ê²€ì¦: ì €ì¥ëœ íŒŒì¼ ë‹¤ì‹œ ë¡œë“œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì €ì¥ëœ íŒŒì¼ ê²€ì¦ ì¤‘...\n",
      "\n",
      "âœ… ê²€ì¦ ê²°ê³¼:\n",
      "  ë¡œë“œëœ ë¬¸ì„œ ìˆ˜: 280\n",
      "  ì›ë³¸ ë¬¸ì„œ ìˆ˜: 280\n",
      "  ì¼ì¹˜: True\n",
      "\n",
      "ğŸ”’ ë°ì´í„° ë¬´ê²°ì„± ê²€ì¦:\n",
      "  ë¬¸ì„œ ìˆ˜ ì¼ì¹˜: âœ…\n",
      "  ID ì¼ì¹˜: âœ…\n",
      "  ë©”íƒ€ë°ì´í„° ì¼ì¹˜: âœ…\n",
      "  ì»¨í…ì¸  ì¼ì¹˜: âœ…\n",
      "\n",
      "  âœ¨ ì™„ë²½í•œ ë°ì´í„° ë¬´ê²°ì„±!\n",
      "\n",
      "ğŸ“‹ ìƒ˜í”Œ ë¬¸ì„œ ìƒì„¸ ì •ë³´:\n",
      "\n",
      "  ë¬¸ì„œ #1:\n",
      "    ID: gv80_owners_manual_test6p_0001\n",
      "    ì›ë³¸ ID: 0\n",
      "    Prefix: gv80_owners_manual_test6p\n",
      "    ì¹´í…Œê³ ë¦¬: heading1\n",
      "    ì†ŒìŠ¤: data/gv80_owners_manual_TEST6P.pdf\n",
      "    í˜ì´ì§€: 1\n",
      "    ë‚´ìš© ê¸¸ì´: 17 ê¸€ì\n",
      "    ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°: # ë‚´ ìš© ì°¾ ê¸° ë°© ë²• ì„¤ ëª…...\n",
      "\n",
      "  ë¬¸ì„œ #2:\n",
      "    ID: gv80_owners_manual_test6p_0002\n",
      "    ì›ë³¸ ID: 1\n",
      "    Prefix: gv80_owners_manual_test6p\n",
      "    ì¹´í…Œê³ ë¦¬: heading1\n",
      "    ì†ŒìŠ¤: data/gv80_owners_manual_TEST6P.pdf\n",
      "    í˜ì´ì§€: 1\n",
      "    ë‚´ìš© ê¸¸ì´: 11 ê¸€ì\n",
      "    ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°: # ë‚´ìš©ìœ¼ë¡œ ì°¾ì„ ë•Œ...\n",
      "\n",
      "  ë¬¸ì„œ #3:\n",
      "    ID: gv80_owners_manual_test6p_0003\n",
      "    ì›ë³¸ ID: 2\n",
      "    Prefix: gv80_owners_manual_test6p\n",
      "    ì¹´í…Œê³ ë¦¬: figure\n",
      "    ì†ŒìŠ¤: data/gv80_owners_manual_TEST6P.pdf\n",
      "    í˜ì´ì§€: 1\n",
      "    ë‚´ìš© ê¸¸ì´: 1 ê¸€ì\n",
      "    ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°: \n",
      "...\n"
     ]
    }
   ],
   "source": [
    "# ì €ì¥ëœ íŒŒì¼ì„ ë‹¤ì‹œ ë¡œë“œí•˜ì—¬ ë¬´ê²°ì„± í™•ì¸\n",
    "print(\"ì €ì¥ëœ íŒŒì¼ ê²€ì¦ ì¤‘...\")\n",
    "\n",
    "with open(MERGED_FILE, 'rb') as f:\n",
    "    loaded_docs = pickle.load(f)\n",
    "\n",
    "print(f\"\\nâœ… ê²€ì¦ ê²°ê³¼:\")\n",
    "print(f\"  ë¡œë“œëœ ë¬¸ì„œ ìˆ˜: {len(loaded_docs)}\")\n",
    "print(f\"  ì›ë³¸ ë¬¸ì„œ ìˆ˜: {len(merged_documents)}\")\n",
    "print(f\"  ì¼ì¹˜: {len(loaded_docs) == len(merged_documents)}\")\n",
    "\n",
    "# ë°ì´í„° ë¬´ê²°ì„± ìƒì„¸ ê²€ì¦\n",
    "def verify_data_integrity(original_docs: List[Document], loaded_docs: List[Document]) -> Dict[str, Any]:\n",
    "    \"\"\"ì €ì¥/ë¡œë“œ í›„ ë°ì´í„° ë¬´ê²°ì„±ì„ ê²€ì¦í•©ë‹ˆë‹¤.\"\"\"\n",
    "    integrity_report = {\n",
    "        'document_count_match': len(original_docs) == len(loaded_docs),\n",
    "        'id_match': True,\n",
    "        'metadata_match': True,\n",
    "        'content_match': True,\n",
    "        'errors': []\n",
    "    }\n",
    "    \n",
    "    # ë¬¸ì„œë³„ ê²€ì¦\n",
    "    for i, (orig, loaded) in enumerate(zip(original_docs, loaded_docs)):\n",
    "        # ID ê²€ì¦\n",
    "        if orig.metadata.get('id') != loaded.metadata.get('id'):\n",
    "            integrity_report['id_match'] = False\n",
    "            integrity_report['errors'].append(f\"Document {i}: ID mismatch\")\n",
    "        \n",
    "        # ë©”íƒ€ë°ì´í„° í‚¤ ê²€ì¦\n",
    "        if set(orig.metadata.keys()) != set(loaded.metadata.keys()):\n",
    "            integrity_report['metadata_match'] = False\n",
    "            integrity_report['errors'].append(f\"Document {i}: Metadata keys mismatch\")\n",
    "        \n",
    "        # ì»¨í…ì¸  ê²€ì¦\n",
    "        if orig.page_content != loaded.page_content:\n",
    "            integrity_report['content_match'] = False\n",
    "            integrity_report['errors'].append(f\"Document {i}: Content mismatch\")\n",
    "    \n",
    "    return integrity_report\n",
    "\n",
    "# ë¬´ê²°ì„± ê²€ì¦ ì‹¤í–‰\n",
    "integrity = verify_data_integrity(merged_documents, loaded_docs)\n",
    "\n",
    "print(\"\\nğŸ”’ ë°ì´í„° ë¬´ê²°ì„± ê²€ì¦:\")\n",
    "print(f\"  ë¬¸ì„œ ìˆ˜ ì¼ì¹˜: {'âœ…' if integrity['document_count_match'] else 'âŒ'}\")\n",
    "print(f\"  ID ì¼ì¹˜: {'âœ…' if integrity['id_match'] else 'âŒ'}\")\n",
    "print(f\"  ë©”íƒ€ë°ì´í„° ì¼ì¹˜: {'âœ…' if integrity['metadata_match'] else 'âŒ'}\")\n",
    "print(f\"  ì»¨í…ì¸  ì¼ì¹˜: {'âœ…' if integrity['content_match'] else 'âŒ'}\")\n",
    "\n",
    "if integrity['errors']:\n",
    "    print(f\"\\n  âš ï¸  ë°œê²¬ëœ ì˜¤ë¥˜:\")\n",
    "    for error in integrity['errors'][:5]:\n",
    "        print(f\"    - {error}\")\n",
    "else:\n",
    "    print(\"\\n  âœ¨ ì™„ë²½í•œ ë°ì´í„° ë¬´ê²°ì„±!\")\n",
    "\n",
    "# ìƒ˜í”Œ ë¬¸ì„œ ìƒì„¸ í™•ì¸\n",
    "if loaded_docs:\n",
    "    print(\"\\nğŸ“‹ ìƒ˜í”Œ ë¬¸ì„œ ìƒì„¸ ì •ë³´:\")\n",
    "    for i in range(min(3, len(loaded_docs))):\n",
    "        doc = loaded_docs[i]\n",
    "        print(f\"\\n  ë¬¸ì„œ #{i+1}:\")\n",
    "        print(f\"    ID: {doc.metadata.get('id', 'N/A')}\")\n",
    "        print(f\"    ì›ë³¸ ID: {doc.metadata.get('original_id', 'N/A')}\")\n",
    "        print(f\"    Prefix: {doc.metadata.get('id_source_prefix', 'N/A')}\")\n",
    "        print(f\"    ì¹´í…Œê³ ë¦¬: {doc.metadata.get('category', 'N/A')}\")\n",
    "        print(f\"    ì†ŒìŠ¤: {doc.metadata.get('source', 'N/A')}\")\n",
    "        print(f\"    í˜ì´ì§€: {doc.metadata.get('page', 'N/A')}\")\n",
    "        print(f\"    ë‚´ìš© ê¸¸ì´: {len(doc.page_content)} ê¸€ì\")\n",
    "        print(f\"    ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°: {doc.page_content[:50]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. ìš”ì•½ ë° ë‹¤ìŒ ë‹¨ê³„\n",
    "\n",
    "### ì™„ë£Œëœ ì‘ì—…:\n",
    "- âœ… ë‘ ê°œì˜ DDU pickle íŒŒì¼ ë¡œë“œ\n",
    "- âœ… ë¬¸ì„œ êµ¬ì¡° ë° í˜¸í™˜ì„± ê²€ì¦\n",
    "- âœ… ID ì¶©ëŒ ì²˜ë¦¬ ë° ë³‘í•©\n",
    "- âœ… ë³‘í•©ëœ ë¬¸ì„œ ì €ì¥ ë° ë©”íƒ€ë°ì´í„° ìƒì„±\n",
    "\n",
    "### ë‹¤ìŒ ë‹¨ê³„ ì œì•ˆ:\n",
    "1. **ë°ì´í„°ë² ì´ìŠ¤ ì¸ì œìŠ¤íŠ¸**: ë³‘í•©ëœ ë¬¸ì„œë¥¼ PostgreSQLì— ì¸ì œìŠ¤íŠ¸\n",
    "2. **ì„ë² ë”© ìƒì„±**: ìƒˆë¡œìš´ ë¬¸ì„œë“¤ì— ëŒ€í•œ ë²¡í„° ì„ë² ë”© ìƒì„±\n",
    "3. **ê²€ìƒ‰ í…ŒìŠ¤íŠ¸**: ë³‘í•©ëœ ë°ì´í„°ì…‹ìœ¼ë¡œ ê²€ìƒ‰ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "ğŸ‰ ë³‘í•© ì‘ì—… ì™„ë£Œ!\n",
      "======================================================================\n",
      "\n",
      "ğŸ“Š ìµœì¢… í†µê³„:\n",
      "  ì‘ì—… ì‹œê°„: 2025-08-26 22:19:10\n",
      "  ì´ ë³‘í•© ë¬¸ì„œ: 280ê°œ\n",
      "    - GV80 ë¬¸ì„œ: 122ê°œ\n",
      "    - ë””ì§€í„¸ì •ë¶€ ë¬¸ì„œ: 158ê°œ\n",
      "\n",
      "  ìƒì„±ëœ Prefix:\n",
      "    - gv80_owners_manual_test6p: 122ê°œ\n",
      "    - ë””ì§€í„¸ì •ë¶€í˜ì‹ _ì¶”ì§„ê³„íš: 158ê°œ\n",
      "\n",
      "  ì¹´í…Œê³ ë¦¬ ë¶„í¬:\n",
      "    - paragraph: 182ê°œ (65.0%)\n",
      "    - heading1: 57ê°œ (20.4%)\n",
      "    - table: 14ê°œ (5.0%)\n",
      "    - figure: 13ê°œ (4.6%)\n",
      "    - header: 5ê°œ (1.8%)\n",
      "\n",
      "ğŸ“ ìƒì„±ëœ íŒŒì¼:\n",
      "  âœ… merged_ddu_documents.pkl - ë³‘í•©ëœ DDU ë¬¸ì„œ (860.35 KB)\n",
      "  âœ… merged_ddu_documents.metadata.json - ë©”íƒ€ë°ì´í„° (445 bytes)\n",
      "  âŒ merged_ddu_documents.id_mapping.json - íŒŒì¼ ì—†ìŒ\n",
      "  âŒ id_analysis_report.json - íŒŒì¼ ì—†ìŒ\n",
      "\n",
      "ğŸš€ ë‹¤ìŒ ë‹¨ê³„:\n",
      "1. ë°ì´í„°ë² ì´ìŠ¤ ì¸ì œìŠ¤íŠ¸:\n",
      "   python scripts/2_phase1_ingest_documents.py --pickle-file data/merged_ddu_documents.pkl\n",
      "\n",
      "2. ì„ë² ë”© ìƒì„± ë° ë²¡í„° ì €ì¥:\n",
      "   - í•œêµ­ì–´/ì˜ì–´ ì´ì¤‘ ì–¸ì–´ ì„ë² ë”© ìƒì„±\n",
      "   - PostgreSQL + pgvectorì— ì €ì¥\n",
      "\n",
      "3. ê²€ìƒ‰ í…ŒìŠ¤íŠ¸:\n",
      "   python scripts/test_retrieval_real_data.py\n",
      "\n",
      "4. ì›Œí¬í”Œë¡œìš° ì‹¤í–‰:\n",
      "   python scripts/test_workflow_complete.py\n",
      "\n",
      "ğŸ’¡ íŒ:\n",
      "  - ID ë§¤í•‘ íŒŒì¼ì„ í™œìš©í•˜ì—¬ ì›ë³¸ ID ì¶”ì  ê°€ëŠ¥\n",
      "  - ë©”íƒ€ë°ì´í„° íŒŒì¼ë¡œ ë°ì´í„° êµ¬ì¡° ë¹ ë¥¸ íŒŒì•… ê°€ëŠ¥\n",
      "  - ID ë¶„ì„ ë³´ê³ ì„œë¡œ ë°ì´í„° í’ˆì§ˆ ëª¨ë‹ˆí„°ë§ ê°€ëŠ¥\n",
      "\n",
      "======================================================================\n",
      "ëª¨ë“  ì‘ì—…ì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤! ğŸŠ\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ğŸ‰ ë³‘í•© ì‘ì—… ì™„ë£Œ!\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# ìµœì¢… ìš”ì•½ í†µê³„\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"\\nğŸ“Š ìµœì¢… í†µê³„:\")\n",
    "print(f\"  ì‘ì—… ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"  ì´ ë³‘í•© ë¬¸ì„œ: {len(merged_documents)}ê°œ\")\n",
    "print(f\"    - GV80 ë¬¸ì„œ: {len(gv80_docs)}ê°œ\")\n",
    "print(f\"    - ë””ì§€í„¸ì •ë¶€ ë¬¸ì„œ: {len(digital_gov_docs)}ê°œ\")\n",
    "\n",
    "# Prefixë³„ ìµœì¢… í†µê³„\n",
    "prefix_counts = {}\n",
    "for doc in merged_documents:\n",
    "    prefix = doc.metadata.get('id_source_prefix', 'unknown')\n",
    "    prefix_counts[prefix] = prefix_counts.get(prefix, 0) + 1\n",
    "\n",
    "print(f\"\\n  ìƒì„±ëœ Prefix:\")\n",
    "for prefix, count in sorted(prefix_counts.items()):\n",
    "    print(f\"    - {prefix}: {count}ê°œ\")\n",
    "\n",
    "# ì¹´í…Œê³ ë¦¬ ìš”ì•½\n",
    "category_counts = {}\n",
    "for doc in merged_documents:\n",
    "    category = doc.metadata.get('category', 'unknown')\n",
    "    category_counts[category] = category_counts.get(category, 0) + 1\n",
    "\n",
    "print(f\"\\n  ì¹´í…Œê³ ë¦¬ ë¶„í¬:\")\n",
    "top_categories = sorted(category_counts.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "for category, count in top_categories:\n",
    "    percentage = (count / len(merged_documents)) * 100\n",
    "    print(f\"    - {category}: {count}ê°œ ({percentage:.1f}%)\")\n",
    "\n",
    "# ìƒì„±ëœ íŒŒì¼ ëª©ë¡\n",
    "print(\"\\nğŸ“ ìƒì„±ëœ íŒŒì¼:\")\n",
    "output_files = [\n",
    "    (MERGED_FILE, \"ë³‘í•©ëœ DDU ë¬¸ì„œ\"),\n",
    "    (MERGED_FILE.with_suffix('.metadata.json'), \"ë©”íƒ€ë°ì´í„°\"),\n",
    "    (MERGED_FILE.with_suffix('.id_mapping.json'), \"ID ë§¤í•‘ í…Œì´ë¸”\"),\n",
    "    (DATA_DIR / \"id_analysis_report.json\", \"ID ë¶„ì„ ë³´ê³ ì„œ\")\n",
    "]\n",
    "\n",
    "for file_path, description in output_files:\n",
    "    if file_path.exists():\n",
    "        file_size = file_path.stat().st_size\n",
    "        if file_size > 1024*1024:\n",
    "            size_str = f\"{file_size/(1024*1024):.2f} MB\"\n",
    "        elif file_size > 1024:\n",
    "            size_str = f\"{file_size/1024:.2f} KB\"\n",
    "        else:\n",
    "            size_str = f\"{file_size} bytes\"\n",
    "        print(f\"  âœ… {file_path.name} - {description} ({size_str})\")\n",
    "    else:\n",
    "        print(f\"  âŒ {file_path.name} - íŒŒì¼ ì—†ìŒ\")\n",
    "\n",
    "print(\"\\nğŸš€ ë‹¤ìŒ ë‹¨ê³„:\")\n",
    "print(\"1. ë°ì´í„°ë² ì´ìŠ¤ ì¸ì œìŠ¤íŠ¸:\")\n",
    "print(f\"   python scripts/2_phase1_ingest_documents.py --pickle-file {MERGED_FILE}\")\n",
    "print(\"\")\n",
    "print(\"2. ì„ë² ë”© ìƒì„± ë° ë²¡í„° ì €ì¥:\")\n",
    "print(\"   - í•œêµ­ì–´/ì˜ì–´ ì´ì¤‘ ì–¸ì–´ ì„ë² ë”© ìƒì„±\")\n",
    "print(\"   - PostgreSQL + pgvectorì— ì €ì¥\")\n",
    "print(\"\")\n",
    "print(\"3. ê²€ìƒ‰ í…ŒìŠ¤íŠ¸:\")\n",
    "print(\"   python scripts/test_retrieval_real_data.py\")\n",
    "print(\"\")\n",
    "print(\"4. ì›Œí¬í”Œë¡œìš° ì‹¤í–‰:\")\n",
    "print(\"   python scripts/test_workflow_complete.py\")\n",
    "\n",
    "print(\"\\nğŸ’¡ íŒ:\")\n",
    "print(\"  - ID ë§¤í•‘ íŒŒì¼ì„ í™œìš©í•˜ì—¬ ì›ë³¸ ID ì¶”ì  ê°€ëŠ¥\")\n",
    "print(\"  - ë©”íƒ€ë°ì´í„° íŒŒì¼ë¡œ ë°ì´í„° êµ¬ì¡° ë¹ ë¥¸ íŒŒì•… ê°€ëŠ¥\")\n",
    "print(\"  - ID ë¶„ì„ ë³´ê³ ì„œë¡œ ë°ì´í„° í’ˆì§ˆ ëª¨ë‹ˆí„°ë§ ê°€ëŠ¥\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ëª¨ë“  ì‘ì—…ì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤! ğŸŠ\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multimodal-rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
