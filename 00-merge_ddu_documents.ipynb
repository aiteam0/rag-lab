{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DDU Documents Merger\n",
    "\n",
    "이 노트북은 두 개의 DDU (Document Decomposition Unit) pickle 파일을 하나로 병합합니다.\n",
    "\n",
    "## 병합할 파일:\n",
    "1. `gv80_owners_manual_TEST6P_documents.pkl` - GV80 차량 매뉴얼\n",
    "2. `디지털정부혁신_추진계획_documents.pkl` - 디지털 정부 혁신 계획 문서\n",
    "\n",
    "## DDU Document Schema\n",
    "각 문서는 Langchain Document 객체로 다음 구조를 가집니다:\n",
    "- `page_content`: 실제 텍스트 내용\n",
    "- `metadata`: 14개 카테고리와 다양한 메타데이터"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 필요한 라이브러리 임포트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "작업 디렉토리: /mnt/e/MyProject2/multimodal-rag-wsl-v2\n",
      "실행 시간: 2025-08-26 16:10:23\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Langchain Document 타입 임포트\n",
    "try:\n",
    "    from langchain.schema import Document\n",
    "except ImportError:\n",
    "    from langchain_core.documents import Document\n",
    "\n",
    "print(f\"작업 디렉토리: {os.getcwd()}\")\n",
    "print(f\"실행 시간: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 데이터 파일 경로 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GV80 파일 존재: True\n",
      "디지털정부 파일 존재: True\n"
     ]
    }
   ],
   "source": [
    "# 데이터 디렉토리 경로\n",
    "DATA_DIR = Path(\"data\")\n",
    "\n",
    "# 입력 파일 경로\n",
    "GV80_FILE = DATA_DIR / \"gv80_owners_manual_TEST6P_documents.pkl\"\n",
    "DIGITAL_GOV_FILE = DATA_DIR / \"디지털정부혁신_추진계획_documents.pkl\"\n",
    "\n",
    "# 출력 파일 경로\n",
    "MERGED_FILE = DATA_DIR / \"merged_ddu_documents.pkl\"\n",
    "\n",
    "# 파일 존재 확인\n",
    "print(f\"GV80 파일 존재: {GV80_FILE.exists()}\")\n",
    "print(f\"디지털정부 파일 존재: {DIGITAL_GOV_FILE.exists()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Pickle 파일 로드 및 구조 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "로딩 중: gv80_owners_manual_TEST6P_documents.pkl\n",
      "- 문서 개수: 122\n",
      "- 카테고리 분포:\n",
      "  • caption: 4\n",
      "  • figure: 10\n",
      "  • header: 3\n",
      "  • heading1: 23\n",
      "  • list: 1\n",
      "  • paragraph: 78\n",
      "  • table: 3\n",
      "\n",
      "- 첫 번째 문서 메타데이터 키: ['source', 'page', 'category', 'id', 'raw_output', 'translation_text', 'translation_markdown', 'translation_html', 'contextualize_text', 'caption', 'entity', 'image_path', 'coordinates', 'processing_type', 'processing_status', 'source_parser', 'element_type', 'human_feedback']\n",
      "- 소스: data/gv80_owners_manual_TEST6P.pdf\n",
      "\n",
      "로딩 중: 디지털정부혁신_추진계획_documents.pkl\n",
      "- 문서 개수: 158\n",
      "- 카테고리 분포:\n",
      "  • figure: 3\n",
      "  • header: 2\n",
      "  • heading1: 34\n",
      "  • list: 4\n",
      "  • paragraph: 104\n",
      "  • table: 11\n",
      "\n",
      "- 첫 번째 문서 메타데이터 키: ['source', 'page', 'category', 'id', 'raw_output', 'translation_text', 'translation_markdown', 'translation_html', 'contextualize_text', 'caption', 'entity', 'image_path', 'coordinates', 'processing_type', 'processing_status', 'source_parser', 'element_type', 'human_feedback']\n",
      "- 소스: data/디지털정부혁신_추진계획.pdf\n"
     ]
    }
   ],
   "source": [
    "def load_pickle_file(file_path: Path) -> List[Document]:\n",
    "    \"\"\"Pickle 파일을 로드하고 기본 정보를 출력합니다.\"\"\"\n",
    "    print(f\"\\n로딩 중: {file_path.name}\")\n",
    "    \n",
    "    with open(file_path, 'rb') as f:\n",
    "        documents = pickle.load(f)\n",
    "    \n",
    "    print(f\"- 문서 개수: {len(documents)}\")\n",
    "    \n",
    "    # 카테고리 분포 확인\n",
    "    categories = {}\n",
    "    for doc in documents:\n",
    "        category = doc.metadata.get('category', 'unknown')\n",
    "        categories[category] = categories.get(category, 0) + 1\n",
    "    \n",
    "    print(\"- 카테고리 분포:\")\n",
    "    for cat, count in sorted(categories.items()):\n",
    "        print(f\"  • {cat}: {count}\")\n",
    "    \n",
    "    # 첫 번째 문서 샘플 확인\n",
    "    if documents:\n",
    "        first_doc = documents[0]\n",
    "        print(f\"\\n- 첫 번째 문서 메타데이터 키: {list(first_doc.metadata.keys())}\")\n",
    "        print(f\"- 소스: {first_doc.metadata.get('source', 'N/A')}\")\n",
    "    \n",
    "    return documents\n",
    "\n",
    "# 두 파일 로드\n",
    "gv80_docs = load_pickle_file(GV80_FILE)\n",
    "digital_gov_docs = load_pickle_file(DIGITAL_GOV_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 문서 병합 전 검증\n",
    "\n",
    "병합 전에 확인해야 할 사항:\n",
    "- ID 중복 여부\n",
    "- 메타데이터 구조 일관성\n",
    "- 소스 경로 충돌"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 병합 전 검증 결과 ===\n",
      "ID 충돌 개수: 116\n",
      "  충돌 ID: [1, 2, 3, 4, 5]...\n",
      "\n",
      "메타데이터 키 차이: 없음\n",
      "\n",
      "고유 소스 파일: 2개\n",
      "  - data/gv80_owners_manual_TEST6P.pdf\n",
      "  - data/디지털정부혁신_추진계획.pdf\n",
      "\n",
      "전체 카테고리 종류: 7개\n",
      "  caption, figure, header, heading1, list, paragraph, table\n"
     ]
    }
   ],
   "source": [
    "def validate_documents(docs1: List[Document], docs2: List[Document]) -> Dict[str, Any]:\n",
    "    \"\"\"두 문서 세트의 호환성을 검증합니다.\"\"\"\n",
    "    validation_result = {\n",
    "        'id_conflicts': [],\n",
    "        'metadata_keys_diff': set(),\n",
    "        'source_files': set(),\n",
    "        'total_pages': set(),\n",
    "        'categories': set()\n",
    "    }\n",
    "    \n",
    "    # ID 수집\n",
    "    ids1 = {doc.metadata.get('id') for doc in docs1 if doc.metadata.get('id')}\n",
    "    ids2 = {doc.metadata.get('id') for doc in docs2 if doc.metadata.get('id')}\n",
    "    \n",
    "    # ID 충돌 확인\n",
    "    validation_result['id_conflicts'] = list(ids1.intersection(ids2))\n",
    "    \n",
    "    # 메타데이터 키 비교\n",
    "    keys1 = set(docs1[0].metadata.keys()) if docs1 else set()\n",
    "    keys2 = set(docs2[0].metadata.keys()) if docs2 else set()\n",
    "    validation_result['metadata_keys_diff'] = keys1.symmetric_difference(keys2)\n",
    "    \n",
    "    # 소스 파일 수집\n",
    "    for doc in docs1 + docs2:\n",
    "        if 'source' in doc.metadata:\n",
    "            validation_result['source_files'].add(doc.metadata['source'])\n",
    "        if 'page' in doc.metadata:\n",
    "            validation_result['total_pages'].add(doc.metadata['page'])\n",
    "        if 'category' in doc.metadata:\n",
    "            validation_result['categories'].add(doc.metadata['category'])\n",
    "    \n",
    "    return validation_result\n",
    "\n",
    "# 검증 실행\n",
    "validation = validate_documents(gv80_docs, digital_gov_docs)\n",
    "\n",
    "print(\"\\n=== 병합 전 검증 결과 ===\")\n",
    "print(f\"ID 충돌 개수: {len(validation['id_conflicts'])}\")\n",
    "if validation['id_conflicts']:\n",
    "    print(f\"  충돌 ID: {validation['id_conflicts'][:5]}...\")\n",
    "\n",
    "print(f\"\\n메타데이터 키 차이: {validation['metadata_keys_diff'] if validation['metadata_keys_diff'] else '없음'}\")\n",
    "print(f\"\\n고유 소스 파일: {len(validation['source_files'])}개\")\n",
    "for source in validation['source_files']:\n",
    "    print(f\"  - {source}\")\n",
    "\n",
    "print(f\"\\n전체 카테고리 종류: {len(validation['categories'])}개\")\n",
    "print(f\"  {', '.join(sorted(validation['categories']))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 문서 병합 로직\n",
    "\n",
    "병합 시 고려사항:\n",
    "- ID 충돌 시 소스별 prefix 추가\n",
    "- 메타데이터 무결성 유지\n",
    "- 원본 순서 보존"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "옵션 1: 카테고리를 포함한 ID 생성\n",
      "📝 ID 재구성 중...\n",
      "\n",
      "✅ ID 재구성 완료!\n",
      "  - 총 163개 ID 매핑 생성\n",
      "\n",
      "📊 Prefix별 문서 수:\n",
      "  - gv80_owners_manual_test6p: 122개\n",
      "  - 디지털정부혁신_추진계획: 158개\n",
      "\n",
      "✅ 모든 ID가 고유합니다.\n",
      "\n",
      "==================================================\n",
      "\n",
      "옵션 2: 단순 순차 ID 생성\n",
      "📝 ID 재구성 중...\n",
      "\n",
      "✅ ID 재구성 완료!\n",
      "  - 총 163개 ID 매핑 생성\n",
      "\n",
      "📊 Prefix별 문서 수:\n",
      "  - gv80_owners_manual_test6p: 122개\n",
      "  - 디지털정부혁신_추진계획: 158개\n",
      "\n",
      "✅ 모든 ID가 고유합니다.\n",
      "\n",
      "병합 완료!\n",
      "총 문서 수: 280\n",
      "예상 문서 수: 280\n",
      "일치 여부: True\n",
      "\n",
      "📋 새 ID 샘플 (처음 5개):\n",
      "  [gv80_owners_manual_test6p] 0 → gv80_owners_manual_test6p_0001 (카테고리: heading1)\n",
      "  [gv80_owners_manual_test6p] 1 → gv80_owners_manual_test6p_0002 (카테고리: heading1)\n",
      "  [gv80_owners_manual_test6p] 2 → gv80_owners_manual_test6p_0003 (카테고리: figure)\n",
      "  [gv80_owners_manual_test6p] 3 → gv80_owners_manual_test6p_0004 (카테고리: table)\n",
      "  [gv80_owners_manual_test6p] 4 → gv80_owners_manual_test6p_0005 (카테고리: caption)\n"
     ]
    }
   ],
   "source": [
    "def merge_documents(\n",
    "    docs1: List[Document], \n",
    "    docs2: List[Document],\n",
    "    reorganize_ids: bool = True,\n",
    "    include_category_in_id: bool = False\n",
    ") -> tuple[List[Document], Dict[str, str]]:\n",
    "    \"\"\"두 문서 리스트를 병합하고 새로운 ID 체계를 적용합니다.\n",
    "    \n",
    "    Args:\n",
    "        docs1: 첫 번째 문서 리스트\n",
    "        docs2: 두 번째 문서 리스트\n",
    "        reorganize_ids: 모든 문서에 새 ID 부여 여부\n",
    "        include_category_in_id: ID에 카테고리 포함 여부\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (병합된 문서 리스트, ID 매핑 딕셔너리)\n",
    "    \"\"\"\n",
    "    import copy\n",
    "    import re\n",
    "    from collections import defaultdict\n",
    "    \n",
    "    merged_docs = []\n",
    "    id_mapping = {}  # old_id -> new_id 매핑\n",
    "    \n",
    "    def extract_source_prefix(source_path: str) -> str:\n",
    "        \"\"\"소스 파일명에서 의미있는 prefix를 추출합니다.\n",
    "        파일명을 그대로 사용하되, 확장자와 특수문자는 제거합니다.\n",
    "        \"\"\"\n",
    "        import os\n",
    "        \n",
    "        # 파일명만 추출 (경로 제거)\n",
    "        filename = os.path.basename(source_path)\n",
    "        \n",
    "        # 확장자 제거\n",
    "        name_without_ext = os.path.splitext(filename)[0]\n",
    "        \n",
    "        # 특수문자를 언더스코어로 변환하고 소문자로 변환\n",
    "        # 한글은 유지하되, 공백과 특수문자는 언더스코어로 변환\n",
    "        clean_name = re.sub(r'[^\\w가-힣]+', '_', name_without_ext)\n",
    "        clean_name = clean_name.strip('_')  # 앞뒤 언더스코어 제거\n",
    "        \n",
    "        # 너무 길면 적절히 자름 (최대 20자)\n",
    "        # if len(clean_name) > 20:\n",
    "        #     clean_name = clean_name[:20]\n",
    "        \n",
    "        return clean_name.lower()\n",
    "    \n",
    "    def get_category_abbr(category: str) -> str:\n",
    "        \"\"\"카테고리를 짧은 약어로 변환합니다.\"\"\"\n",
    "        category_map = {\n",
    "            'heading1': 'h1',\n",
    "            'heading2': 'h2', \n",
    "            'heading3': 'h3',\n",
    "            'paragraph': 'para',\n",
    "            'list': 'list',\n",
    "            'table': 'tbl',\n",
    "            'figure': 'fig',\n",
    "            'chart': 'chrt',\n",
    "            'equation': 'eq',\n",
    "            'caption': 'cap',\n",
    "            'footnote': 'fn',\n",
    "            'header': 'hdr',\n",
    "            'footer': 'ftr',\n",
    "            'reference': 'ref'\n",
    "        }\n",
    "        return category_map.get(category, 'unk')\n",
    "    \n",
    "    def process_documents(docs: List[Document], default_prefix: str) -> List[Document]:\n",
    "        \"\"\"문서 리스트를 처리하고 새 ID를 부여합니다.\"\"\"\n",
    "        processed_docs = []\n",
    "        \n",
    "        # source별로 문서 그룹화\n",
    "        docs_by_source = defaultdict(list)\n",
    "        for doc in docs:\n",
    "            source = doc.metadata.get('source', 'unknown')\n",
    "            docs_by_source[source].append(doc)\n",
    "        \n",
    "        # 각 source별로 처리\n",
    "        for source, source_docs in docs_by_source.items():\n",
    "            prefix = extract_source_prefix(source) if source != 'unknown' else default_prefix\n",
    "            \n",
    "            # 카테고리별로 카운터 관리 (옵션)\n",
    "            if include_category_in_id:\n",
    "                category_counters = defaultdict(int)\n",
    "                \n",
    "                for doc in source_docs:\n",
    "                    new_doc = copy.deepcopy(doc)\n",
    "                    category = doc.metadata.get('category', 'unknown')\n",
    "                    cat_abbr = get_category_abbr(category)\n",
    "                    \n",
    "                    category_counters[cat_abbr] += 1\n",
    "                    new_id = f\"{prefix}_{cat_abbr}_{category_counters[cat_abbr]:03d}\"\n",
    "                    \n",
    "                    # 원본 ID 보존\n",
    "                    if 'id' in new_doc.metadata:\n",
    "                        old_id = new_doc.metadata['id']\n",
    "                        new_doc.metadata['original_id'] = old_id\n",
    "                        id_mapping[old_id] = new_id\n",
    "                    \n",
    "                    new_doc.metadata['id'] = new_id\n",
    "                    new_doc.metadata['id_source_prefix'] = prefix\n",
    "                    processed_docs.append(new_doc)\n",
    "            else:\n",
    "                # 단순 순차 번호\n",
    "                for idx, doc in enumerate(source_docs, 1):\n",
    "                    new_doc = copy.deepcopy(doc)\n",
    "                    new_id = f\"{prefix}_{idx:04d}\"\n",
    "                    \n",
    "                    # 원본 ID 보존\n",
    "                    if 'id' in new_doc.metadata:\n",
    "                        old_id = new_doc.metadata['id']\n",
    "                        new_doc.metadata['original_id'] = old_id\n",
    "                        id_mapping[old_id] = new_id\n",
    "                    \n",
    "                    new_doc.metadata['id'] = new_id\n",
    "                    new_doc.metadata['id_source_prefix'] = prefix\n",
    "                    processed_docs.append(new_doc)\n",
    "        \n",
    "        return processed_docs\n",
    "    \n",
    "    # 두 문서 세트 처리\n",
    "    print(\"📝 ID 재구성 중...\")\n",
    "    processed_docs1 = process_documents(docs1, 'doc1')\n",
    "    processed_docs2 = process_documents(docs2, 'doc2')\n",
    "    \n",
    "    merged_docs = processed_docs1 + processed_docs2\n",
    "    \n",
    "    # ID 재구성 통계 출력\n",
    "    print(f\"\\n✅ ID 재구성 완료!\")\n",
    "    print(f\"  - 총 {len(id_mapping)}개 ID 매핑 생성\")\n",
    "    \n",
    "    # prefix별 통계\n",
    "    prefix_stats = defaultdict(int)\n",
    "    for doc in merged_docs:\n",
    "        prefix = doc.metadata.get('id_source_prefix', 'unknown')\n",
    "        prefix_stats[prefix] += 1\n",
    "    \n",
    "    print(\"\\n📊 Prefix별 문서 수:\")\n",
    "    for prefix, count in sorted(prefix_stats.items()):\n",
    "        print(f\"  - {prefix}: {count}개\")\n",
    "    \n",
    "    # 중복 ID 체크\n",
    "    all_new_ids = [doc.metadata.get('id') for doc in merged_docs if 'id' in doc.metadata]\n",
    "    unique_ids = set(all_new_ids)\n",
    "    if len(all_new_ids) != len(unique_ids):\n",
    "        print(f\"\\n⚠️  경고: {len(all_new_ids) - len(unique_ids)}개의 중복 ID 발견!\")\n",
    "    else:\n",
    "        print(f\"\\n✅ 모든 ID가 고유합니다.\")\n",
    "    \n",
    "    return merged_docs, id_mapping\n",
    "\n",
    "# 병합 실행 (카테고리 포함 옵션)\n",
    "print(\"옵션 1: 카테고리를 포함한 ID 생성\")\n",
    "merged_documents_with_cat, id_mapping_with_cat = merge_documents(\n",
    "    gv80_docs, \n",
    "    digital_gov_docs, \n",
    "    include_category_in_id=True\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"\\n옵션 2: 단순 순차 ID 생성\")\n",
    "merged_documents, id_mapping = merge_documents(\n",
    "    gv80_docs, \n",
    "    digital_gov_docs, \n",
    "    include_category_in_id=False\n",
    ")\n",
    "\n",
    "print(f\"\\n병합 완료!\")\n",
    "print(f\"총 문서 수: {len(merged_documents)}\")\n",
    "print(f\"예상 문서 수: {len(gv80_docs) + len(digital_gov_docs)}\")\n",
    "print(f\"일치 여부: {len(merged_documents) == len(gv80_docs) + len(digital_gov_docs)}\")\n",
    "\n",
    "# 샘플 ID 확인\n",
    "print(\"\\n📋 새 ID 샘플 (처음 5개):\")\n",
    "for doc in merged_documents[:5]:\n",
    "    old_id = doc.metadata.get('original_id', 'N/A')\n",
    "    new_id = doc.metadata.get('id', 'N/A')\n",
    "    category = doc.metadata.get('category', 'N/A')\n",
    "    source_prefix = doc.metadata.get('id_source_prefix', 'N/A')\n",
    "    print(f\"  [{source_prefix}] {old_id} → {new_id} (카테고리: {category})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 병합 결과 검증"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 병합 결과 분석 ===\n",
      "\n",
      "소스별 문서 분포:\n",
      "  data/gv80_owners_manual_TEST6P.pdf: 122 문서\n",
      "  data/디지털정부혁신_추진계획.pdf: 158 문서\n",
      "\n",
      "소스별 카테고리 분포:\n",
      "\n",
      "  [data/gv80_owners_manual_TEST6P.pdf]\n",
      "    • caption: 4\n",
      "    • figure: 10\n",
      "    • header: 3\n",
      "    • heading1: 23\n",
      "    • list: 1\n",
      "    • paragraph: 78\n",
      "    • table: 3\n",
      "\n",
      "  [data/디지털정부혁신_추진계획.pdf]\n",
      "    • figure: 3\n",
      "    • header: 2\n",
      "    • heading1: 34\n",
      "    • list: 4\n",
      "    • paragraph: 104\n",
      "    • table: 11\n",
      "\n",
      "✅ 모든 문서의 메타데이터가 완전합니다.\n"
     ]
    }
   ],
   "source": [
    "def analyze_merged_documents(documents: List[Document]) -> None:\n",
    "    \"\"\"병합된 문서의 통계를 분석합니다.\"\"\"\n",
    "    \n",
    "    # 소스별 분류\n",
    "    sources = {}\n",
    "    for doc in documents:\n",
    "        source = doc.metadata.get('source', 'unknown')\n",
    "        if source not in sources:\n",
    "            sources[source] = []\n",
    "        sources[source].append(doc)\n",
    "    \n",
    "    print(\"\\n=== 병합 결과 분석 ===\")\n",
    "    print(f\"\\n소스별 문서 분포:\")\n",
    "    for source, docs in sources.items():\n",
    "        print(f\"  {source}: {len(docs)} 문서\")\n",
    "    \n",
    "    # 카테고리별 통계\n",
    "    categories_by_source = {}\n",
    "    for doc in documents:\n",
    "        source = doc.metadata.get('source', 'unknown')\n",
    "        category = doc.metadata.get('category', 'unknown')\n",
    "        \n",
    "        if source not in categories_by_source:\n",
    "            categories_by_source[source] = {}\n",
    "        \n",
    "        if category not in categories_by_source[source]:\n",
    "            categories_by_source[source][category] = 0\n",
    "        \n",
    "        categories_by_source[source][category] += 1\n",
    "    \n",
    "    print(\"\\n소스별 카테고리 분포:\")\n",
    "    for source, categories in categories_by_source.items():\n",
    "        print(f\"\\n  [{source}]\")\n",
    "        for cat, count in sorted(categories.items()):\n",
    "            print(f\"    • {cat}: {count}\")\n",
    "    \n",
    "    # 메타데이터 완전성 체크\n",
    "    required_fields = ['source', 'category', 'id', 'page']\n",
    "    incomplete_docs = []\n",
    "    \n",
    "    for i, doc in enumerate(documents):\n",
    "        missing = [field for field in required_fields if field not in doc.metadata]\n",
    "        if missing:\n",
    "            incomplete_docs.append((i, missing))\n",
    "    \n",
    "    if incomplete_docs:\n",
    "        print(f\"\\n⚠️  메타데이터 불완전 문서: {len(incomplete_docs)}개\")\n",
    "        for idx, missing in incomplete_docs[:5]:\n",
    "            print(f\"    문서 {idx}: {missing} 누락\")\n",
    "    else:\n",
    "        print(f\"\\n✅ 모든 문서의 메타데이터가 완전합니다.\")\n",
    "\n",
    "# 분석 실행\n",
    "analyze_merged_documents(merged_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 병합된 문서 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ 저장 완료!\n",
      "  파일: data/merged_ddu_documents.pkl\n",
      "  크기: 0.84 MB\n",
      "  문서 수: 280\n",
      "  메타데이터: data/merged_ddu_documents.metadata.json\n"
     ]
    }
   ],
   "source": [
    "def save_merged_documents(\n",
    "    documents: List[Document], \n",
    "    output_path: Path,\n",
    "    create_backup: bool = True\n",
    ") -> None:\n",
    "    \"\"\"병합된 문서를 pickle 파일로 저장합니다.\"\"\"\n",
    "    \n",
    "    # 백업 생성 (기존 파일이 있는 경우)\n",
    "    if create_backup and output_path.exists():\n",
    "        backup_path = output_path.with_suffix(f'.backup_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.pkl')\n",
    "        print(f\"기존 파일 백업: {backup_path}\")\n",
    "        output_path.rename(backup_path)\n",
    "    \n",
    "    # 저장\n",
    "    with open(output_path, 'wb') as f:\n",
    "        pickle.dump(documents, f)\n",
    "    \n",
    "    file_size = output_path.stat().st_size / (1024 * 1024)  # MB로 변환\n",
    "    print(f\"\\n✅ 저장 완료!\")\n",
    "    print(f\"  파일: {output_path}\")\n",
    "    print(f\"  크기: {file_size:.2f} MB\")\n",
    "    print(f\"  문서 수: {len(documents)}\")\n",
    "    \n",
    "    # 메타데이터 파일도 생성 (JSON)\n",
    "    metadata_path = output_path.with_suffix('.metadata.json')\n",
    "    metadata = {\n",
    "        'created_at': datetime.now().isoformat(),\n",
    "        'total_documents': len(documents),\n",
    "        'sources': list({doc.metadata.get('source') for doc in documents if 'source' in doc.metadata}),\n",
    "        'categories': list({doc.metadata.get('category') for doc in documents if 'category' in doc.metadata}),\n",
    "        'merged_from': [\n",
    "            str(GV80_FILE),\n",
    "            str(DIGITAL_GOV_FILE)\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    with open(metadata_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(metadata, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    print(f\"  메타데이터: {metadata_path}\")\n",
    "\n",
    "# 저장 실행\n",
    "save_merged_documents(merged_documents, MERGED_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📁 ID 매핑 저장: data/id_mapping.json\n",
      "   - 총 163개 매핑\n",
      "   - 디지털정부혁신: 158개\n",
      "   - gv80: 5개\n",
      "\n",
      "======================================================================\n",
      "🔍 ID 패턴 상세 분석\n",
      "======================================================================\n",
      "\n",
      "📌 ID 중복 체크:\n",
      "  ✅ 모든 ID가 고유합니다.\n",
      "\n",
      "📊 카테고리별 ID 분포:\n",
      "  카테고리            | 문서수      | 사용 Prefix                      | 샘플 ID\n",
      "  ------------------------------------------------------------------------------------------\n",
      "  paragraph       | 182      | 디지털정부혁신_추진계획, gv80_owners_manual_test6p | gv80_owners_manual_test6p_0006\n",
      "  heading1        | 57       | 디지털정부혁신_추진계획, gv80_owners_manual_test6p | gv80_owners_manual_test6p_0001\n",
      "  table           | 14       | 디지털정부혁신_추진계획, gv80_owners_manual_test6p | gv80_owners_manual_test6p_0004\n",
      "  figure          | 13       | 디지털정부혁신_추진계획, gv80_owners_manual_test6p | gv80_owners_manual_test6p_0003\n",
      "  header          | 5        | 디지털정부혁신_추진계획, gv80_owners_manual_test6p | gv80_owners_manual_test6p_0044\n",
      "  list            | 5        | 디지털정부혁신_추진계획, gv80_owners_manual_test6p | gv80_owners_manual_test6p_0095\n",
      "  caption         | 4        | gv80_owners_manual_test6p      | gv80_owners_manual_test6p_0005\n",
      "\n",
      "📄 페이지별 ID 순서 분석 (샘플):\n",
      "  data/gv80_owners_manual_TEST6P.pdf:p5: 36개 문서 (순차적)\n",
      "    샘플: gv80_owners_manual_test6p_0056, gv80_owners_manual_test6p_0057, gv80_owners_manual_test6p_0058\n",
      "  data/gv80_owners_manual_TEST6P.pdf:p6: 31개 문서 (순차적)\n",
      "    샘플: gv80_owners_manual_test6p_0092, gv80_owners_manual_test6p_0093, gv80_owners_manual_test6p_0094\n",
      "  data/gv80_owners_manual_TEST6P.pdf:p1: 30개 문서 (순차적)\n",
      "    샘플: gv80_owners_manual_test6p_0001, gv80_owners_manual_test6p_0002, gv80_owners_manual_test6p_0003\n",
      "  data/디지털정부혁신_추진계획.pdf:p6: 26개 문서 (순차적)\n",
      "    샘플: 디지털정부혁신_추진계획_0065, 디지털정부혁신_추진계획_0066, 디지털정부혁신_추진계획_0067\n",
      "  data/디지털정부혁신_추진계획.pdf:p10: 22개 문서 (순차적)\n",
      "    샘플: 디지털정부혁신_추진계획_0137, 디지털정부혁신_추진계획_0138, 디지털정부혁신_추진계획_0139\n",
      "\n",
      "🏷️  Prefix 패턴 분석:\n",
      "\n",
      "  [디지털정부혁신_추진계획] - 158개 문서\n",
      "    카테고리: table, header, figure, list, heading1\n",
      "    ID 샘플: 디지털정부혁신_추진계획_0005, 디지털정부혁신_추진계획_0004, 디지털정부혁신_추진계획_0002\n",
      "\n",
      "  [gv80_owners_manual_test6p] - 122개 문서\n",
      "    카테고리: table, header, figure, list, caption\n",
      "    ID 샘플: gv80_owners_manual_test6p_0002, gv80_owners_manual_test6p_0001, gv80_owners_manual_test6p_0003\n",
      "\n",
      "📏 ID 길이 통계:\n",
      "  최소: 17자\n",
      "  최대: 30자\n",
      "  평균: 22.7자\n",
      "  가장 흔한 길이: 17자\n",
      "\n",
      "🔄 ID 변환 예시 (10개):\n",
      "  Original ID                                   → New ID                   \n",
      "  ---------------------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'int' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 228\u001b[39m\n\u001b[32m    226\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m  \u001b[39m\u001b[33m\"\u001b[39m + \u001b[33m\"\u001b[39m\u001b[33m-\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m75\u001b[39m)\n\u001b[32m    227\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, (old_id, new_id) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mlist\u001b[39m(id_mapping.items())[:\u001b[32m10\u001b[39m]):\n\u001b[32m--> \u001b[39m\u001b[32m228\u001b[39m     truncated_old = old_id[:\u001b[32m45\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mold_id\u001b[49m\u001b[43m)\u001b[49m > \u001b[32m45\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m old_id\n\u001b[32m    229\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtruncated_old\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m<45\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m → \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnew_id\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m<25\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    231\u001b[39m \u001b[38;5;66;03m# 분석 결과를 JSON으로도 저장\u001b[39;00m\n",
      "\u001b[31mTypeError\u001b[39m: object of type 'int' has no len()"
     ]
    }
   ],
   "source": [
    "def save_id_mapping(id_mapping: Dict[str, str], output_path: Path = None) -> None:\n",
    "    \"\"\"ID 매핑을 JSON 파일로 저장합니다.\"\"\"\n",
    "    if output_path is None:\n",
    "        output_path = DATA_DIR / \"id_mapping.json\"\n",
    "    \n",
    "    # ID 매핑 정보 확장\n",
    "    mapping_info = {\n",
    "        'created_at': datetime.now().isoformat(),\n",
    "        'total_mappings': len(id_mapping),\n",
    "        'mapping': id_mapping,\n",
    "        'statistics': {}\n",
    "    }\n",
    "    \n",
    "    # prefix별 통계 계산\n",
    "    prefix_counts = {}\n",
    "    for new_id in id_mapping.values():\n",
    "        prefix = new_id.split('_')[0]\n",
    "        prefix_counts[prefix] = prefix_counts.get(prefix, 0) + 1\n",
    "    \n",
    "    mapping_info['statistics']['by_prefix'] = prefix_counts\n",
    "    \n",
    "    # JSON으로 저장\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(mapping_info, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    print(f\"📁 ID 매핑 저장: {output_path}\")\n",
    "    print(f\"   - 총 {len(id_mapping)}개 매핑\")\n",
    "    for prefix, count in prefix_counts.items():\n",
    "        print(f\"   - {prefix}: {count}개\")\n",
    "\n",
    "# ID 매핑 저장\n",
    "save_id_mapping(id_mapping)\n",
    "\n",
    "# ========================================================================\n",
    "# ID 패턴 분석 로직 - 완전 구현\n",
    "# ========================================================================\n",
    "\n",
    "def analyze_id_patterns(documents: List[Document], id_mapping: Dict[str, str]) -> Dict[str, Any]:\n",
    "    \"\"\"ID 패턴을 상세하게 분석합니다.\"\"\"\n",
    "    analysis_result = {\n",
    "        'total_documents': len(documents),\n",
    "        'id_duplicates': [],\n",
    "        'category_distribution': {},\n",
    "        'page_distribution': {},\n",
    "        'prefix_patterns': {},\n",
    "        'consistency_check': {},\n",
    "        'id_length_stats': {}\n",
    "    }\n",
    "    \n",
    "    # 1. ID 중복 체크\n",
    "    all_ids = []\n",
    "    id_counter = {}\n",
    "    for doc in documents:\n",
    "        doc_id = doc.metadata.get('id')\n",
    "        if doc_id:\n",
    "            all_ids.append(doc_id)\n",
    "            id_counter[doc_id] = id_counter.get(doc_id, 0) + 1\n",
    "    \n",
    "    # 중복 ID 찾기\n",
    "    duplicates = {id: count for id, count in id_counter.items() if count > 1}\n",
    "    analysis_result['id_duplicates'] = duplicates\n",
    "    \n",
    "    # 2. 카테고리별 ID 분포\n",
    "    category_ids = {}\n",
    "    for doc in documents:\n",
    "        category = doc.metadata.get('category', 'unknown')\n",
    "        doc_id = doc.metadata.get('id', 'no_id')\n",
    "        prefix = doc.metadata.get('id_source_prefix', 'unknown')\n",
    "        \n",
    "        if category not in category_ids:\n",
    "            category_ids[category] = {'count': 0, 'prefixes': set(), 'sample_ids': []}\n",
    "        \n",
    "        category_ids[category]['count'] += 1\n",
    "        category_ids[category]['prefixes'].add(prefix)\n",
    "        if len(category_ids[category]['sample_ids']) < 3:\n",
    "            category_ids[category]['sample_ids'].append(doc_id)\n",
    "    \n",
    "    # Set을 리스트로 변환\n",
    "    for cat in category_ids:\n",
    "        category_ids[cat]['prefixes'] = list(category_ids[cat]['prefixes'])\n",
    "    \n",
    "    analysis_result['category_distribution'] = category_ids\n",
    "    \n",
    "    # 3. 페이지별 ID 순서 분석\n",
    "    page_ids = {}\n",
    "    for doc in documents:\n",
    "        page = doc.metadata.get('page', 'unknown')\n",
    "        doc_id = doc.metadata.get('id', 'no_id')\n",
    "        source = doc.metadata.get('source', 'unknown')\n",
    "        \n",
    "        page_key = f\"{source}:p{page}\"\n",
    "        if page_key not in page_ids:\n",
    "            page_ids[page_key] = []\n",
    "        page_ids[page_key].append(doc_id)\n",
    "    \n",
    "    # 각 페이지의 ID 수와 순서 정보 저장\n",
    "    for page_key in page_ids:\n",
    "        page_ids[page_key] = {\n",
    "            'count': len(page_ids[page_key]),\n",
    "            'ids': page_ids[page_key][:5],  # 처음 5개만 샘플로\n",
    "            'is_sequential': check_sequential(page_ids[page_key])\n",
    "        }\n",
    "    \n",
    "    analysis_result['page_distribution'] = page_ids\n",
    "    \n",
    "    # 4. Prefix 패턴 분석\n",
    "    prefix_patterns = {}\n",
    "    for doc in documents:\n",
    "        prefix = doc.metadata.get('id_source_prefix', 'unknown')\n",
    "        doc_id = doc.metadata.get('id', 'no_id')\n",
    "        \n",
    "        if prefix not in prefix_patterns:\n",
    "            prefix_patterns[prefix] = {\n",
    "                'count': 0,\n",
    "                'id_format_samples': set(),\n",
    "                'categories': set()\n",
    "            }\n",
    "        \n",
    "        prefix_patterns[prefix]['count'] += 1\n",
    "        prefix_patterns[prefix]['categories'].add(doc.metadata.get('category', 'unknown'))\n",
    "        \n",
    "        # ID 형식 샘플 저장 (처음 5개만)\n",
    "        if len(prefix_patterns[prefix]['id_format_samples']) < 5:\n",
    "            prefix_patterns[prefix]['id_format_samples'].add(doc_id)\n",
    "    \n",
    "    # Set을 리스트로 변환\n",
    "    for prefix in prefix_patterns:\n",
    "        prefix_patterns[prefix]['id_format_samples'] = list(prefix_patterns[prefix]['id_format_samples'])\n",
    "        prefix_patterns[prefix]['categories'] = list(prefix_patterns[prefix]['categories'])\n",
    "    \n",
    "    analysis_result['prefix_patterns'] = prefix_patterns\n",
    "    \n",
    "    # 5. ID 길이 통계\n",
    "    id_lengths = [len(doc_id) for doc_id in all_ids if doc_id]\n",
    "    if id_lengths:\n",
    "        analysis_result['id_length_stats'] = {\n",
    "            'min': min(id_lengths),\n",
    "            'max': max(id_lengths),\n",
    "            'avg': sum(id_lengths) / len(id_lengths),\n",
    "            'most_common': max(set(id_lengths), key=id_lengths.count)\n",
    "        }\n",
    "    \n",
    "    return analysis_result\n",
    "\n",
    "def check_sequential(ids: List[str]) -> bool:\n",
    "    \"\"\"ID 리스트가 순차적인지 확인합니다.\"\"\"\n",
    "    try:\n",
    "        # ID에서 숫자 부분만 추출\n",
    "        numbers = []\n",
    "        for id in ids:\n",
    "            # 마지막 언더스코어 이후의 숫자 추출\n",
    "            parts = id.split('_')\n",
    "            if parts:\n",
    "                last_part = parts[-1]\n",
    "                if last_part.isdigit():\n",
    "                    numbers.append(int(last_part))\n",
    "        \n",
    "        # 순차적인지 확인\n",
    "        if numbers:\n",
    "            sorted_nums = sorted(numbers)\n",
    "            for i in range(1, len(sorted_nums)):\n",
    "                if sorted_nums[i] - sorted_nums[i-1] != 1:\n",
    "                    return False\n",
    "            return True\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    return False\n",
    "\n",
    "# ID 패턴 분석 실행\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"🔍 ID 패턴 상세 분석\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "analysis = analyze_id_patterns(merged_documents, id_mapping)\n",
    "\n",
    "# 1. 중복 ID 체크\n",
    "print(\"\\n📌 ID 중복 체크:\")\n",
    "if analysis['id_duplicates']:\n",
    "    print(f\"  ⚠️  {len(analysis['id_duplicates'])}개의 중복 ID 발견!\")\n",
    "    for dup_id, count in list(analysis['id_duplicates'].items())[:5]:\n",
    "        print(f\"    - {dup_id}: {count}회 중복\")\n",
    "else:\n",
    "    print(\"  ✅ 모든 ID가 고유합니다.\")\n",
    "\n",
    "# 2. 카테고리별 분포\n",
    "print(\"\\n📊 카테고리별 ID 분포:\")\n",
    "print(f\"  {'카테고리':<15} | {'문서수':<8} | {'사용 Prefix':<30} | {'샘플 ID'}\")\n",
    "print(\"  \" + \"-\"*90)\n",
    "for category, info in sorted(analysis['category_distribution'].items(), \n",
    "                            key=lambda x: x[1]['count'], reverse=True):\n",
    "    prefixes = ', '.join(info['prefixes'][:3])\n",
    "    sample_id = info['sample_ids'][0] if info['sample_ids'] else 'N/A'\n",
    "    print(f\"  {category:<15} | {info['count']:<8} | {prefixes:<30} | {sample_id}\")\n",
    "\n",
    "# 3. 페이지별 분석 (상위 5개만)\n",
    "print(\"\\n📄 페이지별 ID 순서 분석 (샘플):\")\n",
    "page_items = sorted(analysis['page_distribution'].items(), \n",
    "                   key=lambda x: x[1]['count'], reverse=True)[:5]\n",
    "for page_key, info in page_items:\n",
    "    seq_status = \"순차적\" if info['is_sequential'] else \"비순차적\"\n",
    "    print(f\"  {page_key}: {info['count']}개 문서 ({seq_status})\")\n",
    "    if info['ids']:\n",
    "        print(f\"    샘플: {', '.join(info['ids'][:3])}\")\n",
    "\n",
    "# 4. Prefix 패턴 분석\n",
    "print(\"\\n🏷️  Prefix 패턴 분석:\")\n",
    "for prefix, pattern in sorted(analysis['prefix_patterns'].items(), \n",
    "                              key=lambda x: x[1]['count'], reverse=True):\n",
    "    print(f\"\\n  [{prefix}] - {pattern['count']}개 문서\")\n",
    "    print(f\"    카테고리: {', '.join(pattern['categories'][:5])}\")\n",
    "    print(f\"    ID 샘플: {', '.join(pattern['id_format_samples'][:3])}\")\n",
    "\n",
    "# 5. ID 길이 통계\n",
    "if analysis['id_length_stats']:\n",
    "    stats = analysis['id_length_stats']\n",
    "    print(\"\\n📏 ID 길이 통계:\")\n",
    "    print(f\"  최소: {stats['min']}자\")\n",
    "    print(f\"  최대: {stats['max']}자\")\n",
    "    print(f\"  평균: {stats['avg']:.1f}자\")\n",
    "    print(f\"  가장 흔한 길이: {stats['most_common']}자\")\n",
    "\n",
    "# 6. ID 매핑 변환 예시\n",
    "print(\"\\n🔄 ID 변환 예시 (10개):\")\n",
    "print(f\"  {'Original ID':<45} → {'New ID':<25}\")\n",
    "print(\"  \" + \"-\"*75)\n",
    "for i, (old_id, new_id) in enumerate(list(id_mapping.items())[:10]):\n",
    "    truncated_old = old_id[:45] if len(old_id) > 45 else old_id\n",
    "    print(f\"  {truncated_old:<45} → {new_id:<25}\")\n",
    "\n",
    "# 분석 결과를 JSON으로도 저장\n",
    "analysis_path = DATA_DIR / \"id_analysis_report.json\"\n",
    "with open(analysis_path, 'w', encoding='utf-8') as f:\n",
    "    # numpy/datetime 객체를 JSON 직렬화 가능하도록 변환\n",
    "    json_safe_analysis = json.loads(json.dumps(analysis, default=str))\n",
    "    json.dump(json_safe_analysis, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"\\n📊 분석 보고서 저장: {analysis_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def save_merged_documents(\n",
    "    documents: List[Document], \n",
    "    output_path: Path,\n",
    "    id_mapping: Dict[str, str] = None,\n",
    "    create_backup: bool = True\n",
    ") -> None:\n",
    "    \"\"\"병합된 문서를 pickle 파일로 저장합니다.\"\"\"\n",
    "    \n",
    "    # 백업 생성 (기존 파일이 있는 경우)\n",
    "    if create_backup and output_path.exists():\n",
    "        backup_path = output_path.with_suffix(f'.backup_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.pkl')\n",
    "        print(f\"기존 파일 백업: {backup_path}\")\n",
    "        output_path.rename(backup_path)\n",
    "    \n",
    "    # 저장\n",
    "    with open(output_path, 'wb') as f:\n",
    "        pickle.dump(documents, f)\n",
    "    \n",
    "    file_size = output_path.stat().st_size / (1024 * 1024)  # MB로 변환\n",
    "    print(f\"\\n✅ 저장 완료!\")\n",
    "    print(f\"  파일: {output_path}\")\n",
    "    print(f\"  크기: {file_size:.2f} MB\")\n",
    "    print(f\"  문서 수: {len(documents)}\")\n",
    "    \n",
    "    # 메타데이터 파일도 생성 (JSON)\n",
    "    metadata_path = output_path.with_suffix('.metadata.json')\n",
    "    \n",
    "    # ID 체계 정보 수집\n",
    "    id_prefixes = set()\n",
    "    categories_used = set()\n",
    "    sources_used = set()\n",
    "    \n",
    "    for doc in documents:\n",
    "        if 'id_source_prefix' in doc.metadata:\n",
    "            id_prefixes.add(doc.metadata['id_source_prefix'])\n",
    "        if 'category' in doc.metadata:\n",
    "            categories_used.add(doc.metadata['category'])\n",
    "        if 'source' in doc.metadata:\n",
    "            sources_used.add(doc.metadata['source'])\n",
    "    \n",
    "    metadata = {\n",
    "        'created_at': datetime.now().isoformat(),\n",
    "        'total_documents': len(documents),\n",
    "        'sources': list(sources_used),\n",
    "        'categories': list(categories_used),\n",
    "        'id_prefixes': list(id_prefixes),\n",
    "        'id_reorganized': id_mapping is not None,\n",
    "        'total_id_mappings': len(id_mapping) if id_mapping else 0,\n",
    "        'merged_from': [\n",
    "            str(GV80_FILE),\n",
    "            str(DIGITAL_GOV_FILE)\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    with open(metadata_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(metadata, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    print(f\"  메타데이터: {metadata_path}\")\n",
    "    \n",
    "    # ID 매핑도 저장 (있는 경우)\n",
    "    if id_mapping:\n",
    "        mapping_path = output_path.with_suffix('.id_mapping.json')\n",
    "        with open(mapping_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump({\n",
    "                'created_at': datetime.now().isoformat(),\n",
    "                'total_mappings': len(id_mapping),\n",
    "                'mapping': id_mapping\n",
    "            }, f, ensure_ascii=False, indent=2)\n",
    "        print(f\"  ID 매핑: {mapping_path}\")\n",
    "\n",
    "# 저장 실행\n",
    "save_merged_documents(merged_documents, MERGED_FILE, id_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. 검증: 저장된 파일 다시 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "저장된 파일 검증 중...\n",
      "\n",
      "✅ 검증 결과:\n",
      "  로드된 문서 수: 280\n",
      "  원본 문서 수: 280\n",
      "  일치: True\n",
      "\n",
      "🔒 데이터 무결성 검증:\n",
      "  문서 수 일치: ✅\n",
      "  ID 일치: ✅\n",
      "  메타데이터 일치: ✅\n",
      "  컨텐츠 일치: ✅\n",
      "\n",
      "  ✨ 완벽한 데이터 무결성!\n",
      "\n",
      "📋 샘플 문서 상세 정보:\n",
      "\n",
      "  문서 #1:\n",
      "    ID: gv80_owners_manual_test6p_0001\n",
      "    원본 ID: 0\n",
      "    Prefix: gv80_owners_manual_test6p\n",
      "    카테고리: heading1\n",
      "    소스: data/gv80_owners_manual_TEST6P.pdf\n",
      "    페이지: 1\n",
      "    내용 길이: 17 글자\n",
      "    내용 미리보기: # 내 용 찾 기 방 법 설 명...\n",
      "\n",
      "  문서 #2:\n",
      "    ID: gv80_owners_manual_test6p_0002\n",
      "    원본 ID: 1\n",
      "    Prefix: gv80_owners_manual_test6p\n",
      "    카테고리: heading1\n",
      "    소스: data/gv80_owners_manual_TEST6P.pdf\n",
      "    페이지: 1\n",
      "    내용 길이: 11 글자\n",
      "    내용 미리보기: # 내용으로 찾을 때...\n",
      "\n",
      "  문서 #3:\n",
      "    ID: gv80_owners_manual_test6p_0003\n",
      "    원본 ID: 2\n",
      "    Prefix: gv80_owners_manual_test6p\n",
      "    카테고리: figure\n",
      "    소스: data/gv80_owners_manual_TEST6P.pdf\n",
      "    페이지: 1\n",
      "    내용 길이: 1 글자\n",
      "    내용 미리보기: \n",
      "...\n"
     ]
    }
   ],
   "source": [
    "# 저장된 파일을 다시 로드하여 무결성 확인\n",
    "print(\"저장된 파일 검증 중...\")\n",
    "\n",
    "with open(MERGED_FILE, 'rb') as f:\n",
    "    loaded_docs = pickle.load(f)\n",
    "\n",
    "print(f\"\\n✅ 검증 결과:\")\n",
    "print(f\"  로드된 문서 수: {len(loaded_docs)}\")\n",
    "print(f\"  원본 문서 수: {len(merged_documents)}\")\n",
    "print(f\"  일치: {len(loaded_docs) == len(merged_documents)}\")\n",
    "\n",
    "# 데이터 무결성 상세 검증\n",
    "def verify_data_integrity(original_docs: List[Document], loaded_docs: List[Document]) -> Dict[str, Any]:\n",
    "    \"\"\"저장/로드 후 데이터 무결성을 검증합니다.\"\"\"\n",
    "    integrity_report = {\n",
    "        'document_count_match': len(original_docs) == len(loaded_docs),\n",
    "        'id_match': True,\n",
    "        'metadata_match': True,\n",
    "        'content_match': True,\n",
    "        'errors': []\n",
    "    }\n",
    "    \n",
    "    # 문서별 검증\n",
    "    for i, (orig, loaded) in enumerate(zip(original_docs, loaded_docs)):\n",
    "        # ID 검증\n",
    "        if orig.metadata.get('id') != loaded.metadata.get('id'):\n",
    "            integrity_report['id_match'] = False\n",
    "            integrity_report['errors'].append(f\"Document {i}: ID mismatch\")\n",
    "        \n",
    "        # 메타데이터 키 검증\n",
    "        if set(orig.metadata.keys()) != set(loaded.metadata.keys()):\n",
    "            integrity_report['metadata_match'] = False\n",
    "            integrity_report['errors'].append(f\"Document {i}: Metadata keys mismatch\")\n",
    "        \n",
    "        # 컨텐츠 검증\n",
    "        if orig.page_content != loaded.page_content:\n",
    "            integrity_report['content_match'] = False\n",
    "            integrity_report['errors'].append(f\"Document {i}: Content mismatch\")\n",
    "    \n",
    "    return integrity_report\n",
    "\n",
    "# 무결성 검증 실행\n",
    "integrity = verify_data_integrity(merged_documents, loaded_docs)\n",
    "\n",
    "print(\"\\n🔒 데이터 무결성 검증:\")\n",
    "print(f\"  문서 수 일치: {'✅' if integrity['document_count_match'] else '❌'}\")\n",
    "print(f\"  ID 일치: {'✅' if integrity['id_match'] else '❌'}\")\n",
    "print(f\"  메타데이터 일치: {'✅' if integrity['metadata_match'] else '❌'}\")\n",
    "print(f\"  컨텐츠 일치: {'✅' if integrity['content_match'] else '❌'}\")\n",
    "\n",
    "if integrity['errors']:\n",
    "    print(f\"\\n  ⚠️  발견된 오류:\")\n",
    "    for error in integrity['errors'][:5]:\n",
    "        print(f\"    - {error}\")\n",
    "else:\n",
    "    print(\"\\n  ✨ 완벽한 데이터 무결성!\")\n",
    "\n",
    "# 샘플 문서 상세 확인\n",
    "if loaded_docs:\n",
    "    print(\"\\n📋 샘플 문서 상세 정보:\")\n",
    "    for i in range(min(3, len(loaded_docs))):\n",
    "        doc = loaded_docs[i]\n",
    "        print(f\"\\n  문서 #{i+1}:\")\n",
    "        print(f\"    ID: {doc.metadata.get('id', 'N/A')}\")\n",
    "        print(f\"    원본 ID: {doc.metadata.get('original_id', 'N/A')}\")\n",
    "        print(f\"    Prefix: {doc.metadata.get('id_source_prefix', 'N/A')}\")\n",
    "        print(f\"    카테고리: {doc.metadata.get('category', 'N/A')}\")\n",
    "        print(f\"    소스: {doc.metadata.get('source', 'N/A')}\")\n",
    "        print(f\"    페이지: {doc.metadata.get('page', 'N/A')}\")\n",
    "        print(f\"    내용 길이: {len(doc.page_content)} 글자\")\n",
    "        print(f\"    내용 미리보기: {doc.page_content[:50]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. 요약 및 다음 단계\n",
    "\n",
    "### 완료된 작업:\n",
    "- ✅ 두 개의 DDU pickle 파일 로드\n",
    "- ✅ 문서 구조 및 호환성 검증\n",
    "- ✅ ID 충돌 처리 및 병합\n",
    "- ✅ 병합된 문서 저장 및 메타데이터 생성\n",
    "\n",
    "### 다음 단계 제안:\n",
    "1. **데이터베이스 인제스트**: 병합된 문서를 PostgreSQL에 인제스트\n",
    "2. **임베딩 생성**: 새로운 문서들에 대한 벡터 임베딩 생성\n",
    "3. **검색 테스트**: 병합된 데이터셋으로 검색 성능 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "🎉 병합 작업 완료!\n",
      "======================================================================\n",
      "\n",
      "📊 최종 통계:\n",
      "  작업 시간: 2025-08-26 22:19:10\n",
      "  총 병합 문서: 280개\n",
      "    - GV80 문서: 122개\n",
      "    - 디지털정부 문서: 158개\n",
      "\n",
      "  생성된 Prefix:\n",
      "    - gv80_owners_manual_test6p: 122개\n",
      "    - 디지털정부혁신_추진계획: 158개\n",
      "\n",
      "  카테고리 분포:\n",
      "    - paragraph: 182개 (65.0%)\n",
      "    - heading1: 57개 (20.4%)\n",
      "    - table: 14개 (5.0%)\n",
      "    - figure: 13개 (4.6%)\n",
      "    - header: 5개 (1.8%)\n",
      "\n",
      "📁 생성된 파일:\n",
      "  ✅ merged_ddu_documents.pkl - 병합된 DDU 문서 (860.35 KB)\n",
      "  ✅ merged_ddu_documents.metadata.json - 메타데이터 (445 bytes)\n",
      "  ❌ merged_ddu_documents.id_mapping.json - 파일 없음\n",
      "  ❌ id_analysis_report.json - 파일 없음\n",
      "\n",
      "🚀 다음 단계:\n",
      "1. 데이터베이스 인제스트:\n",
      "   python scripts/2_phase1_ingest_documents.py --pickle-file data/merged_ddu_documents.pkl\n",
      "\n",
      "2. 임베딩 생성 및 벡터 저장:\n",
      "   - 한국어/영어 이중 언어 임베딩 생성\n",
      "   - PostgreSQL + pgvector에 저장\n",
      "\n",
      "3. 검색 테스트:\n",
      "   python scripts/test_retrieval_real_data.py\n",
      "\n",
      "4. 워크플로우 실행:\n",
      "   python scripts/test_workflow_complete.py\n",
      "\n",
      "💡 팁:\n",
      "  - ID 매핑 파일을 활용하여 원본 ID 추적 가능\n",
      "  - 메타데이터 파일로 데이터 구조 빠른 파악 가능\n",
      "  - ID 분석 보고서로 데이터 품질 모니터링 가능\n",
      "\n",
      "======================================================================\n",
      "모든 작업이 성공적으로 완료되었습니다! 🎊\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"🎉 병합 작업 완료!\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# 최종 요약 통계\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"\\n📊 최종 통계:\")\n",
    "print(f\"  작업 시간: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"  총 병합 문서: {len(merged_documents)}개\")\n",
    "print(f\"    - GV80 문서: {len(gv80_docs)}개\")\n",
    "print(f\"    - 디지털정부 문서: {len(digital_gov_docs)}개\")\n",
    "\n",
    "# Prefix별 최종 통계\n",
    "prefix_counts = {}\n",
    "for doc in merged_documents:\n",
    "    prefix = doc.metadata.get('id_source_prefix', 'unknown')\n",
    "    prefix_counts[prefix] = prefix_counts.get(prefix, 0) + 1\n",
    "\n",
    "print(f\"\\n  생성된 Prefix:\")\n",
    "for prefix, count in sorted(prefix_counts.items()):\n",
    "    print(f\"    - {prefix}: {count}개\")\n",
    "\n",
    "# 카테고리 요약\n",
    "category_counts = {}\n",
    "for doc in merged_documents:\n",
    "    category = doc.metadata.get('category', 'unknown')\n",
    "    category_counts[category] = category_counts.get(category, 0) + 1\n",
    "\n",
    "print(f\"\\n  카테고리 분포:\")\n",
    "top_categories = sorted(category_counts.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "for category, count in top_categories:\n",
    "    percentage = (count / len(merged_documents)) * 100\n",
    "    print(f\"    - {category}: {count}개 ({percentage:.1f}%)\")\n",
    "\n",
    "# 생성된 파일 목록\n",
    "print(\"\\n📁 생성된 파일:\")\n",
    "output_files = [\n",
    "    (MERGED_FILE, \"병합된 DDU 문서\"),\n",
    "    (MERGED_FILE.with_suffix('.metadata.json'), \"메타데이터\"),\n",
    "    (MERGED_FILE.with_suffix('.id_mapping.json'), \"ID 매핑 테이블\"),\n",
    "    (DATA_DIR / \"id_analysis_report.json\", \"ID 분석 보고서\")\n",
    "]\n",
    "\n",
    "for file_path, description in output_files:\n",
    "    if file_path.exists():\n",
    "        file_size = file_path.stat().st_size\n",
    "        if file_size > 1024*1024:\n",
    "            size_str = f\"{file_size/(1024*1024):.2f} MB\"\n",
    "        elif file_size > 1024:\n",
    "            size_str = f\"{file_size/1024:.2f} KB\"\n",
    "        else:\n",
    "            size_str = f\"{file_size} bytes\"\n",
    "        print(f\"  ✅ {file_path.name} - {description} ({size_str})\")\n",
    "    else:\n",
    "        print(f\"  ❌ {file_path.name} - 파일 없음\")\n",
    "\n",
    "print(\"\\n🚀 다음 단계:\")\n",
    "print(\"1. 데이터베이스 인제스트:\")\n",
    "print(f\"   python scripts/2_phase1_ingest_documents.py --pickle-file {MERGED_FILE}\")\n",
    "print(\"\")\n",
    "print(\"2. 임베딩 생성 및 벡터 저장:\")\n",
    "print(\"   - 한국어/영어 이중 언어 임베딩 생성\")\n",
    "print(\"   - PostgreSQL + pgvector에 저장\")\n",
    "print(\"\")\n",
    "print(\"3. 검색 테스트:\")\n",
    "print(\"   python scripts/test_retrieval_real_data.py\")\n",
    "print(\"\")\n",
    "print(\"4. 워크플로우 실행:\")\n",
    "print(\"   python scripts/test_workflow_complete.py\")\n",
    "\n",
    "print(\"\\n💡 팁:\")\n",
    "print(\"  - ID 매핑 파일을 활용하여 원본 ID 추적 가능\")\n",
    "print(\"  - 메타데이터 파일로 데이터 구조 빠른 파악 가능\")\n",
    "print(\"  - ID 분석 보고서로 데이터 품질 모니터링 가능\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"모든 작업이 성공적으로 완료되었습니다! 🎊\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multimodal-rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
